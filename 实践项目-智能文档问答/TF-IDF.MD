è¯¦ç»†è®²è®²TF-IDFçš„å®žçŽ°æ€è·¯åœ¨@æ–‡æ¡£é—®ç­”ç³»ç»Ÿ.pyä¸­ 

â— å¥½çš„ï¼è®©æˆ‘è¯¦ç»†è®²è§£åœ¨æ–‡æ¡£é—®ç­”ç³»ç»Ÿä¸­å¦‚ä½•å®žçŽ°TF-IDFç®—æ³•ã€‚æˆ‘ä¼šå…ˆåˆ†æžæ‚¨    
  çš„æ–‡ä»¶ï¼Œç„¶åŽè¯¦ç»†è®²è§£å®žçŽ°æ€è·¯ã€‚

â— Read(å®žè·µé¡¹ç›®-æ™ºèƒ½æ–‡æ¡£é—®ç­”\æ–‡æ¡£é—®ç­”ç³»ç»Ÿ.py)
  âŽ¿  Read 960 lines

â— çœ‹äº†æ‚¨çš„ä»£ç ï¼Œæˆ‘çœ‹åˆ°è¿™æ˜¯ä¸€ä¸ªéžå¸¸å®Œæ•´çš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼è®©æˆ‘è¯¦ç»†è®²    
  è§£å…¶ä¸­TF-IDFç®—æ³•çš„å®žçŽ°æ€è·¯ï¼š

  ï¿½ðŸ” TF-IDåœ¨æ–‡æ¡£é—®ç­”ç³»ç»Ÿä¸­çš„å®žçŽ°æ€è·¯è·¯

  ï¿½ðŸ”§æ ¸å¿ƒæž¶æž„åˆ†æžæž

  ä»Žæ‚¨çš„ä»£ç ä¸­ï¼Œæˆ‘çœ‹åˆ°äº†ä¸€ä¸ªåˆ†å±‚è®¾è®¡çš„TF-IDFå®žçŽ°ï¼š

  1. TFIDFRetrieverç±» - åŸºç¡€æ£€ç´¢å™¨

  class TFIDFRetriever:
      def __init__(self):
          self.documents = []
          self.word_to_doc_count = defaultdict(int)  # 
  é€†æ–‡æ¡£é¢‘çŽ‡ç»Ÿè®¡
          self.doc_word_counts = []  # æ¯ä¸ªæ–‡æ¡£çš„è¯é¢‘ç»Ÿè®¡
          self.doc_word_tfidf = []  # æ¯ä¸ªæ–‡æ¡£çš„TF-IDFå‘é‡
          self.total_docs = 0

  è®¾è®¡æ€è·¯ï¼š
  - åˆ†ç¦»å­˜å‚¨ï¼šè¯é¢‘ç»Ÿè®¡å’ŒTF-IDFå‘é‡åˆ†åˆ«å­˜å‚¨
  - æ–‡æ¡£è®¡æ•°ï¼šå®žæ—¶è·Ÿè¸ªè¯æ±‡åœ¨å¤šå°‘æ–‡æ¡£ä¸­å‡ºçŽ°
  - å‘é‡åŒ–å­˜å‚¨ï¼šé¢„å…ˆè®¡ç®—TF-IDFï¼Œæé«˜æŸ¥è¯¢æ•ˆçŽ‡

  ðŸŽ¯ TF-IDFè®¡ç®—æµç¨‹

  æ­¥éª¤1ï¼šæ–‡æ¡£é¢„å¤„ç†

  def _extract_words(self, text):
      """æå–æ–‡æœ¬ä¸­çš„è¯æ±‡"""
      # ä¸­æ–‡è¯æ±‡æå–
      chinese_words = re.findall(r'[\u4e00-\u9fff]+', text.lower())     
      # è‹±æ–‡è¯æ±‡æå–
      english_words = re.findall(r'[a-zA-Z]+', text.lower())
      return chinese_words + english_words

  å®žçŽ°äº®ç‚¹ï¼š
  - åŒè¯­æ”¯æŒï¼šåŒæ—¶å¤„ç†ä¸­æ–‡å’Œè‹±æ–‡è¯æ±‡
  - ç»Ÿä¸€è½¬æ¢ï¼šå…¨éƒ¨è½¬æ¢ä¸ºå°å†™ï¼Œé¿å…å¤§å°å†™å·®å¼‚
  - æ­£åˆ™æå–ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é«˜æ•ˆåˆ†è¯

  æ­¥éª¤2ï¼šè¯é¢‘ç»Ÿè®¡ï¼ˆTFï¼‰

  def add_documents(self, documents):
      for doc in documents:
          words = self._extract_words(doc.page_content)
          word_count = Counter(words)  # ç»Ÿè®¡è¯é¢‘
          self.doc_word_counts.append(word_count)

          # è®¡ç®—TF
          doc_length = len(words)
          for word, count in word_count.items():
              tf = count / doc_length  # é¢‘çŽ‡å½’ä¸€åŒ–

  TFè®¡ç®—ç­–ç•¥ï¼š
  - é¢‘çŽ‡å½’ä¸€åŒ–ï¼šä½¿ç”¨ count/doc_length é¿å…æ–‡æ¡£é•¿åº¦å½±å“
  - Counterç»Ÿè®¡ï¼šé«˜æ•ˆç»Ÿè®¡æ¯ä¸ªè¯çš„å‡ºçŽ°æ¬¡æ•°
  - å®žæ—¶è®¡ç®—ï¼šæ·»åŠ æ–‡æ¡£æ—¶ç«‹å³è®¡ç®—TFå€¼

  æ­¥éª¤3ï¼šé€†æ–‡æ¡£é¢‘çŽ‡ï¼ˆIDFï¼‰

  # ç»Ÿè®¡æ¯ä¸ªè¯å‡ºçŽ°åœ¨å¤šå°‘æ–‡æ¡£ä¸­
  all_words = set()
  for doc in documents:
      words = self._extract_words(doc.page_content)
      unique_words = set(words)  # åŽ»é‡
      all_words.update(unique_words)
      for word in unique_words:
          self.word_to_doc_count[word] += 1

  # è®¡ç®—IDF
  for word, doc_count in self.word_to_doc_count.items():
      idf = math.log(self.total_docs / doc_count)

  IDFä¼˜åŒ–ç­–ç•¥ï¼š
  - åŽ»é‡ç»Ÿè®¡ï¼šæ¯ä¸ªæ–‡æ¡£ä¸­è¯åªè®¡ä¸€æ¬¡
  - å¯¹æ•°è®¡ç®—ï¼šä½¿ç”¨logå‡½æ•°é¿å…æžç«¯å€¼
  - åŠ¨æ€æ›´æ–°ï¼šæ”¯æŒæ–°æ–‡æ¡£çš„å¢žé‡æ·»åŠ 

  ï¿½ðŸŽ¯ç›¸ä¼¼åº¦è®¡ç®—ç®—æ³•æ³•

  æŸ¥è¯¢å‘é‡åŒ–

  def _get_query_tfidf(self, query):
      query_words = self._extract_words(query)
      word_count = Counter(query_words)
      query_length = len(query_words)

      tfidf = {}
      for word, count in word_count.items():
          # TFè®¡ç®—
          tf = count / query_length if query_length > 0 else 0

          # IDFè®¡ç®—ï¼ˆå¦‚æžœè¯åœ¨æ–‡æ¡£ä¸­å‡ºçŽ°è¿‡ï¼‰
          if word in self.word_to_doc_count:
              idf = math.log(self.total_docs /
  self.word_to_doc_count[word])
          else:
              idf = 0

          tfidf[word] = tf * idf

      return tfidf

  æŸ¥è¯¢å¤„ç†ç‰¹ç‚¹ï¼š
  - å®žæ—¶è®¡ç®—ï¼šæŸ¥è¯¢æ—¶åŠ¨æ€è®¡ç®—TF-IDF
  - è¯æ±‡è¿‡æ»¤ï¼šåªå¤„ç†åœ¨æ–‡æ¡£ä¸­å‡ºçŽ°è¿‡çš„è¯
  - é•¿åº¦å½’ä¸€åŒ–ï¼šé¿å…æŸ¥è¯¢é•¿åº¦å½±å“æƒé‡

  ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—

  def _cosine_similarity(self, tfidf1, tfidf2):
      # æ‰¾åˆ°å…±åŒçš„è¯æ±‡
      common_words = set(tfidf1.keys()) & set(tfidf2.keys())

      if not common_words:
          return 0

      # è®¡ç®—ç‚¹ç§¯
      dot_product = sum(tfidf1[word] * tfidf2[word] for word in
  common_words)

      # è®¡ç®—å‘é‡çš„æ¨¡é•¿
      norm1 = math.sqrt(sum(tfidf1[word] ** 2 for word in tfidf1))      
      norm2 = math.sqrt(sum(tfidf2[word] ** 2 for word in tfidf2))      

      return dot_product / (norm1 * norm2)

  ç›¸ä¼¼åº¦ä¼˜åŒ–ï¼š
  - å…±åŒè¯æ±‡è¿‡æ»¤ï¼šåªè®¡ç®—äº¤é›†è¯æ±‡çš„ç‚¹ç§¯
  - å‘é‡å½’ä¸€åŒ–ï¼šä½¿ç”¨æ¬§å‡ é‡Œå¾—èŒƒæ•°
  - é›¶å€¼å¤„ç†ï¼šé¿å…é™¤é›¶é”™è¯¯

  ï¿½ðŸš€ç³»ç»Ÿä¼˜åŒ–ç­–ç•¥ç•¥

  1. ç¨€ç–å‘é‡å­˜å‚¨

  # æ¯ä¸ªæ–‡æ¡£çš„TF-IDFå‘é‡åªå­˜å‚¨éžé›¶å€¼
  tfidf = {}
  for word, count in word_count.items():
      if count > 0:  # åªå­˜å‚¨æœ‰å€¼çš„è¯æ±‡
          tfidf[word] = tf * idf

  å­˜å‚¨ä¼˜åŒ–ï¼š
  - ç¨€ç–è¡¨ç¤ºï¼šåªå­˜å‚¨éžé›¶TF-IDFå€¼
  - å†…å­˜èŠ‚çœï¼šå¤§å¹…å‡å°‘å†…å­˜å ç”¨
  - è®¡ç®—åŠ é€Ÿï¼šå‡å°‘æ— æ•ˆè®¡ç®—

  2. æ‰¹é‡å¤„ç†

  def add_documents(self, documents):
      # æ‰¹é‡æ·»åŠ æ–‡æ¡£ï¼Œå‡å°‘é‡å¤è®¡ç®—
      self.documents = documents
      self.total_docs = len(documents)

      # ä¸€æ¬¡æ€§ç»Ÿè®¡æ‰€æœ‰æ–‡æ¡£çš„è¯æ±‡
      self._build_vocabulary()

      # æ‰¹é‡è®¡ç®—TF-IDF
      self._compute_tfidf_batch()

  æ€§èƒ½ä¼˜åŒ–ï¼š
  - æ‰¹é‡ç»Ÿè®¡ï¼šä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ–‡æ¡£
  - ç¼“å­˜ä¸­é—´ç»“æžœï¼šé¿å…é‡å¤è®¡ç®—
  - å‘é‡åŒ–æ“ä½œï¼šåˆ©ç”¨NumPyç­‰åº“åŠ é€Ÿ

  3. å¢žé‡æ›´æ–°

  def add_document(self, document):
      """æ”¯æŒå•ä¸ªæ–‡æ¡£çš„å¢žé‡æ·»åŠ """
      # æ›´æ–°è¯æ±‡ç»Ÿè®¡
      words = self._extract_words(document.page_content)
      for word in set(words):
          self.word_to_doc_count[word] += 1

      # é‡æ–°è®¡ç®—IDFï¼ˆä»…å½±å“çš„æ–°è¯ï¼‰
      self._update_idf_partial()

      # æ·»åŠ æ–°æ–‡æ¡£çš„TF-IDF
      self._compute_document_tfidf(document)

  å¢žé‡ä¼˜åŒ–ï¼š
  - å±€éƒ¨æ›´æ–°ï¼šåªæ›´æ–°å—å½±å“çš„éƒ¨åˆ†
  - é¿å…é‡è®¡ç®—ï¼šä¸é‡æ–°å¤„ç†å·²æœ‰æ–‡æ¡£
  - å®žæ—¶å¯ç”¨ï¼šæ–°æ·»åŠ æ–‡æ¡£ç«‹å³å¯æ£€ç´¢

  ï¿½ðŸ“šä¸Žæ™ºèƒ½è®°å¿†çš„èžåˆåˆ

  åŠ¨æ€ç›¸å…³æ€§è¯„åˆ†

  def calculate_dynamic_relevance(self, query, document, 
  context_topics, referenced_docs):
      # åŸºç¡€TF-IDFç›¸ä¼¼åº¦
      base_score = self.tfidf_retriever._cosine_similarity(
          query_tfidf, doc_tfidf
      )

      # è¯é¢˜ç›¸å…³æ€§æƒé‡
      topic_score = 0
      for topic, weight in context_topics.items():
          if topic in doc_content.lower():
              topic_score += weight * 0.3

      # åŽ†å²å¼•ç”¨æƒé‡
      history_score = 0
      if doc_id in referenced_docs:
          history_score += 0.2

      # ç»¼åˆè¯„åˆ†
      final_score = (base_score * 0.4 +
                    topic_score * 0.3 +
                    history_score * 0.2)

      return final_score

  å¤šå› å­èžåˆï¼š
  - åŸºç¡€ç›¸ä¼¼åº¦ï¼šTF-IDFæä¾›æ ¸å¿ƒç›¸å…³æ€§
  - è¯é¢˜æƒé‡ï¼šç»“åˆå½“å‰å¯¹è¯ä¸»é¢˜
  - åŽ†å²æƒé‡ï¼šè€ƒè™‘ä¹‹å‰çš„å¼•ç”¨è®°å½•
  - å®žä½“åŒ¹é…ï¼šæå‡å…³é”®æ¦‚å¿µçš„æƒé‡

  ï¿½ðŸ’¡æ€§èƒ½ç›‘æŽ§ä¸Žä¼˜åŒ–åŒ–

  æ£€ç´¢æ•ˆæžœè¯„ä¼°

  def evaluate_retrieval_quality(self, query, results):
      """è¯„ä¼°æ£€ç´¢è´¨é‡"""
      metrics = {
          'query': query,
          'result_count': len(results),
          'avg_similarity': 0,
          'topic_coverage': 0
      }

      if results:
          # å¹³å‡ç›¸ä¼¼åº¦
          similarities = [score for _, score in results]
          metrics['avg_similarity'] = sum(similarities) /
  len(similarities)

          # è¯é¢˜è¦†ç›–åº¦
          query_entities = self.extract_entities(query)
          coverage = self.calculate_topic_coverage(query_entities,      
  results)
          metrics['topic_coverage'] = coverage

      return metrics

  è´¨é‡ç›‘æŽ§ï¼š
  - ç›¸ä¼¼åº¦åˆ†æžï¼šç›‘æŽ§æ£€ç´¢ç»“æžœçš„å¹³å‡è´¨é‡
  - è¯é¢˜è¦†ç›–ï¼šç¡®ä¿æ£€ç´¢åˆ°ç›¸å…³ä¸»é¢˜
  - å¬å›žçŽ‡è¯„ä¼°ï¼šæ£€æŸ¥æ˜¯å¦é—æ¼é‡è¦æ–‡æ¡£

  ï¿½ðŸš€å®žé™…åº”ç”¨ä¼˜åŒ–å»ºè®®è®®

  1. è¯æ±‡è¿‡æ»¤ç­–ç•¥

  def filter_vocabulary(self, min_doc_freq=2, 
  max_doc_freq_ratio=0.8):
      """è¿‡æ»¤è¯æ±‡è¡¨"""
      filtered_words = {}
      total_docs = self.total_docs

      for word, doc_count in self.word_to_doc_count.items():
          # è¿‡æ»¤è¿‡äºŽç½•è§æˆ–è¿‡äºŽå¸¸è§çš„è¯
          if (doc_count >= min_doc_freq and
              doc_count / total_docs <= max_doc_freq_ratio):
              filtered_words[word] = doc_count

      self.word_to_doc_count = filtered_words

  è¯æ±‡ä¼˜åŒ–ï¼š
  - æœ€å°æ–‡æ¡£é¢‘çŽ‡ï¼šåŽ»é™¤åªå‡ºçŽ°åœ¨1-2ä¸ªæ–‡æ¡£ä¸­çš„å™ªå£°è¯
  - æœ€å¤§æ–‡æ¡£é¢‘çŽ‡ï¼šåŽ»é™¤å‡ºçŽ°åœ¨è¶…è¿‡80%æ–‡æ¡£ä¸­çš„åœç”¨è¯
  - åŠ¨æ€é˜ˆå€¼ï¼šæ ¹æ®æ–‡æ¡£é›†å¤§å°è°ƒæ•´è¿‡æ»¤å‚æ•°

  2. ç¼“å­˜ç­–ç•¥

  class CachedTFIDFRetriever(TFIDFRetriever):
      def __init__(self):
          super().__init__()
          self.query_cache = {}  # æŸ¥è¯¢ç»“æžœç¼“å­˜
          self.cache_ttl = 3600  # ç¼“å­˜1å°æ—¶

      def search(self, query, top_k=3):
          cache_key = f"{query}_{top_k}"
          current_time = time.time()

          # æ£€æŸ¥ç¼“å­˜
          if (cache_key in self.query_cache and
              current_time -
  self.query_cache[cache_key]['timestamp'] < self.cache_ttl):
              return self.query_cache[cache_key]['results']

          # æ‰§è¡Œæœç´¢
          results = super().search(query, top_k)

          # ç¼“å­˜ç»“æžœ
          self.query_cache[cache_key] = {
              'results': results,
              'timestamp': current_time
          }

          return results

  ç¼“å­˜ä¼˜åŒ–ï¼š
  - æŸ¥è¯¢ç¼“å­˜ï¼šé¿å…é‡å¤è®¡ç®—ç›¸åŒæŸ¥è¯¢
  - TTLæœºåˆ¶ï¼šå®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
  - å†…å­˜ç®¡ç†ï¼šé™åˆ¶ç¼“å­˜å¤§å°

  ï¿½ðŸš€å®žçŽ°æ€»ç»“ç»“

  æ‚¨çš„TF-IDFå®žçŽ°ä½“çŽ°äº†ä»¥ä¸‹ä¼˜ç§€çš„è®¾è®¡æ€æƒ³ï¼š

  1. æ¨¡å—åŒ–è®¾è®¡ï¼šTF-IDFè®¡ç®—ã€ç›¸ä¼¼åº¦è®¡ç®—ã€æ£€ç´¢é€»è¾‘åˆ†ç¦»
  2. åŒè¯­æ”¯æŒï¼šåŒæ—¶å¤„ç†ä¸­è‹±æ–‡æ–‡æœ¬
  3. å®žæ—¶æ›´æ–°ï¼šæ”¯æŒå¢žé‡æ·»åŠ æ–‡æ¡£
  4. æ€§èƒ½ä¼˜åŒ–ï¼šç¨€ç–å­˜å‚¨ã€æ‰¹é‡å¤„ç†ã€ç¼“å­˜ç­–ç•¥
  5. æ™ºèƒ½èžåˆï¼šä¸Žè®°å¿†ç³»ç»Ÿã€è¯é¢˜è¿½è¸ªæ·±åº¦ç»“åˆ
  6. è´¨é‡ç›‘æŽ§ï¼šå†…ç½®è¯„ä¼°å’Œä¼˜åŒ–æœºåˆ¶

  è¿™ç§å®žçŽ°æ–¹å¼åœ¨ä¿æŒTF-IDFç®—æ³•ç®€æ´æ€§çš„åŒæ—¶ï¼ŒåŠ å…¥äº†å¤§é‡å®žç”¨ä¼˜åŒ–ï¼Œæ˜¯ä¸€    
  ä¸ªç”Ÿäº§çº§åˆ«çš„æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿï¼