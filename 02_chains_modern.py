#!/usr/bin/env python3
"""
LangChain ç°ä»£é“¾å¼è°ƒç”¨ç¤ºä¾‹ (ä½¿ç”¨LCEL)
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æœ€æ–°çš„LangChain Expression Language (LCEL) æ„å»ºé“¾
 02_chains_modern.py åŒ…å«äº†7ä¸ªç°ä»£åŒ–çš„é“¾ç¤ºä¾‹:

  1. åŸºç¡€LCELé“¾ - ä½¿ç”¨ | ç®¡é“æ“ä½œç¬¦
  2. å¹¶è¡Œé“¾ - åŒæ—¶æ‰§è¡Œå¤šä¸ªä»»åŠ¡
  3. æ¡ä»¶é“¾ - æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒè·¯å¾„
  4. é¡ºåºé“¾ - æ­¥éª¤é—´çš„æ•°æ®ä¼ é€’
  5. JSONè¾“å‡ºé“¾ - ç»“æ„åŒ–æ•°æ®è¾“å‡º
  6. è‡ªå®šä¹‰å‡½æ•°é“¾ - é›†æˆè‡ªå®šä¹‰Pythonå‡½æ•°
  7. èŠå¤©æ¨¡æ¿é“¾ - ä½¿ç”¨ChatPromptTemplate
è¿™æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„ç‰ˆæœ¬ï¼Œè§£å†³äº†åŸç‰ˆæœ¬ä¸­çš„å¯¼å…¥é—®é¢˜
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import (
    RunnablePassthrough,
    RunnableParallel,
    RunnableLambda,
    RunnableBranch
)
from langchain_core.messages import HumanMessage, SystemMessage

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def basic_lcel_chain():
    """åŸºç¡€LCELé“¾ - ä½¿ç”¨ç®¡é“æ“ä½œç¬¦"""
    print("=== åŸºç¡€LCELé“¾ç¤ºä¾‹ ===\n")

    # åˆ›å»ºLLM (ä½¿ç”¨æ™ºè°±AIé…ç½®)
    llm = ChatOpenAI(
        model="glm-4.6",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = PromptTemplate(
        input_variables=["topic"],
        template="è¯·ç”¨ä¸­æ–‡å†™ä¸€æ®µå…³äº{topic}çš„ç®€çŸ­ä»‹ç»ï¼Œå¤§çº¦50ä¸ªå­—ã€‚"
    )

    # åˆ›å»ºè¾“å‡ºè§£æå™¨
    output_parser = StrOutputParser()

#ä½¿ç”¨LCELè¯­æ³•åˆ›å»ºé“¾ï¼šprompt | llm | output_parser
#LCELé“¾ä¸­ï¼Œæ•°æ®æŒ‰ç…§ä»å·¦åˆ°å³çš„é¡ºåºä¾æ¬¡é€šè¿‡æ¯ä¸ªç»„ä»¶ã€‚
#æ•°æ®æµåŠ¨è§„åˆ™ï¼š
#å•å‘æµåŠ¨ï¼šæ•°æ®åªèƒ½ä»å·¦å‘å³æµåŠ¨
#ä¼ é€’æ ¼å¼ï¼šå‰ä¸€ä¸ªç»„ä»¶çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªç»„ä»¶çš„è¾“å…¥
#ç±»å‹åŒ¹é…ï¼šç›¸é‚»ç»„ä»¶çš„è¾“å…¥è¾“å‡ºç±»å‹å¿…é¡»å…¼å®¹
#ç®¡é“æ“ä½œï¼š| æ“ä½œç¬¦è¡¨ç¤ºæ•°æ®çš„é¡ºåºä¼ é€’
    chain = prompt | llm | output_parser

    # æµ‹è¯•ä¸åŒçš„ä¸»é¢˜
    topics = ["äººå·¥æ™ºèƒ½", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]

    for topic in topics:
        try:
            result = chain.invoke({"topic": topic})
            print(f"ä¸»é¢˜: {topic}")
            print(f"ä»‹ç»: {result}\n")
        except Exception as e:
            print(f"å¤„ç†ä¸»é¢˜ '{topic}' æ—¶å‡ºé”™: {e}")

def parallel_chain_example():
    """å¹¶è¡Œé“¾ç¤ºä¾‹ - åŒæ—¶æ‰§è¡Œå¤šä¸ªä»»åŠ¡"""
    print("=== å¹¶è¡Œé“¾ç¤ºä¾‹ ===\n")

    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # å®šä¹‰å¤šä¸ªæç¤ºæ¨¡æ¿
    summary_prompt = PromptTemplate(
        input_variables=["text"],
        template="æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„è¦ç‚¹ï¼š\n{text}\n\nè¦ç‚¹ï¼š"
    )

    translation_prompt = PromptTemplate(
        input_variables=["text"],
        template="å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼š\n{text}\n\nTranslationï¼š"
    )

    sentiment_prompt = PromptTemplate(
        input_variables=["text"],
        template="åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/æ¶ˆæ/ä¸­æ€§ï¼‰ï¼š\n{text}\n\næƒ…æ„Ÿï¼š"
    )

    # åˆ›å»ºå¹¶è¡Œé“¾
    # RunnableParallel æ˜¯LangChainä¸­çš„å¹¶è¡Œä»»åŠ¡è°ƒåº¦å™¨
    #  - è¾“å…¥å…±äº«ï¼šæ‰€æœ‰å¤„ç†é“¾æ¥æ”¶ç›¸åŒçš„è¾“å…¥æ•°æ®
    #  - å¹¶è¡Œæ‰§è¡Œï¼šåŒæ—¶è¿è¡Œå¤šä¸ªç‹¬ç«‹çš„å¤„ç†ä»»åŠ¡
    #  - ç»“æœèšåˆï¼šå°†ç»“æœç»Ÿä¸€ç»„ç»‡åˆ°ä¸€ä¸ªå­—å…¸ä¸­
    parallel_chain = RunnableParallel({
        "summary": summary_prompt | llm | StrOutputParser(),
        "translation": translation_prompt | llm | StrOutputParser(),
        "sentiment": sentiment_prompt | llm | StrOutputParser()
    })

    test_text = "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œæˆ‘å¿ƒæƒ…å¾ˆæ„‰å¿«ï¼"

    try:
        results = parallel_chain.invoke({"text": test_text})
        print(f"åŸå§‹æ–‡æœ¬: {test_text}\n")
        print("å¹¶è¡Œå¤„ç†ç»“æœ:")
        for key, value in results.items():
            print(f"  {key}: {value}")
        print()
    except Exception as e:
        print(f"å¹¶è¡Œå¤„ç†æ—¶å‡ºé”™: {e}")

def conditional_chain_example():
    """æ¡ä»¶é“¾ç¤ºä¾‹ - æ ¹æ®æ¡ä»¶é€‰æ‹©ä¸åŒçš„å¤„ç†è·¯å¾„"""
    print("=== æ¡ä»¶é“¾ç¤ºä¾‹ ===\n")

    llm = ChatOpenAI(
        model="glm-4.6",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # å®šä¹‰ä¸åŒç±»å‹çš„æç¤ºæ¨¡æ¿
    technical_prompt = PromptTemplate(
        input_variables=["question"],
        template="è¯·ç”¨æŠ€æœ¯æ€§çš„è¯­è¨€å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{question}"
    )

    simple_prompt = PromptTemplate(
        input_variables=["question"],
        template="è¯·ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€å›ç­”è¿™ä¸ªé—®é¢˜ï¼š{question}"
    )

    # æ¡ä»¶å‡½æ•° - åˆ¤æ–­é—®é¢˜æ˜¯å¦æ˜¯æŠ€æœ¯æ€§çš„
    def is_technical_question(question):
        tech_keywords = ["ç®—æ³•", "ç¼–ç¨‹", "ä»£ç ", "æ•°æ®ç»“æ„", "API", "æ•°æ®åº“", "ç½‘ç»œ"]
        return any(keyword in question for keyword in tech_keywords)
#   1. tech_keywords åˆ—è¡¨

#   tech_keywords = ["ç®—æ³•", "ç¼–ç¨‹", "ä»£ç ", "æ•°æ®ç»“æ„",
#   "API", "æ•°æ®åº“", "ç½‘ç»œ"]
#   - å®šä¹‰äº†æŠ€æœ¯é¢†åŸŸçš„å…³é”®è¯
#   - åŒ…å«å¸¸è§çš„ç¼–ç¨‹å’ŒæŠ€æœ¯æ¦‚å¿µ

#   2. any() å‡½æ•°

#   - ä½œç”¨ï¼šæ£€æŸ¥å¯è¿­ä»£å¯¹è±¡ä¸­æ˜¯å¦æœ‰ä»»ä½•ä¸€ä¸ªå…ƒç´ ä¸ºTrue
#   - è¿”å›å€¼ï¼šTrueï¼ˆå¦‚æœè‡³å°‘æœ‰ä¸€ä¸ªåŒ¹é…ï¼‰æˆ–
#   Falseï¼ˆå¦‚æœéƒ½æ²¡æœ‰åŒ¹é…ï¼‰

#   3. åˆ—è¡¨æ¨å¯¼å¼ + any()

#   any(keyword in question for keyword in tech_keywords)        

#   è¿™ç›¸å½“äºï¼š
#   for keyword in tech_keywords:           # 
#   éå†æ¯ä¸ªæŠ€æœ¯å…³é”®è¯
#       if keyword in question:             # 
#   æ£€æŸ¥æ˜¯å¦å‡ºç°åœ¨é—®é¢˜ä¸­
#           return True                     # 
#   æ‰¾åˆ°ä»»ä½•ä¸€ä¸ªå°±è¿”å›True
#   return False                            # 
#   éƒ½æ²¡æ‰¾åˆ°è¿”å›False

    # åˆ›å»ºæ¡ä»¶é“¾
    conditional_chain = (
        RunnablePassthrough.assign(
            is_technical=lambda x: is_technical_question(x["question"])
        )
        | RunnableBranch(
            (lambda x: x["is_technical"], technical_prompt | llm | StrOutputParser()),
            (lambda x: not x["is_technical"], simple_prompt | llm | StrOutputParser()),
        )
    )

    questions = [
        {"question": "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ"},
        {"question": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"},
        {"question": "å¦‚ä½•å®ç°å¿«é€Ÿæ’åºç®—æ³•ï¼Ÿ"},
        {"question": "ä½ æœ€å–œæ¬¢çš„é¢œè‰²æ˜¯ä»€ä¹ˆï¼Ÿ"}
    ]

    for item in questions:
        try:
            question = item["question"]
            print(f"é—®é¢˜: {question}")

            # åˆ¤æ–­æ˜¯å¦ä¸ºæŠ€æœ¯é—®é¢˜
            is_tech = is_technical_question(question)
            print(f"ç±»å‹: {'æŠ€æœ¯æ€§é—®é¢˜' if is_tech else 'ä¸€èˆ¬æ€§é—®é¢˜'}")

            # è·å–å›ç­”
            answer = conditional_chain.invoke({"question": question})
            print(f"å›ç­”: {answer}\n")

        except Exception as e:
            print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")

def sequential_chain_example():
    """é¡ºåºé“¾ç¤ºä¾‹ - å‰ä¸€ä¸ªæ­¥éª¤çš„è¾“å‡ºä½œä¸ºåä¸€ä¸ªæ­¥éª¤çš„è¾“å…¥"""
    print("=== é¡ºåºé“¾ç¤ºä¾‹ ===\n")

    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # ç¬¬ä¸€æ­¥ï¼šæ•…äº‹æ¦‚è¦ç”Ÿæˆ
    story_prompt = PromptTemplate(
        input_variables=["character"],
        template="åˆ›å»ºä¸€ä¸ªå…³äº{character}çš„æ•…äº‹æ¦‚è¦ï¼Œå¤§çº¦100å­—ã€‚"
    )

    # ç¬¬äºŒæ­¥ï¼šåŸºäºæ¦‚è¦ç”Ÿæˆæ•…äº‹æ ‡é¢˜
    title_prompt = PromptTemplate(
        input_variables=["story_summary"],
        template="åŸºäºä»¥ä¸‹æ•…äº‹æ¦‚è¦ï¼Œåˆ›ä½œä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜ï¼š\n{story_summary}\n\næ ‡é¢˜ï¼š"
    )

    # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆæ•…äº‹ç»“å±€
    ending_prompt = PromptTemplate(
        input_variables=["title", "story_summary"],
        template="æ•…äº‹æ ‡é¢˜ï¼š{title}\næ•…äº‹æ¦‚è¦ï¼š{story_summary}\n\nè¯·ä¸ºè¿™ä¸ªæ•…äº‹å†™ä¸€ä¸ªç²¾å½©çš„ç»“å°¾ï¼Œå¤§çº¦50å­—ï¼š"
    )

    # åˆ›å»ºé¡ºåºé“¾
    story_chain = (
        {"character": RunnablePassthrough()}
        | story_prompt
        | llm
        | StrOutputParser()
        | (lambda story: {"story_summary": story})
    )

    full_chain = (
        story_chain
        | RunnablePassthrough.assign(
            title=lambda x: (title_prompt | llm | StrOutputParser()).invoke({"story_summary": x["story_summary"]})
        )
        | RunnablePassthrough.assign(
            ending=lambda x: (ending_prompt | llm | StrOutputParser()).invoke({
                "title": x["title"],
                "story_summary": x["story_summary"]
            })
        )
    )

    character = "ä¸€ä¸ªä¼šè¯´è¯çš„çŒ«å’ª"

    try:
        result = full_chain.invoke(character)

        print(f"è§’è‰²: {character}\n")
        print(f"æ•…äº‹æ¦‚è¦: {result['story_summary']}\n")
        print(f"æ•…äº‹æ ‡é¢˜: {result['title']}\n")
        print(f"æ•…äº‹ç»“å°¾: {result['ending']}\n")

        print("å®Œæ•´æ•…äº‹:")
        print(f"æ ‡é¢˜: {result['title']}")
        print(f"æ¦‚è¦: {result['story_summary']}")
        print(f"ç»“å°¾: {result['ending']}\n")

    except Exception as e:
        print(f"ç”Ÿæˆæ•…äº‹æ—¶å‡ºé”™: {e}")

def json_output_chain_example():
    """JSONè¾“å‡ºé“¾ç¤ºä¾‹ - ç»“æ„åŒ–è¾“å‡º"""
    print("=== JSONè¾“å‡ºé“¾ç¤ºä¾‹ ===\n")

    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„JSONè¾“å‡º
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # åˆ›å»ºJSONè¾“å‡ºè§£æå™¨
    json_parser = JsonOutputParser()

    # åˆ›å»ºæç¤ºæ¨¡æ¿ï¼Œè¦æ±‚JSONæ ¼å¼è¾“å‡º
    json_prompt = PromptTemplate(
        input_variables=["question"],
        template="""è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœã€‚
è¦æ±‚ï¼š
1. answer: ç›´æ¥å›ç­”é—®é¢˜
2. confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ä¹‹é—´çš„æ•°å­—ï¼‰
3. sources: ç›¸å…³ä¿¡æ¯æ¥æºï¼ˆåˆ—è¡¨ï¼‰

é—®é¢˜ï¼š{question}

è¯·åªè¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ï¼š"""
    )

    # åˆ›å»ºJSONè¾“å‡ºé“¾
    json_chain = json_prompt | llm | json_parser

    questions = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "Pythonæœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ",
        "å¦‚ä½•æé«˜ç¼–ç¨‹æŠ€èƒ½ï¼Ÿ"
    ]

    for question in questions:
        try:
            result = json_chain.invoke({"question": question})
            print(f"é—®é¢˜: {question}")
            print("JSONå›ç­”:")
            for key, value in result.items():
                print(f"  {key}: {value}")
            print()
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")

def custom_function_chain_example():
    """è‡ªå®šä¹‰å‡½æ•°é“¾ç¤ºä¾‹ - åœ¨é“¾ä¸­ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°"""
    print("=== è‡ªå®šä¹‰å‡½æ•°é“¾ç¤ºä¾‹ ===\n")

    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # è‡ªå®šä¹‰å‡½æ•°ï¼šæ–‡æœ¬é¢„å¤„ç†
    def preprocess_text(text):
        """é¢„å¤„ç†æ–‡æœ¬ï¼šæ¸…ç†æ ¼å¼ã€è½¬æ¢å¤§å°å†™ç­‰"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        cleaned = ' '.join(text.split())
        # è½¬æ¢ä¸ºå°å†™ï¼ˆæŸäº›ä»»åŠ¡å¯èƒ½éœ€è¦ï¼‰
        # cleaned = cleaned.lower()
        return cleaned

    # è‡ªå®šä¹‰å‡½æ•°ï¼šåå¤„ç†ç»“æœ
    def format_output(output):
        """æ ¼å¼åŒ–è¾“å‡ºç»“æœ"""
        return f"ğŸ“ **å¤„ç†ç»“æœ**ï¼š\n{output}\n\nâœ¨ å¤„ç†å®Œæˆï¼"

    # è‡ªå®šä¹‰å‡½æ•°ï¼šæ–‡æœ¬åˆ†æ
    def analyze_text(text):
        """åˆ†ææ–‡æœ¬ç‰¹å¾"""
        word_count = len(text.split())
        char_count = len(text)
        return {
            "word_count": word_count,
            "char_count": char_count,
            "complexity": "é«˜" if word_count > 50 else "ä¸­" if word_count > 20 else "ä½"
        }

    # åˆ›å»ºæç¤ºæ¨¡æ¿
    analysis_prompt = PromptTemplate(
        input_variables=["text", "analysis"],
        template="""æ–‡æœ¬åˆ†æç»“æœï¼š
- å­—æ•°ï¼š{word_count}
- å­—ç¬¦æ•°ï¼š{char_count}
- å¤æ‚åº¦ï¼š{complexity}

è¯·åŸºäºä»¥ä¸Šåˆ†æï¼Œå¯¹è¿™ä¸ªæ–‡æœ¬æä¾›è¯¦ç»†çš„è§è§£ï¼š
{text}

è§è§£ï¼š"""
    )

    # åˆ›å»ºåŒ…å«è‡ªå®šä¹‰å‡½æ•°çš„é“¾
    custom_chain = (
        RunnablePassthrough.assign(
            cleaned_text=lambda x: preprocess_text(x["text"]),
            analysis=lambda x: analyze_text(x["text"])
        )
        | RunnablePassthrough.assign(
            insights=lambda x: (analysis_prompt | llm | StrOutputParser()).invoke({
                "text": x["cleaned_text"],
                "word_count": x["analysis"]["word_count"],
                "char_count": x["analysis"]["char_count"],
                "complexity": x["analysis"]["complexity"]
            })
        )
        | RunnableLambda(format_output)
    )

    test_text = """
    LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œå®ƒå¯ä»¥å¸®åŠ©å¼€å‘äººå‘˜æ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºã€‚
    é€šè¿‡é“¾å¼è°ƒç”¨ï¼Œå¯ä»¥ç»„åˆå¤šä¸ªæ­¥éª¤æ¥å®Œæˆå¤æ‚çš„ä»»åŠ¡ã€‚
    """

    try:
        result = custom_chain.invoke({"text": test_text})
        print(f"åŸå§‹æ–‡æœ¬: {test_text}")
        print("\nå¤„ç†ç»“æœ:")
        print(result)
    except Exception as e:
        print(f"è‡ªå®šä¹‰å‡½æ•°é“¾å¤„ç†æ—¶å‡ºé”™: {e}")

def chat_template_chain_example():
    """èŠå¤©æ¨¡æ¿é“¾ç¤ºä¾‹ - ä½¿ç”¨ChatPromptTemplate"""
    print("=== èŠå¤©æ¨¡æ¿é“¾ç¤ºä¾‹ ===\n")

    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}åŠ©æ‰‹ã€‚å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€ç®€æ´ã€‚"),
        ("human", "{question}")
    ])

    # åˆ›å»ºèŠå¤©é“¾
    chat_chain = chat_prompt | llm | StrOutputParser()

    # æµ‹è¯•ä¸åŒè§’è‰²çš„åŠ©æ‰‹
    scenarios = [
        {"role": "Pythonç¼–ç¨‹", "question": "ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"},
        {"role": "æ•°æ®ç§‘å­¦", "question": "æœºå™¨å­¦ä¹ çš„ä¸»è¦ç±»å‹æœ‰å“ªäº›ï¼Ÿ"},
        {"role": "å¿ƒç†å’¨è¯¢", "question": "å¦‚ä½•ç®¡ç†å·¥ä½œå‹åŠ›ï¼Ÿ"}
    ]

    for scenario in scenarios:
        try:
            result = chat_chain.invoke(scenario)
            print(f"è§’è‰²: {scenario['role']} åŠ©æ‰‹")
            print(f"é—®é¢˜: {scenario['question']}")
            print(f"å›ç­”: {result}\n")
        except Exception as e:
            print(f"å¤„ç†èŠå¤©æ¨¡æ¿æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    print("ğŸ”— æ¬¢è¿æ¥åˆ°LangChainç°ä»£é“¾å¼è°ƒç”¨å­¦ä¹ ä¸–ç•Œï¼\n")

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("GLM_API_KEY") or not os.getenv("GLM_BASE_URL"):
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°GLM_API_KEYæˆ–GLM_BASE_URLç¯å¢ƒå˜é‡")
        print("è¯·ç¡®ä¿åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®äº†æ­£ç¡®çš„æ™ºè°±AI APIé…ç½®")
        print()
        

    # è¿è¡ŒåŸºç¡€LCELé“¾ç¤ºä¾‹
    print("ğŸ¯ 1. åŸºç¡€LCELé“¾")
    #basic_lcel_chain()

    print("\n" + "="*60 + "\n")

    # è¿è¡Œå¹¶è¡Œé“¾ç¤ºä¾‹
    print("ğŸ¯ 2. å¹¶è¡Œé“¾")
    #parallel_chain_example()

    print("\n" + "="*60 + "\n")

    # è¿è¡Œæ¡ä»¶é“¾ç¤ºä¾‹
    print("ğŸ¯ 3. æ¡ä»¶é“¾")
    #conditional_chain_example()

    print("\n" + "="*60 + "\n")

    # è¿è¡Œé¡ºåºé“¾ç¤ºä¾‹
    print("ğŸ¯ 4. é¡ºåºé“¾")
    #sequential_chain_example()

    print("\n" + "="*60 + "\n")

    # è¿è¡ŒJSONè¾“å‡ºé“¾ç¤ºä¾‹
    print("ğŸ¯ 5. JSONè¾“å‡ºé“¾")
    #json_output_chain_example()

    print("\n" + "="*60 + "\n")

    # è¿è¡Œè‡ªå®šä¹‰å‡½æ•°é“¾ç¤ºä¾‹
    print("ğŸ¯ 6. è‡ªå®šä¹‰å‡½æ•°é“¾")
    #custom_function_chain_example()

    print("\n" + "="*60 + "\n")

    # è¿è¡ŒèŠå¤©æ¨¡æ¿é“¾ç¤ºä¾‹
    print("ğŸ¯ 7. èŠå¤©æ¨¡æ¿é“¾")
    chat_template_chain_example()

    print("\nâœ¨ ç°ä»£é“¾å¼è°ƒç”¨ç¤ºä¾‹å®Œæˆï¼")
    print("æ‚¨å·²ç»å­¦ä¼šäº†å¦‚ä½•ä½¿ç”¨æœ€æ–°çš„LCELè¯­æ³•æ„å»ºå„ç§ç±»å‹çš„é“¾ã€‚")
    print()
    print("ğŸ“š LCELä¸»è¦ç‰¹æ€§:")
    print("  â€¢ ç»Ÿä¸€æ¥å£: æ‰€æœ‰ç»„ä»¶éƒ½ä½¿ç”¨ç›¸åŒçš„ invoke æ–¹æ³•")
    print("  â€¢ åŸç”Ÿæµå¼: æ”¯æŒæµå¼è¾“å‡º")
    print("  â€¢ å¼‚æ­¥æ”¯æŒ: åŸç”Ÿæ”¯æŒå¼‚æ­¥æ“ä½œ")
    print("  â€¢ æ‰¹å¤„ç†: æ”¯æŒæ‰¹é‡å¤„ç†")
    print("  â€¢ ç»„åˆæ€§: ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ | ç»„åˆç»„ä»¶")
    print("  â€¢ å¹¶è¡ŒåŒ–: ä½¿ç”¨ RunnableParallel å®ç°å¹¶è¡Œæ‰§è¡Œ")
    print("  â€¢ æ¡ä»¶é€»è¾‘: ä½¿ç”¨ RunnableBranch å®ç°æ¡ä»¶åˆ†æ”¯")
    print("  â€¢ å›é€€æœºåˆ¶: æ”¯æŒå›é€€å’Œé”™è¯¯å¤„ç†")