#!/usr/bin/env python3
"""
Embeddingså­˜å‚¨åŸç†å’Œæ•°æ®ç»“æ„è¯¦è§£
"""

import os
import pickle
import json
import sqlite3
import numpy as np
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS, Chroma
from langchain_core.documents import Document

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ========================
# 1. Embeddingsæœ¬è´¨æ¼”ç¤º
# ========================

def embeddings_basic_demo():
    """æ¼”ç¤ºEmbeddingsçš„æœ¬è´¨"""
    print("=" * 60)
    print("ğŸ”¢ Embeddingsæœ¬è´¨æ¼”ç¤º")
    print("=" * 60)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    texts = [
        "äººå·¥æ™ºèƒ½",
        "æœºå™¨å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ ",
        "ç¥ç»ç½‘ç»œ",
        "è‡ªç„¶è¯­è¨€å¤„ç†"
    ]

    print(f"\nğŸ“ å¤„ç†æ–‡æœ¬: {texts}")

    # ç”Ÿæˆembeddings
    print("\nğŸ”„ ç”Ÿæˆembeddings...")
    embedding_vectors = embeddings.embed_documents(texts)

    # å±•ç¤ºembeddingå‘é‡çš„ç‰¹ç‚¹
    print(f"\nğŸ“Š Embeddingå‘é‡ä¿¡æ¯:")
    print(f"âœ… ç”Ÿæˆäº† {len(embedding_vectors)} ä¸ªå‘é‡")
    print(f"ğŸ“ æ¯ä¸ªå‘é‡çš„ç»´åº¦: {len(embedding_vectors[0])}")
    print(f"ğŸ”¢ æ•°æ®ç±»å‹: {type(embedding_vectors[0])}")
    print(f"ğŸ“ å‘é‡ç¤ºä¾‹(å‰10ç»´): {embedding_vectors[0][:10]}")

    # ä¿å­˜åŸå§‹å‘é‡æ•°æ®åˆ°æ–‡ä»¶
    print(f"\nğŸ’¾ ä¿å­˜åŸå§‹å‘é‡æ•°æ®...")

    # ä¿å­˜ä¸ºnumpyæ ¼å¼
    vectors_array = np.array(embedding_vectors)
    np.save("text_vectors.npy", vectors_array)
    print(f"âœ… ä¿å­˜ä¸ºnpyæ ¼å¼: text_vectors.npy")

    # ä¿å­˜ä¸ºpickleæ ¼å¼
    with open("text_vectors.pkl", "wb") as f:
        pickle.dump({
            "texts": texts,
            "vectors": embedding_vectors
        }, f)
    print(f"âœ… ä¿å­˜ä¸ºpickleæ ¼å¼: text_vectors.pkl")

    return texts, embedding_vectors

# ========================
# 2. å‘é‡æ•°æ®åº“ç»“æ„æ¼”ç¤º
# ========================

def vector_database_structure_demo():
    """æ¼”ç¤ºå‘é‡æ•°æ®åº“çš„å†…éƒ¨ç»“æ„"""
    print("\n" + "=" * 60)
    print("ğŸ—„ï¸ å‘é‡æ•°æ®åº“ç»“æ„æ¼”ç¤º")
    print("=" * 60)

    embeddings = OpenAIEmbeddings(
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    # åˆ›å»ºæ–‡æ¡£
    documents = [
        Document(
            page_content="äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
            metadata={"id": 1, "category": "AIåŸºç¡€", "source": "textbook"}
        ),
        Document(
            page_content="æœºå™¨å­¦ä¹ ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ",
            metadata={"id": 2, "category": "ML", "source": "paper"}
        ),
        Document(
            page_content="æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ",
            metadata={"id": 3, "category": "DL", "source": "article"}
        )
    ]

    print(f"\nğŸ“ å‡†å¤‡æ–‡æ¡£: {len(documents)} ä¸ª")

    # ç”Ÿæˆå‘é‡
    print("\nğŸ”„ ç”Ÿæˆæ–‡æ¡£å‘é‡...")
    vectors = embeddings.embed_documents([doc.page_content for doc in documents])

    # å±•ç¤ºå‘é‡æ•°æ®åº“çš„ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶
    print(f"\nğŸ—ï¸ å‘é‡æ•°æ®åº“çš„æ ¸å¿ƒç»„ä»¶:")

    # 1. å‘é‡ç´¢å¼•
    print(f"\n1ï¸âƒ£ å‘é‡ç´¢å¼• (Vector Index):")
    print(f"   ğŸ“Š å­˜å‚¨å†…å®¹: {len(vectors)} ä¸ª {len(vectors[0])} ç»´å‘é‡")
    print(f"   ğŸ¯ ç”¨é€”: å¿«é€Ÿç›¸ä¼¼æ€§æœç´¢")
    print(f"   ğŸ“ æ–‡ä»¶: FAISSåˆ›å»º .index æ–‡ä»¶")

    # 2. æ–‡æ¡£å­˜å‚¨
    print(f"\n2ï¸âƒ£ æ–‡æ¡£å­˜å‚¨ (Document Store):")
    print(f"   ğŸ“„ å­˜å‚¨å†…å®¹:")
    for i, doc in enumerate(documents):
        print(f"      ID {doc.metadata['id']}: {doc.page_content[:30]}...")
    print(f"   ğŸ¯ ç”¨é€”: å­˜å‚¨åŸå§‹æ–‡æœ¬å†…å®¹")
    print(f"   ğŸ“ æ–‡ä»¶: é€šå¸¸å­˜å‚¨ä¸º pickle æˆ– JSON")

    # 3. IDæ˜ å°„
    print(f"\n3ï¸âƒ£ IDæ˜ å°„ (ID Mapping):")
    print(f"   ğŸ”— æ˜ å°„å…³ç³»:")
    for i, doc in enumerate(documents):
        print(f"      å‘é‡ç´¢å¼• {i} â†’ æ–‡æ¡£ID {doc.metadata['id']}")
    print(f"   ğŸ¯ ç”¨é€”: è¿æ¥å‘é‡å’Œæ–‡æ¡£")
    print(f"   ğŸ“ æ–‡ä»¶: å†…éƒ¨ç´¢å¼•æ–‡ä»¶")

    return documents, vectors

# ========================
# 3. ä¸åŒå­˜å‚¨æ ¼å¼æ¼”ç¤º
# ========================

def storage_formats_demo(documents, vectors):
    """æ¼”ç¤ºä¸åŒçš„å­˜å‚¨æ ¼å¼"""
    print("\n" + "=" * 60)
    print("ğŸ’¾ ä¸åŒå­˜å‚¨æ ¼å¼æ¼”ç¤º")
    print("=" * 60)

    # æ ¼å¼1: FAISS (å†…å­˜ + æ–‡ä»¶)
    print(f"\n1ï¸âƒ£ FAISS å­˜å‚¨æ ¼å¼:")
    embeddings = OpenAIEmbeddings(
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    faiss_store = FAISS.from_documents(documents, embeddings)
    faiss_store.save_local("faiss_demo")

    print(f"   ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    for file in os.listdir("."):
        if file.startswith("faiss_demo"):
            print(f"      - {file}")

    print(f"   ğŸ¯ ç‰¹ç‚¹:")
    print(f"      - å†…å­˜æ•°æ®åº“ï¼Œæ”¯æŒæŒä¹…åŒ–")
    print(f"      - é«˜æ€§èƒ½å‘é‡æœç´¢")
    print(f"      - é€‚åˆå¤§è§„æ¨¡æ•°æ®")

    # æ ¼å¼2: Chroma (æ•°æ®åº“)
    print(f"\n2ï¸âƒ£ Chroma å­˜å‚¨æ ¼å¼:")
    chroma_store = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        collection_name="demo_collection",
        persist_directory="./chroma_demo"
    )

    print(f"   ğŸ“ ç”Ÿæˆçš„ç›®å½•ç»“æ„:")
    if os.path.exists("./chroma_demo"):
        for root, dirs, files in os.walk("./chroma_demo"):
            level = root.replace("./chroma_demo", "").count(os.sep)
            indent = " " * 2 * level
            print(f"{indent}{os.path.basename(root)}/")
            subindent = " " * 2 * (level + 1)
            for file in files:
                print(f"{subindent}{file}")

    print(f"   ğŸ¯ ç‰¹ç‚¹:")
    print(f"      - çœŸæ­£çš„æ•°æ®åº“ç³»ç»Ÿ")
    print(f"      - æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤")
    print(f"      - æŒä¹…åŒ–å­˜å‚¨")

    # æ ¼å¼3: SQLite (ä¼ ç»Ÿæ•°æ®åº“)
    print(f"\n3ï¸âƒ£ SQLite è‡ªå®šä¹‰å­˜å‚¨:")

    # åˆ›å»ºè‡ªå®šä¹‰æ•°æ®åº“
    conn = sqlite3.connect("vectors.db")
    cursor = conn.cursor()

    # åˆ›å»ºè¡¨ç»“æ„
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS documents (
        id INTEGER PRIMARY KEY,
        content TEXT,
        metadata TEXT,
        vector BLOB
    )
    ''')

    # æ’å…¥æ•°æ®
    for i, (doc, vector) in enumerate(zip(documents, vectors)):
        cursor.execute('''
        INSERT INTO documents (id, content, metadata, vector)
        VALUES (?, ?, ?, ?)
        ''', (
            doc.metadata['id'],
            doc.page_content,
            json.dumps(doc.metadata),
            pickle.dumps(vector)
        ))

    conn.commit()
    print(f"   ğŸ“ ç”Ÿæˆæ–‡ä»¶: vectors.db")
    print(f"   ğŸ“Š å­˜å‚¨äº† {len(documents)} æ¡è®°å½•")

    # æŸ¥è¯¢æ¼”ç¤º
    cursor.execute("SELECT id, content FROM documents")
    records = cursor.fetchall()
    print(f"   ğŸ” æŸ¥è¯¢ç»“æœç¤ºä¾‹:")
    for record in records:
        print(f"      ID {record[0]}: {record[1][:30]}...")

    conn.close()

    return faiss_store, chroma_store

# ========================
# 4. æœç´¢åŸç†æ¼”ç¤º
# ========================

def search_principle_demo(faiss_store, chroma_store):
    """æ¼”ç¤ºå‘é‡æœç´¢çš„åŸç†"""
    print("\n" + "=" * 60)
    print("ğŸ” å‘é‡æœç´¢åŸç†æ¼”ç¤º")
    print("=" * 60)

    embeddings = OpenAIEmbeddings(
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    # æœç´¢æŸ¥è¯¢
    query = "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ"
    print(f"â“ æœç´¢æŸ¥è¯¢: {query}")

    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    print(f"\nğŸ”„ ç”ŸæˆæŸ¥è¯¢å‘é‡...")
    query_vector = embeddings.embed_query(query)
    print(f"ğŸ“ æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_vector)}")

    # FAISSæœç´¢
    print(f"\nğŸš€ FAISSç›¸ä¼¼æ€§æœç´¢:")
    faiss_results = faiss_store.similarity_search_with_score(query, k=2)
    print(f"   æ‰¾åˆ° {len(faiss_results)} ä¸ªç›¸ä¼¼æ–‡æ¡£:")
    for i, (doc, score) in enumerate(faiss_results, 1):
        print(f"      {i}. {doc.page_content}")
        print(f"         ç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}")
        print(f"         å…ƒæ•°æ®: {doc.metadata}")

    # Chromaæœç´¢
    print(f"\nğŸ—„ï¸ Chromaç›¸ä¼¼æ€§æœç´¢:")
    chroma_results = chroma_store.similarity_search_with_score(query, k=2)
    print(f"   æ‰¾åˆ° {len(chroma_results)} ä¸ªç›¸ä¼¼æ–‡æ¡£:")
    for i, (doc, score) in enumerate(chroma_results, 1):
        print(f"      {i}. {doc.page_content}")
        print(f"         è·ç¦»åˆ†æ•°: {score:.4f}")
        print(f"         å…ƒæ•°æ®: {doc.metadata}")

    # æ‰‹åŠ¨è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ¼”ç¤ºåŸç†ï¼‰
    print(f"\nğŸ§® æ‰‹åŠ¨è®¡ç®—ç›¸ä¼¼åº¦åŸç†:")

    # è·å–æ‰€æœ‰æ–‡æ¡£å‘é‡
    conn = sqlite3.connect("vectors.db")
    cursor = conn.cursor()
    cursor.execute("SELECT id, content, vector FROM documents")
    db_records = cursor.fetchall()
    conn.close()

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    def cosine_similarity(vec1, vec2):
        vec1, vec2 = np.array(vec1), np.array(vec2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

    similarities = []
    for doc_id, content, vector_blob in db_records:
        stored_vector = pickle.loads(vector_blob)
        similarity = cosine_similarity(query_vector, stored_vector)
        similarities.append((doc_id, content, similarity))

    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[2], reverse=True)

    print(f"   ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ:")
    for i, (doc_id, content, similarity) in enumerate(similarities[:2], 1):
        print(f"      {i}. ID {doc_id}: {content[:30]}...")
        print(f"         ä½™å¼¦ç›¸ä¼¼åº¦: {similarity:.4f}")

# ========================
# 5. æ€§èƒ½å¯¹æ¯”æ¼”ç¤º
# ========================

def performance_comparison_demo():
    """æ¼”ç¤ºä¸åŒå­˜å‚¨æ–¹å¼çš„æ€§èƒ½å¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("âš¡ å­˜å‚¨æ–¹å¼æ€§èƒ½å¯¹æ¯”")
    print("=" * 60)

    import time

    embeddings = OpenAIEmbeddings(
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
    print(f"\nğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®...")
    test_docs = [
        Document(
            page_content=f"è¿™æ˜¯ç¬¬{i}ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œå†…å®¹æ˜¯å…³äºAIæŠ€æœ¯çš„ç¬¬{i}ä¸ªæ–¹é¢",
            metadata={"id": i, "category": f"category_{i % 5}"}
        )
        for i in range(1000)
    ]

    print(f"   ç”Ÿæˆäº† {len(test_docs)} ä¸ªæµ‹è¯•æ–‡æ¡£")

    # æµ‹è¯•ä¸åŒå­˜å‚¨æ–¹å¼çš„æ€§èƒ½
    storage_methods = {
        "FAISS": lambda: FAISS.from_documents(test_docs[:100], embeddings),  # é™åˆ¶æ•°é‡ä»¥èŠ‚çœæ—¶é—´
        "Chroma": lambda: Chroma.from_documents(
            test_docs[:100], embeddings,
            collection_name="perf_test",
            persist_directory="./perf_chroma"
        ),
        "SQLite": lambda: create_sqlite_store(test_docs[:100], embeddings)
    }

    results = {}

    for method_name, create_func in storage_methods.items():
        print(f"\nğŸ”§ æµ‹è¯• {method_name} å­˜å‚¨...")

        # å­˜å‚¨æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        try:
            store = create_func()
            storage_time = time.time() - start_time

            # æœç´¢æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            if hasattr(store, 'similarity_search'):
                _ = store.similarity_search("AIæŠ€æœ¯", k=5)
            search_time = time.time() - start_time

            results[method_name] = {
                "storage_time": storage_time,
                "search_time": search_time,
                "success": True
            }
            print(f"   âœ… å­˜å‚¨æ—¶é—´: {storage_time:.2f}ç§’")
            print(f"   âœ… æœç´¢æ—¶é—´: {search_time:.2f}ç§’")

        except Exception as e:
            results[method_name] = {
                "error": str(e),
                "success": False
            }
            print(f"   âŒ æµ‹è¯•å¤±è´¥: {e}")

    # æ€§èƒ½æ€»ç»“
    print(f"\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ€»ç»“:")
    print(f"{'æ–¹æ³•':<10} {'å­˜å‚¨æ—¶é—´':<12} {'æœç´¢æ—¶é—´':<12} {'çŠ¶æ€':<8}")
    print("-" * 50)
    for method, result in results.items():
        if result["success"]:
            print(f"{method:<10} {result['storage_time']:<12.2f} {result['search_time']:<12.4f} {'æˆåŠŸ':<8}")
        else:
            print(f"{method:<10} {'N/A':<12} {'N/A':<12} {'å¤±è´¥':<8}")

def create_sqlite_store(documents, embeddings):
    """åˆ›å»ºSQLiteå‘é‡å­˜å‚¨"""
    conn = sqlite3.connect(":memory:")  # å†…å­˜æ•°æ®åº“
    cursor = conn.cursor()

    cursor.execute('''
    CREATE TABLE documents (
        id INTEGER PRIMARY KEY,
        content TEXT,
        metadata TEXT,
        vector BLOB
    )
    ''')

    vectors = embeddings.embed_documents([doc.page_content for doc in documents])

    for i, (doc, vector) in enumerate(zip(documents, vectors)):
        cursor.execute('''
        INSERT INTO documents (id, content, metadata, vector)
        VALUES (?, ?, ?, ?)
        ''', (
            doc.metadata['id'],
            doc.page_content,
            json.dumps(doc.metadata),
            pickle.dumps(vector)
        ))

    conn.commit()
    return conn

# ========================
# 6. å®é™…åº”ç”¨åœºæ™¯æ¼”ç¤º
# ========================

def real_world_scenarios_demo():
    """æ¼”ç¤ºå®é™…åº”ç”¨åœºæ™¯"""
    print("\n" + "=" * 60)
    print("ğŸŒ å®é™…åº”ç”¨åœºæ™¯æ¼”ç¤º")
    print("=" * 60)

    embeddings = OpenAIEmbeddings(
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    # åœºæ™¯1: æ–‡æ¡£çŸ¥è¯†åº“
    print(f"\nğŸ“š åœºæ™¯1: æ–‡æ¡£çŸ¥è¯†åº“")
    knowledge_docs = [
        Document(page_content="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´æ˜“å­¦",
                metadata={"type": "ç¼–ç¨‹è¯­è¨€", "difficulty": "åˆçº§"}),
        Document(page_content="æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥ä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æ¨¡å¼",
                metadata={"type": "AIæŠ€æœ¯", "difficulty": "é«˜çº§"}),
        Document(page_content="æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œè¿›è¡Œç‰¹å¾å­¦ä¹ ",
                metadata={"type": "AIæŠ€æœ¯", "difficulty": "é«˜çº§"}),
        Document(page_content="SQLæ˜¯ç”¨äºç®¡ç†å…³ç³»æ•°æ®åº“çš„æ ‡å‡†è¯­è¨€",
                metadata={"type": "æ•°æ®åº“", "difficulty": "ä¸­çº§"}),
    ]

    knowledge_store = FAISS.from_documents(knowledge_docs, embeddings)

    # æ¨¡æ‹ŸçŸ¥è¯†åº“æŸ¥è¯¢
    questions = [
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æ•°æ®åº“æ“ä½œè¯­è¨€"
    ]

    for question in questions:
        print(f"\nâ“ é—®é¢˜: {question}")
        results = knowledge_store.similarity_search(question, k=2)
        for i, doc in enumerate(results, 1):
            print(f"   ğŸ“„ {i}. {doc.page_content}")
            print(f"      ğŸ·ï¸ æ ‡ç­¾: {doc.metadata}")

    # åœºæ™¯2: äº§å“æ¨èç³»ç»Ÿ
    print(f"\nğŸ›’ åœºæ™¯2: äº§å“æ¨èç³»ç»Ÿ")
    products = [
        Document(page_content="iPhone 15 Pro - æœ€æ–°æ¬¾è‹¹æœæ‰‹æœºï¼Œé’›é‡‘å±è®¾è®¡",
                metadata={"category": "æ‰‹æœº", "brand": "Apple", "price": "é«˜ç«¯"}),
        Document(page_content="MacBook Pro M3 - ä¸“ä¸šç¬”è®°æœ¬ç”µè„‘ï¼Œæ€§èƒ½å¼ºåŠ²",
                metadata={"category": "ç¬”è®°æœ¬", "brand": "Apple", "price": "é«˜ç«¯"}),
        Document(page_content="å°ç±³14 - æ€§ä»·æ¯”é«˜çš„å›½äº§æ——èˆ°æ‰‹æœº",
                metadata={"category": "æ‰‹æœº", "brand": "Xiaomi", "price": "ä¸­ç«¯"}),
        Document(page_content="ThinkPad X1 - å•†åŠ¡åŠå…¬ç¬”è®°æœ¬ï¼Œé”®ç›˜æ‰‹æ„Ÿå¥½",
                metadata={"category": "ç¬”è®°æœ¬", "brand": "Lenovo", "price": "é«˜ç«¯"}),
    ]

    product_store = FAISS.from_documents(products, embeddings)

    # æ¨¡æ‹Ÿç”¨æˆ·æŸ¥è¯¢
    user_queries = [
        "æƒ³è¦ä¸€éƒ¨æ‹ç…§å¥½çš„æ‰‹æœº",
        "åŠå…¬ç”¨çš„ç¬”è®°æœ¬ç”µè„‘",
        "è‹¹æœå…¬å¸çš„äº§å“"
    ]

    for query in user_queries:
        print(f"\nğŸ” ç”¨æˆ·æŸ¥è¯¢: {query}")
        results = product_store.similarity_search(query, k=2)
        for i, doc in enumerate(results, 1):
            print(f"   ğŸ›ï¸ {i}. {doc.page_content}")
            print(f"      ğŸ’° ä»·æ ¼æ®µ: {doc.metadata['price']}")

# ========================
# æ¸…ç†å‡½æ•°
# ========================

def cleanup_demo_files():
    """æ¸…ç†æ¼”ç¤ºç”Ÿæˆçš„æ–‡ä»¶"""
    print(f"\nğŸ§¹ æ¸…ç†æ¼”ç¤ºæ–‡ä»¶...")

    files_to_remove = [
        "text_vectors.npy", "text_vectors.pkl",
        "faiss_demo.index", "vectors.db"
    ]

    import shutil
    dirs_to_remove = ["chroma_demo", "perf_chroma"]

    for file in files_to_remove:
        if os.path.exists(file):
            os.remove(file)
            print(f"   ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {file}")

    for dir_name in dirs_to_remove:
        if os.path.exists(dir_name):
            shutil.rmtree(dir_name)
            print(f"   ğŸ—‘ï¸ åˆ é™¤ç›®å½•: {dir_name}")

    print("âœ… æ¸…ç†å®Œæˆ")

# ========================
# ä¸»å‡½æ•°
# ========================

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Embeddingså­˜å‚¨åŸç†å’Œæ•°æ®ç»“æ„è¯¦è§£")
    print("=" * 80)

    try:
        # 1. Embeddingsæœ¬è´¨
        texts, vectors = embeddings_basic_demo()

        # 2. å‘é‡æ•°æ®åº“ç»“æ„
        documents, doc_vectors = vector_database_structure_demo()

        # 3. ä¸åŒå­˜å‚¨æ ¼å¼
        faiss_store, chroma_store = storage_formats_demo(documents, doc_vectors)

        # 4. æœç´¢åŸç†
        search_principle_demo(faiss_store, chroma_store)

        # 5. æ€§èƒ½å¯¹æ¯”
        performance_comparison_demo()

        # 6. å®é™…åº”ç”¨åœºæ™¯
        real_world_scenarios_demo()

        print("\n" + "=" * 80)
        print("ğŸ‰ Embeddingså­˜å‚¨åŸç†æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 80)

        print("\nğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“:")
        print("1. ğŸ”¢ Embeddingsç”Ÿæˆçš„æ˜¯æ•°å€¼å‘é‡ï¼Œä¸æ˜¯æ•°æ®åº“æ–‡ä»¶")
        print("2. ğŸ—„ï¸ å‘é‡æ•°æ®åº“å­˜å‚¨ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šå‘é‡ç´¢å¼•ã€æ–‡æ¡£å†…å®¹ã€IDæ˜ å°„")
        print("3. ğŸš€ ä¸åŒå­˜å‚¨æ–¹å¼æœ‰ä¸åŒç‰¹ç‚¹ï¼šFAISS(å¿«)ã€Chroma(åŠŸèƒ½å…¨)ã€SQLite(çµæ´»)")
        print("4. ğŸ” æœç´¢åŸç†ï¼šè®¡ç®—æŸ¥è¯¢å‘é‡ä¸å­˜å‚¨å‘é‡çš„ç›¸ä¼¼åº¦")
        print("5. âš¡ é€‰æ‹©å­˜å‚¨æ–¹å¼è¦è€ƒè™‘æ•°æ®è§„æ¨¡ã€æŸ¥è¯¢é¢‘ç‡ã€åŠŸèƒ½éœ€æ±‚")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")

    finally:
        # æ¸…ç†æ–‡ä»¶
        cleanup_demo_files()

if __name__ == "__main__":
    main()