#!/usr/bin/env python3
"""
ChatOpenAI å®Œæ•´å‚æ•°å‚è€ƒæŒ‡å—
è¿™ä¸ªæ–‡ä»¶åŒ…å«äº† LangChain ChatOpenAI ç±»çš„æ‰€æœ‰å¯ç”¨å‚æ•°çš„è¯¦ç»†è¯´æ˜å’Œç¤ºä¾‹
é€‚ç”¨äºæ™ºè°±AI (GLM) API é…ç½®ï¼Œä½†åŒæ ·é€‚ç”¨äºæ ‡å‡† OpenAI API

ä½œè€…: Claude
ç‰ˆæœ¬: 1.0
æ›´æ–°æ—¶é—´: 2025-11-28
"""

import os
import httpx
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI


# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================================
# 1. åŸºç¡€æ¨¡å‹å‚æ•°ç¤ºä¾‹
# ================================

def basic_model_parameters():
    """
    åŸºç¡€æ¨¡å‹å‚æ•°ç¤ºä¾‹
    åŒ…æ‹¬: model, temperature, top_p, max_tokens, n
    """
    print("ğŸ”§ === åŸºç¡€æ¨¡å‹å‚æ•°ç¤ºä¾‹ ===\n")

    # å®Œæ•´çš„åŸºç¡€å‚æ•°é…ç½®
    llm = ChatOpenAI(
        # æ ¸å¿ƒæ¨¡å‹å‚æ•°
        model="glm-4",  # æ¨¡å‹åç§°ï¼Œå¯¹äºæ™ºè°±AIé€šå¸¸ä½¿ç”¨ "glm-4"
        temperature=0.7,  # æ§åˆ¶è¾“å‡ºéšæœºæ€§: 0.0=ç¡®å®šæ€§, 1.0=æ ‡å‡†éšæœºæ€§, 2.0=æœ€å¤§éšæœºæ€§
        top_p=0.9,  # æ ¸é‡‡æ ·: 0.1=åªè€ƒè™‘å‰10%æ¦‚ç‡çš„tokens, 1.0=è€ƒè™‘æ‰€æœ‰tokens
        max_tokens=150,  # ç”Ÿæˆå“åº”çš„æœ€å¤§tokenæ•°é‡
        n=1,  # ä¸ºæ¯ä¸ªè¾“å…¥ç”Ÿæˆå¤šå°‘ä¸ªå“åº”(æŸäº›æ¨¡å‹æ”¯æŒ)

        # APIé…ç½®
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),
        verbose=True  # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
    )

    message = HumanMessage(content="è¯·ç®€å•ä»‹ç»ä¸€ä¸‹LangChainæ˜¯ä»€ä¹ˆï¼Ÿ")

    try:
        response = llm.invoke([message])
        print(f"é—®é¢˜: {message.content}")
        print(f"å›ç­”: {response.content}\n")
        print(f"ä½¿ç”¨çš„å‚æ•°: temperature={llm.temperature}, max_tokens={llm.max_tokens}")
    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}\n")

# ================================
# 2. æƒ©ç½šå‚æ•°ç¤ºä¾‹
# ================================

def penalty_parameters():
    """
    æƒ©ç½šå‚æ•°ç¤ºä¾‹
    åŒ…æ‹¬: frequency_penalty, presence_penalty, logit_bias
    """
    print("âš–ï¸ === æƒ©ç½šå‚æ•°ç¤ºä¾‹ ===\n")

    # é…ç½®æƒ©ç½šå‚æ•°
    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.8,
        max_tokens=200,

        # æƒ©ç½šå‚æ•°
        frequency_penalty=0.5,  # é¢‘ç‡æƒ©ç½š: æ­£å€¼å‡å°‘é‡å¤ï¼Œè´Ÿå€¼å¢åŠ é‡å¤ï¼ŒèŒƒå›´-2åˆ°2
        presence_penalty=0.3,   # å­˜åœ¨æƒ©ç½š: æ­£å€¼é¼“åŠ±è°ˆè®ºæ–°è¯é¢˜ï¼ŒèŒƒå›´-2åˆ°2

        # APIé…ç½®
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    message = HumanMessage(content="è¯·é‡å¤ä¸‰æ¬¡ï¼šç¼–ç¨‹å¾ˆæœ‰è¶£ï¼Œç¼–ç¨‹å¾ˆæœ‰è¶£ï¼Œç¼–ç¨‹å¾ˆæœ‰è¶£ã€‚")

    try:
        response = llm.invoke([message])
        print(f"é—®é¢˜: {message.content}")
        print(f"å›ç­” (åº”ç”¨äº†æƒ©ç½šå‚æ•°): {response.content}\n")
        print(f"ä½¿ç”¨çš„å‚æ•°: frequency_penalty={llm.frequency_penalty}, presence_penalty={llm.presence_penalty}")
    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}\n")

# ================================
# 3. è¾“å‡ºæ§åˆ¶å‚æ•°ç¤ºä¾‹
# ================================

def output_control_parameters():
    """
    è¾“å‡ºæ§åˆ¶å‚æ•°ç¤ºä¾‹
    åŒ…æ‹¬: stop, streaming, logprobs, top_logprobs
    """
    print("ğŸ›ï¸ === è¾“å‡ºæ§åˆ¶å‚æ•°ç¤ºä¾‹ ===\n")

    # é…ç½®è¾“å‡ºæ§åˆ¶å‚æ•°
    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.7,
        max_tokens=100,

        # è¾“å‡ºæ§åˆ¶
        stop=["\n", "å›ç­”å®Œæ¯•", "END"],  # é‡åˆ°è¿™äº›åºåˆ—æ—¶åœæ­¢ç”Ÿæˆ
        streaming=False,  # æ˜¯å¦æµå¼è¿”å›å“åº”

        # APIé…ç½®
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    message = HumanMessage(content="è¯·åˆ—ä¸¾ä¸‰ä¸ªç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹ï¼Œæ¯è¡Œä¸€ä¸ªã€‚")

    try:
        response = llm.invoke([message])
        print(f"é—®é¢˜: {message.content}")
        print(f"å›ç­”: {response.content}\n")
        print(f"ä½¿ç”¨çš„åœæ­¢åºåˆ—: {llm.stop}")
    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}\n")

# ================================
# 4. APIé…ç½®å‚æ•°ç¤ºä¾‹
# ================================

def api_configuration_parameters():
    """
    APIé…ç½®å‚æ•°ç¤ºä¾‹
    åŒ…æ‹¬: timeout, max_retries, organization, custom headers
    """
    print("ğŸŒ === APIé…ç½®å‚æ•°ç¤ºä¾‹ ===\n")

    # é…ç½®APIå‚æ•°
    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.5,
        max_tokens=150,

        # APIé…ç½®
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),
        timeout=30.0,  # è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
        max_retries=3,  # æœ€å¤§é‡è¯•æ¬¡æ•°

        # è‡ªå®šä¹‰é…ç½®
        default_headers={  # é»˜è®¤HTTPå¤´
            "User-Agent": "MyLangChainApp/1.0",
            "X-Custom-Header": "custom-value"
        },

        verbose=True
    )

    message = HumanMessage(content="è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯APIè¶…æ—¶å’Œé‡è¯•æœºåˆ¶ï¼Ÿ")

    try:
        response = llm.invoke([message])
        print(f"é—®é¢˜: {message.content}")
        print(f"å›ç­”: {response.content}\n")
        print(f"APIé…ç½®: timeout={llm.timeout}s, max_retries={llm.max_retries}")
    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}\n")

# ================================
# 5. å®¢æˆ·ç«¯é…ç½®å‚æ•°ç¤ºä¾‹
# ================================

def client_configuration_parameters():
    """
    å®¢æˆ·ç«¯é…ç½®å‚æ•°ç¤ºä¾‹
    åŒ…æ‹¬: http_client, http_async_client, custom query params
    """
    print("ğŸ”Œ === å®¢æˆ·ç«¯é…ç½®å‚æ•°ç¤ºä¾‹ ===\n")

    # è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯
    custom_client = httpx.Client(
        limits=httpx.Limits(
            max_keepalive_connections=5,
            max_connections=10
        ),
        timeout=httpx.Timeout(10.0, connect=5.0)
    )

    # é…ç½®å®¢æˆ·ç«¯å‚æ•°
    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.6,
        max_tokens=120,

        # APIé…ç½®
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),

        # å®¢æˆ·ç«¯é…ç½®
        http_client=custom_client,  # è‡ªå®šä¹‰åŒæ­¥HTTPå®¢æˆ·ç«¯
        default_query={"model_version": "latest"},  # é»˜è®¤æŸ¥è¯¢å‚æ•°

        verbose=True
    )

    message = HumanMessage(content="è¯·è§£é‡Šä¸€ä¸‹HTTPå®¢æˆ·ç«¯è¿æ¥æ± çš„ä½œç”¨ã€‚")

    try:
        response = llm.invoke([message])
        print(f"é—®é¢˜: {message.content}")
        print(f"å›ç­”: {response.content}\n")
        print("âœ… ä½¿ç”¨äº†è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯é…ç½®")

        # æ¸…ç†å®¢æˆ·ç«¯èµ„æº
        custom_client.close()
    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}")
        try:
            custom_client.close()
        except:
            pass

# ================================
# 6. é«˜çº§å‚æ•°ç¤ºä¾‹
# ================================

def advanced_parameters():
    """
    é«˜çº§å‚æ•°ç¤ºä¾‹
    åŒ…æ‹¬: seed, model_kwargs, extra_body, disabled_params
    """
    print("ğŸš€ === é«˜çº§å‚æ•°ç¤ºä¾‹ ===\n")

    # é…ç½®é«˜çº§å‚æ•°
    llm = ChatOpenAI(
        model="glm-4",
        temperature=0.7,
        max_tokens=150,

        # APIé…ç½®
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),

        # é«˜çº§å‚æ•°
        seed=42,  # éšæœºç§å­ï¼Œç›¸åŒç§å­ä¼šå¾—åˆ°ç›¸åŒç»“æœ
        model_kwargs={  # ä¼ é€’ä»»ä½•æœ‰æ•ˆçš„OpenAI APIå‚æ•°
            "response_format": {"type": "text"},  # å“åº”æ ¼å¼
            # "tools": [...],  # å·¥å…·è°ƒç”¨
            # "tool_choice": "auto"
        },
        extra_body={  # å‘OpenAIå…¼å®¹APIè¯·æ±‚ä¸­æ·»åŠ é¢å¤–JSONå±æ€§
            "custom_parameter": "custom_value",
            "provider_specific": {"option": true}
        },
        disabled_params={  # ç¦ç”¨ç‰¹å®šæ¨¡å‹ä¸æ”¯æŒçš„å‚æ•°
            # "parallel_tool_calls": None  # ç¦ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨
        },

        verbose=True
    )

    message = HumanMessage(content="è¯·ç”Ÿæˆä¸€ä¸ªå…³äºç¼–ç¨‹çš„éšæœºç¬‘è¯ã€‚")

    try:
        response = llm.invoke([message])
        print(f"é—®é¢˜: {message.content}")
        print(f"å›ç­”: {response.content}\n")
        print(f"é«˜çº§å‚æ•°: seed={llm.seed}")
        print(f"Model kwargs: {llm.model_kwargs}")
    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}\n")

# ================================
# 7. Responses APIå‚æ•°ç¤ºä¾‹ (é€‚ç”¨äºæ–°ç‰ˆlangchain-openai)
# ================================

def responses_api_parameters():
    """
    Responses APIå‚æ•°ç¤ºä¾‹
    åŒ…æ‹¬: use_responses_api, reasoning, verbosity, includeç­‰
    æ³¨æ„: éœ€è¦langchain-openai 0.3.24+ç‰ˆæœ¬
    """
    print("ğŸ†• === Responses APIå‚æ•°ç¤ºä¾‹ ===\n")

    try:
        # é…ç½®Responses APIå‚æ•°
        llm = ChatOpenAI(
            model="glm-4",
            temperature=0.7,
            max_tokens=150,

            # APIé…ç½®
            openai_api_key=os.getenv("GLM_API_KEY"),
            openai_api_base=os.getenv("GLM_BASE_URL"),

            # Responses APIå‚æ•° (å¦‚æœæ”¯æŒ)
            # use_responses_api=True,  # ä½¿ç”¨Responses APIè€ŒéChat Completions API
            # reasoning={  # æ¨ç†æ¨¡å‹å‚æ•°
            #     "effort": "medium",  # "low", "medium", "high"
            #     "summary": "detailed"  # "auto", "concise", "detailed"
            # },
            # verbosity="medium",  # "low", "medium", "high"
            # service_tier="auto",  # "auto", "default", "flex"
            # store=True,  # æ˜¯å¦å­˜å‚¨å“åº”æ•°æ®

            verbose=True
        )

        message = HumanMessage(content="è¯·åˆ†æä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿ã€‚")

        response = llm.invoke([message])
        print(f"é—®é¢˜: {message.content}")
        print(f"å›ç­”: {response.content}\n")
        print("âœ… Responses APIé…ç½® (å¦‚æœæ”¯æŒ)")

    except TypeError as e:
        if "unexpected keyword argument" in str(e):
            print("âš ï¸ å½“å‰ç‰ˆæœ¬çš„langchain-openaiä¸æ”¯æŒResponses APIå‚æ•°")
            print("è¯·å‡çº§åˆ°langchain-openai 0.3.24+ç‰ˆæœ¬\n")
        else:
            print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}\n")
    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}\n")

# ================================
# 8. å®Œæ•´é…ç½®ç¤ºä¾‹
# ================================

def complete_configuration_example():
    """
    å®Œæ•´é…ç½®ç¤ºä¾‹
    å±•ç¤ºå¦‚ä½•ç»„åˆä½¿ç”¨å¤šä¸ªå‚æ•°
    """
    print("ğŸ¯ === å®Œæ•´é…ç½®ç¤ºä¾‹ ===\n")

    # è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯
    custom_client = httpx.Client(
        limits=httpx.Limits(max_connections=20),
        timeout=httpx.Timeout(60.0)
    )

    # æœ€å®Œæ•´çš„é…ç½®
    llm = ChatOpenAI(
        # === æ ¸å¿ƒæ¨¡å‹å‚æ•° ===
        model="glm-4",
        temperature=0.7,  # æ§åˆ¶éšæœºæ€§
        top_p=0.9,  # æ ¸é‡‡æ ·
        max_tokens=300,  # æœ€å¤§tokens
        n=1,  # ç”Ÿæˆå“åº”æ•°é‡

        # === æƒ©ç½šå‚æ•° ===
        frequency_penalty=0.1,  # é¢‘ç‡æƒ©ç½š
        presence_penalty=0.1,   # å­˜åœ¨æƒ©ç½š

        # === è¾“å‡ºæ§åˆ¶ ===
        stop=["\n\n", "===END==="],  # åœæ­¢åºåˆ—
        streaming=False,  # éæµå¼

        # === APIé…ç½® ===
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),
        timeout=60.0,  # è¶…æ—¶æ—¶é—´
        max_retries=5,  # æœ€å¤§é‡è¯•

        # === å®¢æˆ·ç«¯é…ç½® ===
        http_client=custom_client,  # è‡ªå®šä¹‰å®¢æˆ·ç«¯
        default_headers={
            "User-Agent": "LangChainCompleteExample/1.0",
            "X-Request-ID": "complete-example"
        },
        default_query={"version": "v1"},

        # === é«˜çº§å‚æ•° ===
        seed=12345,  # éšæœºç§å­
        model_kwargs={
            "response_format": {"type": "text"}
        },
        extra_body={
            "custom_provider_config": {
                "optimization": true
            }
        },

        # === å…¶ä»–å‚æ•° ===
        tiktoken_model_name="glm-4",  # ç”¨äºtokenè®¡ç®—çš„æ¨¡å‹åç§°
        include_response_headers=True,  # åŒ…å«å“åº”å¤´

        verbose=True
    )

    messages = [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹,æ“…é•¿æŠ€æœ¯è§£é‡Šã€‚"),
        HumanMessage(content="è¯·è¯¦ç»†è§£é‡ŠLangChainçš„æ ¸å¿ƒæ¦‚å¿µ,åŒ…æ‹¬Chainsã€Agentså’ŒMemoryã€‚")
    ]

    try:
        response = llm.invoke(messages)
        print("ç³»ç»Ÿæ¶ˆæ¯:", messages[0].content)
        print("ç”¨æˆ·æ¶ˆæ¯:", messages[1].content)
        print("\nAIå›ç­”:")
        print("=" * 50)
        print(response.content)
        print("=" * 50)

        # æ˜¾ç¤ºå“åº”å…ƒæ•°æ®
        if hasattr(response, 'response_metadata'):
            print(f"\nå“åº”å…ƒæ•°æ®: {response.response_metadata}")

        print(f"\nâœ… å®Œæ•´é…ç½®ç¤ºä¾‹æˆåŠŸæ‰§è¡Œ")
        print(f"ä½¿ç”¨å‚æ•°æ€»ç»“:")
        print(f"  - æ¨¡å‹: {llm.model}")
        print(f"  - æ¸©åº¦: {llm.temperature}")
        print(f"  - æœ€å¤§tokens: {llm.max_tokens}")
        print(f"  - ç§å­: {llm.seed}")
        print(f"  - è¶…æ—¶: {llm.timeout}s")
        print(f"  - æœ€å¤§é‡è¯•: {llm.max_retries}")

    except Exception as e:
        print(f"âŒ è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}")
    finally:
        # æ¸…ç†èµ„æº
        custom_client.close()

# ================================
# 9. å‚æ•°å¯¹æ¯”ç¤ºä¾‹
# ================================

def parameter_comparison_example():
    """
    å‚æ•°å¯¹æ¯”ç¤ºä¾‹
    å±•ç¤ºä¸åŒå‚æ•°å€¼å¯¹è¾“å‡ºç»“æœçš„å½±å“
    """
    print("âš–ï¸ === å‚æ•°å¯¹æ¯”ç¤ºä¾‹ ===\n")

    question = "è¯·ç”¨åˆ›æ„çš„æ–¹å¼æè¿°ä¸€ä¸‹ç¼–ç¨‹çš„ä¹è¶£ã€‚"
    message = HumanMessage(content=question)

    # ä½æ¸©åº¦é…ç½® - æ›´ç¡®å®šæ€§
    llm_deterministic = ChatOpenAI(
        model="glm-4",
        temperature=0.1,  # ä½æ¸©åº¦
        max_tokens=150,
        frequency_penalty=0.0,  # æ— æƒ©ç½š
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # é«˜æ¸©åº¦é…ç½® - æ›´åˆ›æ„æ€§
    llm_creative = ChatOpenAI(
        model="glm-4",
        temperature=1.2,  # é«˜æ¸©åº¦
        max_tokens=150,
        frequency_penalty=0.3,  # æœ‰é¢‘ç‡æƒ©ç½š
        presence_penalty=0.2,    # æœ‰å­˜åœ¨æƒ©ç½š
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    try:
        print(f"é—®é¢˜: {question}\n")

        # ç¡®å®šæ€§å›ç­”
        print("ğŸ“Š ä½æ¸©åº¦é…ç½® (temperature=0.1, æ— æƒ©ç½š):")
        print("-" * 40)
        response1 = llm_deterministic.invoke([message])
        print(response1.content)
        print()

        # åˆ›æ„æ€§å›ç­”
        print("ğŸ¨ é«˜æ¸©åº¦é…ç½® (temperature=1.2, æœ‰æƒ©ç½š):")
        print("-" * 40)
        response2 = llm_creative.invoke([message])
        print(response2.content)
        print()

        print("ğŸ’¡ å¯¹æ¯”åˆ†æ:")
        print("  - ä½æ¸©åº¦: è¾“å‡ºæ›´ç¨³å®šã€å¯é¢„æµ‹ï¼Œé€‚åˆäº‹å®æ€§å›ç­”")
        print("  - é«˜æ¸©åº¦: è¾“å‡ºæ›´æœ‰åˆ›æ„ã€å¤šæ ·æ€§ï¼Œé€‚åˆåˆ›æ„æ€§ä»»åŠ¡")
        print("  - æƒ©ç½šå‚æ•°: å¯ä»¥å‡å°‘é‡å¤å†…å®¹ï¼Œé¼“åŠ±æ›´å¤šæ ·åŒ–è¡¨è¾¾")

    except Exception as e:
        print(f"âŒ å‚æ•°å¯¹æ¯”ç¤ºä¾‹å‡ºé”™: {e}")

# ================================
# ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
# ================================

def main():
    """
    ä¸»å‡½æ•° - è¿è¡Œæ‰€æœ‰å‚æ•°ç¤ºä¾‹
    """
    import sys
    import io

    # è®¾ç½®UTF-8ç¼–ç è¾“å‡º
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("ğŸ¤– ChatOpenAI å®Œæ•´å‚æ•°å‚è€ƒæŒ‡å—")
    print("=" * 60)
    print()

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv("GLM_API_KEY") or not os.getenv("GLM_BASE_URL"):
        print("âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ°GLM_API_KEYæˆ–GLM_BASE_URLç¯å¢ƒå˜é‡")
        print("è¯·ç¡®ä¿åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®äº†æ­£ç¡®çš„æ™ºè°±AI APIé…ç½®")
        print("ç¤ºä¾‹.envæ–‡ä»¶å†…å®¹:")
        print("GLM_API_KEY=your_api_key_here")
        print("GLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4/")
        print()
        return

    # è¿è¡Œå„ä¸ªç¤ºä¾‹
    examples = [
        ("åŸºç¡€æ¨¡å‹å‚æ•°", basic_model_parameters),
        ("æƒ©ç½šå‚æ•°", penalty_parameters),
        ("è¾“å‡ºæ§åˆ¶å‚æ•°", output_control_parameters),
        ("APIé…ç½®å‚æ•°", api_configuration_parameters),
        ("å®¢æˆ·ç«¯é…ç½®å‚æ•°", client_configuration_parameters),
        ("é«˜çº§å‚æ•°", advanced_parameters),
        ("Responses APIå‚æ•°", responses_api_parameters),
        ("å®Œæ•´é…ç½®ç¤ºä¾‹", complete_configuration_example),
        ("å‚æ•°å¯¹æ¯”ç¤ºä¾‹", parameter_comparison_example)
    ]

    for name, func in examples:
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ è¿è¡Œç¤ºä¾‹: {name}")
        print('='*60)
        print()

        try:
            func()
        except KeyboardInterrupt:
            print(f"\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­äº†ç¤ºä¾‹: {name}")
            break
        except Exception as e:
            print(f"âŒ ç¤ºä¾‹ {name} æ‰§è¡Œå‡ºé”™: {e}")

        # è¯¢é—®æ˜¯å¦ç»§ç»­
        print("\n" + "="*60)
        try:
            user_input = input("æŒ‰Enterç»§ç»­ä¸‹ä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ–è¾“å…¥'q'é€€å‡º: ")
            if user_input.lower() == 'q':
                break
        except (EOFError, KeyboardInterrupt):
            print("\nğŸ‘‹ ç”¨æˆ·é€€å‡ºç¨‹åº")
            break

    print("\n" + "="*60)
    print("âœ¨ ChatOpenAI å‚æ•°å‚è€ƒæŒ‡å—ç»“æŸï¼")
    print("="*60)
    print()
    print("ğŸ“š æ›´å¤šä¿¡æ¯:")
    print("  - LangChainæ–‡æ¡£: https://python.langchain.com/")
    print("  - ChatOpenAI APIå‚è€ƒ: https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html")
    print("  - OpenAI APIæ–‡æ¡£: https://platform.openai.com/docs/api-reference/chat")
    print()
    print("ğŸ’¡ æç¤º:")
    print("  - æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„å‚æ•°ç»„åˆ")
    print("  - ç”Ÿäº§ç¯å¢ƒä¸­å»ºè®®è®¾ç½®é€‚å½“çš„è¶…æ—¶å’Œé‡è¯•æœºåˆ¶")
    print("  - ä½¿ç”¨seedå‚æ•°å¯ä»¥ç¡®ä¿ç»“æœçš„å¯é‡ç°æ€§")
    print("  - é€šè¿‡model_kwargså¯ä»¥ä¼ é€’ä»»ä½•æ–°çš„APIå‚æ•°")

if __name__ == "__main__":
    main()