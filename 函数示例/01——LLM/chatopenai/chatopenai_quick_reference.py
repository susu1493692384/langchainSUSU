#!/usr/bin/env python3
"""
ChatOpenAI å¿«é€ŸæŸ¥è¡¨
åŸºäº chatopenai_complete_reference.py çš„ç²¾åç‰ˆæœ¬
æ‰€æœ‰å¸¸ç”¨å‚æ•°çš„å¿«é€Ÿå‚è€ƒå’Œç¤ºä¾‹

ç”¨æ³•ï¼šå¤åˆ¶ç²˜è´´å³å¯ä½¿ç”¨
"""

import os
import httpx
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================================
# ğŸ”¥ æœ€å¸¸ç”¨é…ç½® (90%æƒ…å†µä¸‹ä½¿ç”¨è¿™ä¸ª)
# ================================

# åŸºç¡€é…ç½® - é€‚ç”¨äºæ™ºè°±AI/GLM
def basic_config():
    return ChatOpenAI(
        model="glm-4",
        temperature=0.7,        # 0-2ä¹‹é—´ï¼Œæ§åˆ¶éšæœºæ€§
        max_tokens=150,         # é™åˆ¶è¾“å‡ºé•¿åº¦
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

# OpenAIå®˜æ–¹APIé…ç½®
def openai_config():
    return ChatOpenAI(
        model="gpt-4",
        temperature=0.7,
        max_tokens=150,
        api_key=os.getenv("OPENAI_API_KEY")  # æˆ–ç›´æ¥ä¼ å…¥å¯†é’¥
    )

# ================================
# âš™ï¸ å‚æ•°å¿«é€Ÿå‚è€ƒ
# ================================

# ğŸ¯ æ ¸å¿ƒå‚æ•°
def core_parameters():
    return ChatOpenAI(
        # === åŸºç¡€å‚æ•° ===
        model="glm-4",           # æ¨¡å‹åç§°
        temperature=0.7,         # éšæœºæ€§: 0=ç¡®å®šæ€§, 1=æ ‡å‡†, 2=æœ€å¤§éšæœºæ€§
        max_tokens=150,          # æœ€å¤§è¾“å‡ºtokenæ•°
        top_p=0.9,              # æ ¸é‡‡æ ·: 0.1=åªè€ƒè™‘å‰10%æ¦‚ç‡çš„tokens

        # === æƒ©ç½šå‚æ•° ===
        frequency_penalty=0.0,   # é¢‘ç‡æƒ©ç½š: -2åˆ°2ï¼Œæ­£å€¼å‡å°‘é‡å¤
        presence_penalty=0.0,    # å­˜åœ¨æƒ©ç½š: -2åˆ°2ï¼Œæ­£å€¼é¼“åŠ±æ–°è¯é¢˜

        # === APIé…ç½® ===
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),
        timeout=30.0,           # è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
        max_retries=3,           # æœ€å¤§é‡è¯•æ¬¡æ•°

        # === è¾“å‡ºæ§åˆ¶ ===
        stop=["\n", "END"],     # é‡åˆ°è¿™äº›åºåˆ—æ—¶åœæ­¢ç”Ÿæˆ

        verbose=True             # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
    )

# ğŸš€ ç”Ÿäº§ç¯å¢ƒé…ç½®
def production_config():
    return ChatOpenAI(
        model="glm-4",
        temperature=0.3,         # ç”Ÿäº§ç¯å¢ƒå»ºè®®è¾ƒä½çš„éšæœºæ€§
        max_tokens=500,

        # ç”Ÿäº§ç¯å¢ƒé‡è¦å‚æ•°
        timeout=60.0,
        max_retries=5,
        streaming=False,         # ç”Ÿäº§ç¯å¢ƒé€šå¸¸å…³é—­æµå¼

        # APIé…ç½®
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),

        # æ€§èƒ½ä¼˜åŒ–
        default_headers={
            "User-Agent": "ProductionApp/1.0",
            "X-Request-ID": "production"
        }
    )

# ğŸ¨ åˆ›æ„å†™ä½œé…ç½®
def creative_config():
    return ChatOpenAI(
        model="glm-4",
        temperature=1.2,         # é«˜éšæœºæ€§ï¼Œé€‚åˆåˆ›æ„å†…å®¹
        max_tokens=300,
        top_p=0.95,             # æ›´å¤šæ ·æ€§

        # åˆ›æ„ä¼˜åŒ–
        frequency_penalty=0.3,   # å‡å°‘é‡å¤
        presence_penalty=0.5,   # é¼“åŠ±æ–°æ€è·¯

        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

# ğŸ’» ä»£ç åŠ©æ‰‹é…ç½®
def code_assistant_config():
    return ChatOpenAI(
        model="glm-4",
        temperature=0.1,         # ä»£ç éœ€è¦ç¡®å®šæ€§
        max_tokens=800,

        # ä»£ç ç›¸å…³è®¾ç½®
        model_kwargs={
            "response_format": {"type": "text"}  # ç¡®ä¿çº¯æ–‡æœ¬è¾“å‡º
        },

        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),

        # åœæ­¢ç¬¦ï¼Œé€‚åˆä»£ç è¾“å‡º
        stop=["```", "END", "å®Œ"]
    )

# ================================
# ğŸ”§ ç‰¹æ®Šåœºæ™¯é…ç½®
# ================================

# ğŸ“Š JSONè¾“å‡ºé…ç½®
def json_output_config():
    return ChatOpenAI(
        model="glm-4",
        temperature=0.3,
        max_tokens=200,

        # å¼ºåˆ¶JSONè¾“å‡º
        model_kwargs={
            "response_format": {"type": "json_object"}
        },

        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

# ğŸŒ é«˜å¹¶å‘é…ç½®
def high_concurrency_config():
    # è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯
    custom_client = httpx.Client(
        limits=httpx.Limits(
            max_keepalive_connections=20,  # ä¿æŒè¿æ¥æ•°
            max_connections=50              # æœ€å¤§è¿æ¥æ•°
        ),
        timeout=httpx.Timeout(10.0, connect=5.0)  # è¿æ¥å’Œè¯»å–è¶…æ—¶
    )

    return ChatOpenAI(
        model="glm-4",
        temperature=0.7,
        max_tokens=150,

        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),

        # é«˜å¹¶å‘ä¼˜åŒ–
        http_client=custom_client,
        timeout=10.0,
        max_retries=2,

        verbose=False  # é«˜å¹¶å‘æ—¶å…³é—­è¯¦ç»†æ—¥å¿—
    )

# ğŸ”’ å¯é‡ç°ç»“æœé…ç½®
def reproducible_config():
    return ChatOpenAI(
        model="glm-4",
        temperature=0.7,
        max_tokens=150,
        seed=42,  # éšæœºç§å­ï¼Œç¡®ä¿ç›¸åŒè¾“å…¥å¾—åˆ°ç›¸åŒè¾“å‡º

        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

# âš¡ å¿«é€Ÿå“åº”é…ç½®
def fast_response_config():
    return ChatOpenAI(
        model="glm-4",
        temperature=0.5,
        max_tokens=100,          # é™åˆ¶è¾“å‡ºé•¿åº¦ä»¥åŠ å¿«å“åº”

        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),

        # å¿«é€Ÿå“åº”è®¾ç½®
        timeout=15.0,
        max_retries=1,

        # æ–°ç‰ˆAPIå‚æ•° (å¦‚æœæ”¯æŒ)
        # service_tier="flex",     # å»¶è¿Ÿä¼˜åŒ–å±‚
        # reasoning_effort="low"   # å‡å°‘æ¨ç†åŠªåŠ›
    )

# ================================
# ğŸ“‹ å‚æ•°è¯´æ˜é€ŸæŸ¥
# ================================

def parameter_reference():
    """
    ChatOpenAI å‚æ•°é€ŸæŸ¥è¡¨

    ğŸ”¥ æ ¸å¿ƒå‚æ•° (å¿…é¡»äº†è§£):
    - model: str              # æ¨¡å‹åç§° (glm-4, gpt-4, ç­‰)
    - temperature: float       # éšæœºæ€§ 0-2 (0.7æ ‡å‡†, 0.1ç²¾ç¡®, 1.2åˆ›æ„)
    - max_tokens: int          # æœ€å¤§è¾“å‡ºé•¿åº¦
    - openai_api_key: str      # APIå¯†é’¥

    âš™ï¸ æ€§èƒ½å‚æ•° (æ¨èè®¾ç½®):
    - timeout: float          # è¶…æ—¶æ—¶é—´ (30sæ ‡å‡†, 60sç”Ÿäº§)
    - max_retries: int         # é‡è¯•æ¬¡æ•° (3æ ‡å‡†, 5ç”Ÿäº§)
    - streaming: bool         # æµå¼è¾“å‡º (Falseæ ‡å‡†, Trueå®æ—¶)

    ğŸ›ï¸ è¾“å‡ºæ§åˆ¶:
    - top_p: float           # æ ¸é‡‡æ · 0-1 (0.9æ ‡å‡†)
    - stop: list[str]        # åœæ­¢åºåˆ—
    - frequency_penalty: float # é¢‘ç‡æƒ©ç½š -2åˆ°2
    - presence_penalty: float  # å­˜åœ¨æƒ©ç½š -2åˆ°2

    ğŸ”§ é«˜çº§å‚æ•° (ç‰¹æ®Šéœ€æ±‚):
    - seed: int              # éšæœºç§å­ (ç”¨äºé‡ç°ç»“æœ)
    - model_kwargs: dict     # è‡ªå®šä¹‰APIå‚æ•°
    - http_client: httpx.Client # è‡ªå®šä¹‰HTTPå®¢æˆ·ç«¯

    ğŸ“Š æ™ºè°±AIç‰¹æœ‰:
    - openai_api_base: str    # "https://open.bigmodel.cn/api/paas/v4/"
    """
    pass

# ================================
# ğŸ¯ ä½¿ç”¨ç¤ºä¾‹
# ================================

def usage_examples():
    """å®é™…ä½¿ç”¨ç¤ºä¾‹"""

    # ç¤ºä¾‹1: åŸºç¡€é—®ç­”
    def basic_qa():
        llm = basic_config()
        messages = [{"role": "user", "content": "ä½ å¥½ï¼è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚"}]
        # response = llm.invoke(messages)
        # print(response.content)

    # ç¤ºä¾‹2: ä»£ç ç”Ÿæˆ
    def code_generation():
        llm = code_assistant_config()
        messages = [{"role": "user", "content": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—"}]
        # response = llm.invoke(messages)
        # print(response.content)

    # ç¤ºä¾‹3: åˆ›æ„å†™ä½œ
    def creative_writing():
        llm = creative_config()
        messages = [{"role": "user", "content": "å†™ä¸€ä¸ªå…³äºAIçš„æœªæ¥æ•…äº‹"}]
        # response = llm.invoke(messages)
        # print(response.content)

    # ç¤ºä¾‹4: JSONæ•°æ®æå–
    def json_extraction():
        llm = json_output_config()
        messages = [{"role": "user", "content": "ä»è¿™æ®µæ–‡æœ¬æå–å…³é”®ä¿¡æ¯å¹¶è¿”å›JSONæ ¼å¼"}]
        # response = llm.invoke(messages)
        # print(response.content)

    # ç¤ºä¾‹5: é«˜å¹¶å‘åº”ç”¨
    def concurrent_requests():
        llm = high_concurrency_config()
        messages = [{"role": "user", "content": "å¿«é€Ÿå›ç­”è¿™ä¸ªé—®é¢˜"}]
        # response = llm.invoke(messages)
        # print(response.content)

# ================================
# ğŸš¨ é”™è¯¯å¤„ç†æœ€ä½³å®è·µ
# ================================

def error_handling_template():
    """æ¨èçš„é”™è¯¯å¤„ç†æ¨¡æ¿"""

    def safe_llm_call(llm, messages, max_attempts=3):
        """
        å®‰å…¨çš„LLMè°ƒç”¨ï¼ŒåŒ…å«é‡è¯•å’Œé”™è¯¯å¤„ç†
        """
        for attempt in range(max_attempts):
            try:
                response = llm.invoke(messages)
                return response
            except Exception as e:
                print(f"å°è¯• {attempt + 1}/{max_attempts} å¤±è´¥: {e}")
                if attempt == max_attempts - 1:
                    print("æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥é…ç½®")
                    return None
                import time
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

    # ä½¿ç”¨ç¤ºä¾‹
    llm = production_config()
    messages = [{"role": "user", "content": "æµ‹è¯•æ¶ˆæ¯"}]
    # response = safe_llm_call(llm, messages)
    # if response:
    #     print(f"æˆåŠŸè·å¾—å›å¤: {response.content}")

# ================================
# ğŸ† æ¨èé…ç½®
# ================================

def recommended_configurations():
    """é’ˆå¯¹ä¸åŒåœºæ™¯çš„æ¨èé…ç½®"""

    recommendations = {
        "ğŸ¢ ç”Ÿäº§ç¯å¢ƒ": """
        model="glm-4",
        temperature=0.3,
        max_tokens=500,
        timeout=60.0,
        max_retries=5,
        streaming=False
        """,

        "ğŸ¨ åˆ›æ„å†™ä½œ": """
        model="glm-4",
        temperature=1.2,
        max_tokens=300,
        frequency_penalty=0.3,
        presence_penalty=0.5
        """,

        "ğŸ’» ä»£ç ç”Ÿæˆ": """
        model="glm-4",
        temperature=0.1,
        max_tokens=800,
        stop=["```", "END"]
        """,

        "âš¡ å¿«é€Ÿå“åº”": """
        model="glm-4",
        temperature=0.5,
        max_tokens=100,
        timeout=15.0,
        max_retries=1
        """,

        "ğŸ”„ å¯é‡ç°ç»“æœ": """
        model="glm-4",
        temperature=0.7,
        max_tokens=150,
        seed=42
        """
    }

    return recommendations

if __name__ == "__main__":
    print("ğŸš€ ChatOpenAI å¿«é€ŸæŸ¥è¡¨")
    print("=" * 50)
    print()

    print("ğŸ”¥ æœ€å¸¸ç”¨é…ç½® (å¤åˆ¶å³ç”¨):")
    print("""
# æ™ºè°±AI/GLMåŸºç¡€é…ç½®
llm = ChatOpenAI(
    model="glm-4",
    temperature=0.7,
    max_tokens=150,
    openai_api_key=os.getenv("GLM_API_KEY"),
    openai_api_base=os.getenv("GLM_BASE_URL")
)
""")

    print("ğŸ¯ ä¸åŒåœºæ™¯æ¨è:")
    configs = recommended_configurations()
    for scene, config in configs.items():
        print(f"\n{scene}:")
        print(config)

    print(f"\nğŸ“‹ å®Œæ•´å‚è€ƒè¯·æŸ¥çœ‹: chatopenai_complete_reference.py")
    print(f"ğŸ’¬ æ¶ˆæ¯ç±»å‹å‚è€ƒ: message_types_reference.py")