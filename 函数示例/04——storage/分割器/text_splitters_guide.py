"""
LangChain æ–‡æœ¬åˆ†å‰²å™¨å®Œæ•´æŒ‡å—
åŒ…å«æ‰€æœ‰åˆ†å‰²å™¨ç±»å‹ã€å‚æ•°è¯´æ˜ã€ç¤ºä¾‹ä»£ç å’Œå¿«é€ŸæŸ¥è¡¨åŠŸèƒ½
"""

# å¯¼å…¥æ‰€éœ€çš„æ‰€æœ‰åˆ†å‰²å™¨
from langchain.text_splitter import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
    HTMLHeaderTextSplitter,
    MarkdownTextSplitter,
    PythonCodeTextSplitter,
    TokenTextSplitter,
    NLTKTextSplitter,
    SpacyTextSplitter,
    SemanticChunker
)
from langchain.embeddings.openai import OpenAIEmbeddings
import pandas as pd
from typing import Dict, Any, List, Optional


class SplitterGuide:
    """LangChainæ–‡æœ¬åˆ†å‰²å™¨æŒ‡å—ç±»"""

    def __init__(self):
        self.splitter_info = self._create_splitter_info()

    def _create_splitter_info(self) -> Dict[str, Dict[str, Any]]:
        """åˆ›å»ºåˆ†å‰²å™¨ä¿¡æ¯å­—å…¸"""
        return {
            "CharacterTextSplitter": {
                "description": "åŸºäºæŒ‡å®šå­—ç¬¦åˆ†å‰²æ–‡æœ¬ï¼Œæ˜¯æœ€åŸºç¡€çš„åˆ†å‰²å™¨",
                "use_case": "ç®€å•æ–‡æœ¬ã€æœ‰æ˜ç¡®åˆ†éš”ç¬¦çš„åœºæ™¯",
                "parameters": {
                    "separator": {"type": "str", "default": '"\\n\\n"', "description": "åˆ†å‰²ç¬¦ï¼Œç”¨äºç¡®å®šåœ¨å“ªé‡Œåˆ†å‰²æ–‡æœ¬"},
                    "chunk_size": {"type": "int", "default": 1000, "description": "æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°"},
                    "chunk_overlap": {"type": "int", "default": 200, "description": "æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°"},
                    "length_function": {"type": "callable", "default": "len", "description": "ç”¨äºè®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•°"},
                    "keep_separator": {"type": "bool", "default": False, "description": "æ˜¯å¦åœ¨åˆ†å‰²åçš„æ–‡æœ¬å—ä¸­ä¿ç•™åˆ†éš”ç¬¦"}
                },
                "example": self._character_splitter_example,
                "pros": ["ç®€å•å¿«é€Ÿ", "å¯æ§æ€§å¼º"],
                "cons": ["å¯èƒ½ç ´åè¯­ä¹‰ç»“æ„"],
                "performance": "æœ€å¿«"
            },

            "RecursiveCharacterTextSplitter": {
                "description": "é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²å™¨ï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•ä¸åŒçš„åˆ†éš”ç¬¦æ¥ä¿æŒæ–‡æœ¬çš„è¯­ä¹‰å®Œæ•´æ€§",
                "use_case": "é€šç”¨æ–‡æœ¬ã€RAGç³»ç»Ÿï¼Œæœ€å¸¸ç”¨çš„åˆ†å‰²å™¨",
                "parameters": {
                    "chunk_size": {"type": "int", "default": 1000, "description": "æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°"},
                    "chunk_overlap": {"type": "int", "default": 200, "description": "æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°"},
                    "length_function": {"type": "callable", "default": "len", "description": "ç”¨äºè®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•°"},
                    "separators": {"type": "List[str]", "default": '["\\n\\n", "\\n", " ", ""]', "description": "åˆ†éš”ç¬¦ä¼˜å…ˆçº§åˆ—è¡¨"},
                    "keep_separator": {"type": "bool", "default": False, "description": "æ˜¯å¦åœ¨åˆ†å‰²åçš„æ–‡æœ¬å—ä¸­ä¿ç•™åˆ†éš”ç¬¦"}
                },
                "example": self._recursive_character_splitter_example,
                "pros": ["ä¿æŒè¯­ä¹‰å®Œæ•´", "æœ€å¸¸ç”¨", "å¹³è¡¡æ€§å¥½"],
                "cons": ["å¯èƒ½äº§ç”Ÿä¸å‡åŒ€çš„æ–‡æœ¬å—"],
                "performance": "å¹³è¡¡"
            },

            "HTMLHeaderTextSplitter": {
                "description": "ä¸“é—¨ç”¨äºHTMLæ–‡æ¡£ï¼ŒåŸºäºæ ‡é¢˜æ ‡ç­¾(h1, h2, h3ç­‰)è¿›è¡Œåˆ†å‰²",
                "use_case": "HTMLç½‘é¡µã€å¯Œæ–‡æœ¬æ–‡æ¡£",
                "parameters": {
                    "headers_to_split_on": {"type": "List[Tuple[str, str]]", "default": None, "description": "è¦åˆ†å‰²çš„æ ‡é¢˜æ ‡ç­¾åˆ—è¡¨"},
                    "return_each_element": {"type": "bool", "default": False, "description": "æ˜¯å¦è¿”å›æ¯ä¸ªHTMLå…ƒç´ "}
                },
                "example": self._html_splitter_example,
                "pros": ["ä¿æŒHTMLç»“æ„", "è¯­ä¹‰æ¸…æ™°"],
                "cons": ["ä»…é€‚ç”¨äºHTML"],
                "performance": "ä¸­ç­‰"
            },

            "MarkdownTextSplitter": {
                "description": "ä¸“é—¨ç”¨äºMarkdownæ–‡æ¡£ï¼ŒåŸºäºæ ‡é¢˜çº§åˆ«è¿›è¡Œåˆ†å‰²",
                "use_case": "Markdownæ–‡æ¡£ã€æŠ€æœ¯æ–‡æ¡£ã€READMEæ–‡ä»¶",
                "parameters": {
                    "chunk_size": {"type": "int", "default": 1000, "description": "æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°"},
                    "chunk_overlap": {"type": "int", "default": 200, "description": "æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°"},
                    "headers_to_split_on": {"type": "List[Tuple[str, str]]", "default": None, "description": "è¦åˆ†å‰²çš„Markdownæ ‡é¢˜çº§åˆ«"}
                },
                "example": self._markdown_splitter_example,
                "pros": ["ä¿æŒæ–‡æ¡£ç»“æ„", "é€‚åˆæŠ€æœ¯æ–‡æ¡£"],
                "cons": ["ä»…é€‚ç”¨äºMarkdown"],
                "performance": "ä¸­ç­‰"
            },

            "PythonCodeTextSplitter": {
                "description": "ä¸“é—¨ç”¨äºPythonä»£ç ï¼ŒåŸºäºç±»ã€å‡½æ•°ç­‰é€»è¾‘ç»“æ„è¿›è¡Œåˆ†å‰²",
                "use_case": "Pythonä»£ç æ–‡æ¡£ã€æºç åˆ†æ",
                "parameters": {
                    "chunk_size": {"type": "int", "default": 1000, "description": "æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°"},
                    "chunk_overlap": {"type": "int", "default": 200, "description": "æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°"},
                    "language": {"type": "str", "default": '"python"', "description": "ç¼–ç¨‹è¯­è¨€"}
                },
                "example": self._python_code_splitter_example,
                "pros": ["ä¿æŒä»£ç é€»è¾‘ç»“æ„", "ç†è§£æ€§å¼º"],
                "cons": ["ä»…é€‚ç”¨äºPythonä»£ç "],
                "performance": "ä¸­ç­‰"
            },

            "TokenTextSplitter": {
                "description": "åŸºäºä»¤ç‰Œæ•°é‡è€Œéå­—ç¬¦æ•°è¿›è¡Œåˆ†å‰²ï¼Œæ›´é€‚åˆLLMçš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶",
                "use_case": "LLMä¸Šä¸‹æ–‡é™åˆ¶ã€APIè°ƒç”¨åœºæ™¯",
                "parameters": {
                    "chunk_size": {"type": "int", "default": 1000, "description": "æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§ä»¤ç‰Œæ•°"},
                    "chunk_overlap": {"type": "int", "default": 200, "description": "æ–‡æœ¬å—ä¹‹é—´çš„é‡å ä»¤ç‰Œæ•°"},
                    "model_name": {"type": "str", "default": None, "description": "ç”¨äºè®¡ç®—ä»¤ç‰Œæ•°é‡çš„æ¨¡å‹åç§°"},
                    "encoding_name": {"type": "str", "default": None, "description": "ä½¿ç”¨çš„ç¼–ç åç§°"}
                },
                "example": self._token_splitter_example,
                "pros": ["ç²¾ç¡®æ§åˆ¶ä»¤ç‰Œæ•°", "é€‚åˆLLM"],
                "cons": ["éœ€è¦é¢å¤–ä¾èµ–", "è®¡ç®—æˆæœ¬é«˜"],
                "performance": "ä¸­ç­‰"
            },

            "NLTKTextSplitter": {
                "description": "ä½¿ç”¨NLTKåº“è¿›è¡Œæ›´æ™ºèƒ½çš„æ–‡æœ¬åˆ†å‰²ï¼ŒåŸºäºå¥å­è¾¹ç•Œ",
                "use_case": "è‡ªç„¶è¯­è¨€æ–‡æœ¬ã€å­¦æœ¯æ–‡æ¡£",
                "parameters": {
                    "chunk_size": {"type": "int", "default": 1000, "description": "æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°"},
                    "chunk_overlap": {"type": "int", "default": 200, "description": "æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°"}
                },
                "example": self._nltk_splitter_example,
                "pros": ["æ™ºèƒ½å¥å­è¾¹ç•Œ", "è¯­è¨€å­¦å‡†ç¡®"],
                "cons": ["éœ€è¦NLTKä¾èµ–", "éœ€è¦ä¸‹è½½è¯­è¨€åŒ…"],
                "performance": "ä¸­ç­‰"
            },

            "SpacyTextSplitter": {
                "description": "ä½¿ç”¨SpaCyåº“è¿›è¡ŒåŸºäºè¯­è¨€å­¦ç‰¹å¾çš„æ–‡æœ¬åˆ†å‰²",
                "use_case": "ä¸“ä¸šæ–‡æ¡£ã€å¤šè¯­è¨€æ–‡æœ¬",
                "parameters": {
                    "chunk_size": {"type": "int", "default": 1000, "description": "æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°"},
                    "chunk_overlap": {"type": "int", "default": 200, "description": "æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°"},
                    "pipeline": {"type": "str", "default": None, "description": "ä½¿ç”¨çš„SpaCyç®¡é“åç§°"}
                },
                "example": self._spacy_splitter_example,
                "pros": ["é«˜è´¨é‡è¯­è¨€å­¦åˆ†æ", "å¤šè¯­è¨€æ”¯æŒ"],
                "cons": ["éœ€è¦SpaCyä¾èµ–", "èµ„æºå ç”¨å¤§"],
                "performance": "è¾ƒæ…¢"
            },

            "SemanticChunker": {
                "description": "åŸºäºæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œåˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰ç›¸å…³çš„æ–‡æœ¬åœ¨ä¸€èµ·",
                "use_case": "é«˜è´¨é‡RAGç³»ç»Ÿã€è¯­ä¹‰æ£€ç´¢",
                "parameters": {
                    "embeddings": {"type": "Embeddings", "default": None, "description": "ç”¨äºè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦çš„åµŒå…¥æ¨¡å‹"},
                    "buffer_size": {"type": "int", "default": None, "description": "ç¼“å†²åŒºå¤§å°"},
                    "min_chunk_size": {"type": "int", "default": None, "description": "æœ€å°æ–‡æœ¬å—å¤§å°"},
                    "max_chunk_size": {"type": "int", "default": None, "description": "æœ€å¤§æ–‡æœ¬å—å¤§å°"},
                    "breakpoint_threshold_type": {"type": "str", "default": '"percentile"', "description": "æ–­ç‚¹é˜ˆå€¼ç±»å‹"}
                },
                "example": self._semantic_chunker_example,
                "pros": ["ä¿æŒè¯­ä¹‰è¿è´¯æ€§", "è´¨é‡æœ€é«˜"],
                "cons": ["è®¡ç®—æˆæœ¬é«˜", "éœ€è¦åµŒå…¥æ¨¡å‹"],
                "performance": "æœ€æ…¢"
            }
        }

    def _character_splitter_example(self):
        """CharacterTextSplitter ç¤ºä¾‹"""
        print("=== CharacterTextSplitter ç¤ºä¾‹ ===")

        # åŸºæœ¬ç”¨æ³•
        splitter = CharacterTextSplitter(
            separator="\n",
            chunk_size=200,
            chunk_overlap=50,
            length_function=len
        )

        text = """è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æœ¬...
åŒ…å«å¤šè¡Œå†…å®¹...
éœ€è¦è¢«åˆ†å‰²æˆåˆé€‚çš„å—ã€‚
æ¯ä¸ªå—éƒ½åº”è¯¥æœ‰åˆé€‚çš„å¤§å°ã€‚
è¿™æ ·å¯ä»¥æ›´å¥½åœ°å¤„ç†é•¿æ–‡æœ¬ã€‚"""

        chunks = splitter.split_text(text)
        print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(text)}")
        print(f"åˆ†å‰²åå¾—åˆ° {len(chunks)} ä¸ªæ–‡æœ¬å—")
        for i, chunk in enumerate(chunks):
            print(f"å— {i+1}: {chunk[:50]}...")

        return chunks

    def _recursive_character_splitter_example(self):
        """RecursiveCharacterTextSplitter ç¤ºä¾‹"""
        print("\n=== RecursiveCharacterTextSplitter ç¤ºä¾‹ ===")

        # è‡ªå®šä¹‰åˆ†éš”ç¬¦ä¼˜å…ˆçº§ï¼ˆé€‚åˆä¸­æ–‡ï¼‰
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=200,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", " ", ""]
        )

        text = """è¿™æ˜¯ä¸€æ®µåŒ…å«å¤šä¸ªæ®µè½çš„ä¸­æ–‡æ–‡æœ¬ã€‚æ¯ä¸ªæ®µè½éƒ½æœ‰ä¸åŒçš„å†…å®¹ï¼
æˆ‘ä»¬å¸Œæœ›ä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼›åŒæ—¶ç¡®ä¿æ–‡æœ¬å—å¤§å°åˆé€‚ã€‚

è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ï¼ŒåŒ…å«æ›´å¤šè¦å±•ç¤ºçš„ä¿¡æ¯ã€‚
æˆ‘ä»¬éœ€è¦æµ‹è¯•åˆ†å‰²å™¨å¦‚ä½•å¤„ç†ä¸åŒç±»å‹çš„å†…å®¹ã€‚"""

        chunks = splitter.split_text(text)
        print(f"åŸå§‹æ–‡æœ¬é•¿åº¦: {len(text)}")
        print(f"åˆ†å‰²åå¾—åˆ° {len(chunks)} ä¸ªæ–‡æœ¬å—")
        for i, chunk in enumerate(chunks):
            print(f"å— {i+1}: {chunk[:50]}...")

        return chunks

    def _html_splitter_example(self):
        """HTMLHeaderTextSplitter ç¤ºä¾‹"""
        print("\n=== HTMLHeaderTextSplitter ç¤ºä¾‹ ===")

        headers_to_split_on = [
            ("h1", "Header 1"),
            ("h2", "Header 2"),
            ("h3", "Header 3"),
        ]

        html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

        html_content = """
        <h1>ä¸»æ ‡é¢˜</h1>
        <p>è¿™æ˜¯ä¸»æ ‡é¢˜ä¸‹çš„ç¬¬ä¸€æ®µå†…å®¹ï¼Œä»‹ç»ä¸»è¦å†…å®¹ã€‚</p>
        <h2>äºŒçº§æ ‡é¢˜</h2>
        <p>è¿™æ˜¯äºŒçº§æ ‡é¢˜ä¸‹çš„å†…å®¹ï¼ŒåŒ…å«è¯¦ç»†ä¿¡æ¯ã€‚</p>
        <h3>ä¸‰çº§æ ‡é¢˜</h3>
        <p>è¿™æ˜¯ä¸‰çº§æ ‡é¢˜ä¸‹çš„å…·ä½“å†…å®¹ã€‚</p>
        <h2>å¦ä¸€ä¸ªäºŒçº§æ ‡é¢˜</h2>
        <p>è¿™æ˜¯å¦ä¸€ä¸ªäºŒçº§æ ‡é¢˜ä¸‹çš„å†…å®¹ã€‚</p>
        """

        chunks = html_splitter.split_text(html_content)
        print(f"HTMLå†…å®¹åˆ†å‰²åå¾—åˆ° {len(chunks)} ä¸ªå—")
        for i, chunk in enumerate(chunks):
            if hasattr(chunk, 'metadata'):
                print(f"å— {i+1} æ ‡é¢˜: {chunk.metadata.get('Header 1', 'N/A')}")
            print(f"å†…å®¹: {chunk.page_content[:50]}...")

        return chunks

    def _markdown_splitter_example(self):
        """MarkdownTextSplitter ç¤ºä¾‹"""
        print("\n=== MarkdownTextSplitter ç¤ºä¾‹ ===")

        markdown_splitter = MarkdownTextSplitter(
            chunk_size=300,
            chunk_overlap=50
        )

        markdown_content = """# ä¸»æ ‡é¢˜

è¿™æ˜¯ä¸»æ ‡é¢˜ä¸‹çš„å†…å®¹ã€‚ä¸»æ ‡é¢˜é€šå¸¸ç”¨äºæ•´ä¸ªæ–‡æ¡£çš„ä¸»é¢˜ã€‚

## äºŒçº§æ ‡é¢˜

è¿™æ˜¯äºŒçº§æ ‡é¢˜ä¸‹çš„å†…å®¹ã€‚äºŒçº§æ ‡é¢˜ç”¨äºç»„ç»‡æ–‡æ¡£çš„ä¸»è¦ç« èŠ‚ã€‚

### ä¸‰çº§æ ‡é¢˜

è¿™æ˜¯ä¸‰çº§æ ‡é¢˜ä¸‹çš„å†…å®¹ã€‚ä¸‰çº§æ ‡é¢˜ç”¨äºç»†åˆ†ç« èŠ‚å†…å®¹ã€‚

#### å››çº§æ ‡é¢˜

è¿™æ˜¯å››çº§æ ‡é¢˜ä¸‹çš„å†…å®¹ï¼Œæä¾›æ›´è¯¦ç»†çš„åˆ†ç±»ã€‚"""

        chunks = markdown_splitter.split_text(markdown_content)
        print(f"Markdownå†…å®¹åˆ†å‰²åå¾—åˆ° {len(chunks)} ä¸ªå—")
        for i, chunk in enumerate(chunks):
            print(f"å— {i+1}: {chunk[:50]}...")

        return chunks

    def _python_code_splitter_example(self):
        """PythonCodeTextSplitter ç¤ºä¾‹"""
        print("\n=== PythonCodeTextSplitter ç¤ºä¾‹ ===")

        python_splitter = PythonCodeTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )

        python_code = '''
def hello_world():
    """è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å‡½æ•°"""
    print("Hello, World!")
    return "Hello, World!"

class MyClass:
    """è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ç±»"""
    def __init__(self):
        self.value = 0
        self.name = "ç¤ºä¾‹"

    def increment(self):
        """å¢åŠ æ•°å€¼"""
        self.value += 1
        return self.value

    def get_info(self):
        """è·å–ä¿¡æ¯"""
        return f"åç§°: {self.name}, å€¼: {self.value}"

def main():
    """ä¸»å‡½æ•°"""
    obj = MyClass()
    print(obj.increment())
    print(obj.get_info())

if __name__ == "__main__":
    main()
        '''

        chunks = python_splitter.split_text(python_code)
        print(f"Pythonä»£ç åˆ†å‰²åå¾—åˆ° {len(chunks)} ä¸ªå—")
        for i, chunk in enumerate(chunks):
            lines = chunk.strip().split('\n')
            print(f"å— {i+1}: {len(lines)} è¡Œä»£ç ")

        return chunks

    def _token_splitter_example(self):
        """TokenTextSplitter ç¤ºä¾‹"""
        print("\n=== TokenTextSplitter ç¤ºä¾‹ ===")

        try:
            token_splitter = TokenTextSplitter(
                chunk_size=100,  # 100ä¸ªä»¤ç‰Œ
                chunk_overlap=10,
                model_name="gpt-3.5-turbo"
            )

            text = """è¿™æ˜¯ä¸€æ®µéœ€è¦åŸºäºä»¤ç‰Œæ•°é‡è¿›è¡Œåˆ†å‰²çš„æ–‡æœ¬ã€‚
            ä»¤ç‰Œåˆ†å‰²å™¨å¯¹äºå¤„ç†å¤§è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶éå¸¸æœ‰ç”¨ã€‚
            å®ƒå¯ä»¥ç¡®ä¿æ¯ä¸ªæ–‡æœ¬å—éƒ½ä¸è¶…è¿‡æ¨¡å‹çš„ä»¤ç‰Œé™åˆ¶ã€‚
            è¿™æ ·å¯ä»¥é¿å…APIè°ƒç”¨æ—¶å‡ºç°ä»¤ç‰Œè¶…é™çš„é”™è¯¯ã€‚"""

            chunks = token_splitter.split_text(text)
            print(f"æ–‡æœ¬åˆ†å‰²åå¾—åˆ° {len(chunks)} ä¸ªå—")
            for i, chunk in enumerate(chunks):
                print(f"å— {i+1}: {chunk[:50]}...")

            return chunks
        except Exception as e:
            print(f"TokenTextSplitteréœ€è¦tiktokenåº“æˆ–é€‚å½“çš„ä¾èµ–: {e}")
            return []

    def _nltk_splitter_example(self):
        """NLTKTextSplitter ç¤ºä¾‹"""
        print("\n=== NLTKTextSplitter ç¤ºä¾‹ ===")

        try:
            nltk_splitter = NLTKTextSplitter(
                chunk_size=200,
                chunk_overlap=30
            )

            text = "è¿™æ˜¯ç¬¬ä¸€å¥è¯ã€‚è¿™æ˜¯ç¬¬äºŒå¥è¯ï¼è¿™æ˜¯ç¬¬ä¸‰å¥è¯ï¼ŸNLTKåˆ†å‰²å™¨èƒ½å¤Ÿæ™ºèƒ½åœ°è¯†åˆ«å¥å­è¾¹ç•Œã€‚å®ƒä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯æ¥ç¡®ä¿åˆ†å‰²çš„å‡†ç¡®æ€§ã€‚è¿™æ ·å¯ä»¥ä¿æŒå¥å­çš„å®Œæ•´æ€§ã€‚"

            chunks = nltk_splitter.split_text(text)
            print(f"æ–‡æœ¬åˆ†å‰²åå¾—åˆ° {len(chunks)} ä¸ªå—")
            for i, chunk in enumerate(chunks):
                print(f"å— {i+1}: {chunk[:50]}...")

            return chunks
        except Exception as e:
            print(f"NLTKTextSplitteréœ€è¦nltkåº“å’Œæ•°æ®åŒ…: {e}")
            print("è¯·è¿è¡Œ: pip install nltk && python -c 'import nltk; nltk.download(\"punkt\")'")
            return []

    def _spacy_splitter_example(self):
        """SpacyTextSplitter ç¤ºä¾‹"""
        print("\n=== SpaCyTextSplitter ç¤ºä¾‹ ===")

        try:
            spacy_splitter = SpacyTextSplitter(
                chunk_size=200,
                chunk_overlap=30,
                pipeline="zh_core_web_sm"  # ä¸­æ–‡æ¨¡å‹
            )

            text = "è¿™æ˜¯ä¸€æ®µéœ€è¦ä½¿ç”¨spaCyè¿›è¡Œæ™ºèƒ½åˆ†å‰²çš„ä¸­æ–‡æ–‡æœ¬ã€‚spaCyæ˜¯ä¸€ä¸ªå¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†åº“ï¼Œå®ƒæä¾›äº†é«˜è´¨é‡çš„è¯­è¨€åˆ†æåŠŸèƒ½ã€‚"

            chunks = spacy_splitter.split_text(text)
            print(f"æ–‡æœ¬åˆ†å‰²åå¾—åˆ° {len(chunks)} ä¸ªå—")
            for i, chunk in enumerate(chunks):
                print(f"å— {i+1}: {chunk[:50]}...")

            return chunks
        except Exception as e:
            print(f"SpacyTextSplitteréœ€è¦spacyåº“å’Œè¯­è¨€æ¨¡å‹: {e}")
            print("è¯·è¿è¡Œ: pip install spacy && python -m spacy download zh_core_web_sm")
            return []

    def _semantic_chunker_example(self):
        """SemanticChunker ç¤ºä¾‹"""
        print("\n=== SemanticChunker ç¤ºä¾‹ ===")

        try:
            # éœ€è¦OpenAI APIå¯†é’¥
            embeddings = OpenAIEmbeddings()
            semantic_splitter = SemanticChunker(
                embeddings=embeddings,
                breakpoint_threshold_type="percentile"
            )

            text = """è¿™æ˜¯ä¸€æ®µéœ€è¦åŸºäºè¯­ä¹‰è¿›è¡Œåˆ†å‰²çš„é•¿æ–‡æœ¬ã€‚
            è¯­ä¹‰åˆ†å‰²å™¨èƒ½å¤Ÿè¯†åˆ«æ–‡æœ¬ä¸­è¯­ä¹‰ç›¸ä¼¼çš„éƒ¨åˆ†ï¼Œ
            å¹¶å°†å®ƒä»¬ç»„ç»‡åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ªæœ‰æ„ä¹‰çš„æ–‡æœ¬å—ã€‚
            è¿™ç§æ–¹æ³•ç‰¹åˆ«é€‚åˆæ„å»ºé«˜è´¨é‡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)ç³»ç»Ÿï¼Œ
            å› ä¸ºå®ƒèƒ½å¤Ÿä¿æŒä¸Šä¸‹æ–‡çš„è¿è´¯æ€§å’Œè¯­ä¹‰çš„å®Œæ•´æ€§ã€‚
            ç›¸æ¯”ä¼ ç»Ÿçš„åŸºäºå­—ç¬¦æˆ–ä»¤ç‰Œçš„åˆ†å‰²æ–¹æ³•ï¼Œ
            è¯­ä¹‰åˆ†å‰²èƒ½å¤Ÿäº§ç”Ÿæ›´é«˜è´¨é‡çš„æ–‡æœ¬å—ï¼Œ
            ä»è€Œæé«˜æ£€ç´¢å’Œç”Ÿæˆçš„è´¨é‡ã€‚"""

            chunks = semantic_splitter.split_text(text)
            print(f"æ–‡æœ¬åˆ†å‰²åå¾—åˆ° {len(chunks)} ä¸ªå—")
            for i, chunk in enumerate(chunks):
                print(f"å— {i+1}: {chunk[:50]}...")

            return chunks
        except Exception as e:
            print(f"SemanticChunkeréœ€è¦OpenAI APIå¯†é’¥å’Œä¾èµ–: {e}")
            return []

    def print_quick_reference_table(self):
        """æ‰“å°å¿«é€ŸæŸ¥è¡¨"""
        print("\n" + "="*80)
        print("ğŸ“Š LangChain æ–‡æœ¬åˆ†å‰²å™¨å¿«é€ŸæŸ¥è¡¨")
        print("="*80)

        # åˆ›å»ºè¡¨æ ¼æ•°æ®
        table_data = []
        for name, info in self.splitter_info.items():
            table_data.append({
                "åˆ†å‰²å™¨ç±»å‹": name,
                "ä¸»è¦ç”¨é€”": info["use_case"],
                "æ€§èƒ½": info["performance"],
                "ä¼˜ç‚¹": ", ".join(info["pros"]),
                "ç¼ºç‚¹": ", ".join(info["cons"])
            })

        # è½¬æ¢ä¸ºDataFrameå¹¶æ‰“å°
        df = pd.DataFrame(table_data)
        pd.set_option('display.max_columns', None)
        pd.set_option('display.width', None)
        pd.set_option('display.max_colwidth', 50)

        print(df.to_string(index=False))
        print("="*80)

    def print_detailed_info(self, splitter_name: Optional[str] = None):
        """æ‰“å°è¯¦ç»†çš„åˆ†å‰²å™¨ä¿¡æ¯"""
        if splitter_name:
            if splitter_name in self.splitter_info:
                self._print_single_splitter_info(splitter_name, self.splitter_info[splitter_name])
            else:
                print(f"æœªæ‰¾åˆ°åˆ†å‰²å™¨: {splitter_name}")
                print(f"å¯ç”¨çš„åˆ†å‰²å™¨: {list(self.splitter_info.keys())}")
        else:
            for name, info in self.splitter_info.items():
                self._print_single_splitter_info(name, info)
                print("\n" + "-"*60)

    def _print_single_splitter_info(self, name: str, info: Dict[str, Any]):
        """æ‰“å°å•ä¸ªåˆ†å‰²å™¨çš„è¯¦ç»†ä¿¡æ¯"""
        print(f"\nğŸ”§ {name}")
        print(f"ğŸ“ æè¿°: {info['description']}")
        print(f"ğŸ¯ ç”¨é€”: {info['use_case']}")
        print(f"âš¡ æ€§èƒ½: {info['performance']}")
        print(f"âœ… ä¼˜ç‚¹: {', '.join(info['pros'])}")
        print(f"âŒ ç¼ºç‚¹: {', '.join(info['cons'])}")

        print("\nğŸ“‹ å‚æ•°è¯´æ˜:")
        for param, details in info["parameters"].items():
            print(f"  â€¢ {param} ({details['type']}): {details['description']}")
            if details['default'] is not None:
                print(f"    é»˜è®¤å€¼: {details['default']}")

    def run_all_examples(self):
        """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
        print("ğŸš€ è¿è¡Œæ‰€æœ‰åˆ†å‰²å™¨ç¤ºä¾‹...")

        for name, info in self.splitter_info.items():
            try:
                info["example"]()
                print(f"âœ… {name} ç¤ºä¾‹è¿è¡ŒæˆåŠŸ")
            except Exception as e:
                print(f"âŒ {name} ç¤ºä¾‹è¿è¡Œå¤±è´¥: {e}")

            print("\n" + "="*60)

    def get_recommended_splitter(self, use_case: str) -> str:
        """æ ¹æ®ç”¨ä¾‹æ¨èåˆ†å‰²å™¨"""
        recommendations = {
            "é€šç”¨": "RecursiveCharacterTextSplitter",
            "html": "HTMLHeaderTextSplitter",
            "markdown": "MarkdownTextSplitter",
            "python": "PythonCodeTextSplitter",
            "ä»£ç ": "PythonCodeTextSplitter",
            "ä»¤ç‰Œ": "TokenTextSplitter",
            "è¯­ä¹‰": "SemanticChunker",
            "å¿«é€Ÿ": "CharacterTextSplitter",
            "nlp": "NLTKTextSplitter",
            "è¯­è¨€å­¦": "SpacyTextSplitter"
        }

        use_case_lower = use_case.lower()
        for key, splitter in recommendations.items():
            if key in use_case_lower:
                return splitter

        return "RecursiveCharacterTextSplitter"  # é»˜è®¤æ¨è


def create_splitter_config_template() -> str:
    """åˆ›å»ºåˆ†å‰²å™¨é…ç½®æ¨¡æ¿"""
    return '''
# åˆ†å‰²å™¨é…ç½®æ¨¡æ¿
def create_splitter(splitter_type="recursive", **kwargs):
    """æ ¹æ®ç±»å‹åˆ›å»ºåˆ†å‰²å™¨

    Args:
        splitter_type: åˆ†å‰²å™¨ç±»å‹
        **kwargs: é¢å¤–å‚æ•°

    Returns:
        å¯¹åº”çš„åˆ†å‰²å™¨å®ä¾‹
    """
    defaults = {
        "chunk_size": 1000,
        "chunk_overlap": 200,
        "length_function": len
    }
    defaults.update(kwargs)

    if splitter_type == "character":
        return CharacterTextSplitter(**defaults)
    elif splitter_type == "recursive":
        return RecursiveCharacterTextSplitter(**defaults)
    elif splitter_type == "html":
        return HTMLHeaderTextSplitter(
            headers_to_split_on=defaults.get("headers_to_split_on", [
                ("h1", "Header 1"), ("h2", "Header 2")
            ])
        )
    elif splitter_type == "markdown":
        return MarkdownTextSplitter(**defaults)
    elif splitter_type == "python":
        return PythonCodeTextSplitter(**defaults)
    elif splitter_type == "token":
        return TokenTextSplitter(
            chunk_size=defaults["chunk_size"],
            chunk_overlap=defaults["chunk_overlap"],
            model_name=defaults.get("model_name", "gpt-3.5-turbo")
        )
    # æ·»åŠ å…¶ä»–åˆ†å‰²å™¨...
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å‰²å™¨ç±»å‹: {splitter_type}")


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºåˆ†å‰²å™¨
    splitter = create_splitter(
        splitter_type="recursive",
        chunk_size=1500,
        chunk_overlap=300
    )

    # ä½¿ç”¨åˆ†å‰²å™¨
    text = "ä½ çš„é•¿æ–‡æœ¬å†…å®¹..."
    chunks = splitter.split_text(text)
    print(f"åˆ†å‰²å¾—åˆ° {len(chunks)} ä¸ªå—")
'''


if __name__ == "__main__":
    # åˆ›å»ºæŒ‡å—å®ä¾‹
    guide = SplitterGuide()

    print("ğŸ¯ LangChain æ–‡æœ¬åˆ†å‰²å™¨å®Œæ•´æŒ‡å—")
    print("="*50)

    # æ˜¾ç¤ºå¿«é€ŸæŸ¥è¡¨
    guide.print_quick_reference_table()

    # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
    print("\nğŸ“š å¯ç”¨æ“ä½œ:")
    print("1. æŸ¥çœ‹æ‰€æœ‰åˆ†å‰²å™¨è¯¦ç»†ä¿¡æ¯")
    print("2. æŸ¥çœ‹ç‰¹å®šåˆ†å‰²å™¨ä¿¡æ¯")
    print("3. è¿è¡Œæ‰€æœ‰ç¤ºä¾‹")
    print("4. è·å–åˆ†å‰²å™¨æ¨è")
    print("5. ç”Ÿæˆé…ç½®æ¨¡æ¿")

    # è¿™é‡Œå¯ä»¥æ·»åŠ ç”¨æˆ·äº¤äº’é€»è¾‘
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç›´æ¥æ˜¾ç¤ºä¸€äº›ä¿¡æ¯

    print("\nğŸ’¡ æ¨èåˆ†å‰²å™¨é€‰æ‹©:")
    print("â€¢ é€šç”¨åœºæ™¯: RecursiveCharacterTextSplitter")
    print("â€¢ HTMLæ–‡æ¡£: HTMLHeaderTextSplitter")
    print("â€¢ Markdown: MarkdownTextSplitter")
    print("â€¢ Pythonä»£ç : PythonCodeTextSplitter")
    print("â€¢ ä»¤ç‰Œæ§åˆ¶: TokenTextSplitter")
    print("â€¢ è¯­ä¹‰åˆ†å‰²: SemanticChunker")

    # ç”Ÿæˆé…ç½®æ¨¡æ¿
    print("\nğŸ“ é…ç½®æ¨¡æ¿:")
    print(create_splitter_config_template())