#!/usr/bin/env python3
"""
LangChain æ¨¡æ¿å’Œè®°å¿†ç®¡ç†ç¤ºä¾‹ï¼ˆä¿®å¤ç‰ˆï¼‰
å±•ç¤ºå¦‚ä½•ä½¿ç”¨PromptTemplateå’ŒMemoryæ¥ç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡
ä½¿ç”¨æ–°çš„LangChain API
"""

import os
import sys
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

# è®¾ç½®ç¼–ç 
if sys.platform == "win32":
    import locale
    locale.setlocale(locale.LC_ALL, 'Chinese (Simplified)_China.utf8')

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def basic_prompt_template_example():
    """åŸºç¡€æç¤ºæ¨¡æ¿ç¤ºä¾‹"""
    print("=== åŸºç¡€æç¤ºæ¨¡æ¿ç¤ºä¾‹ ===")

    llm = ChatOpenAI(
        model="glm-4.6",
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL"),
        temperature=0.7
    )

    # ç®€å•çš„æç¤ºæ¨¡æ¿
    simple_template = PromptTemplate(
        input_variables=["product", "feature"],
        template="è¯·ä¸º{product}çš„{feature}åŠŸèƒ½å†™ä¸€æ®µå®£ä¼ è¯­ï¼Œè¦æ±‚ç®€æ´æœ‰åŠ›ï¼Œä¸è¶…è¿‡50ä¸ªå­—ã€‚"
    )

    chain = simple_template | llm | StrOutputParser()

    # æµ‹è¯•ä¸åŒçš„äº§å“
    products = [
        {"product": "æ™ºèƒ½æ‰‹è¡¨", "feature": "å¿ƒç‡ç›‘æµ‹"},
        {"product": "æ™ºèƒ½æ‰‹æœº", "feature": "æ‹ç…§"},
        {"product": "æ™ºèƒ½éŸ³ç®±", "feature": "è¯­éŸ³åŠ©æ‰‹"}
    ]

    for item in products:
        try:
            result = chain.invoke(item)
            print(f"äº§å“: {item['product']} | åŠŸèƒ½: {item['feature']}")
            print(f"å®£ä¼ è¯­: {result}\n")
        except Exception as e:
            print(f"å¤„ç†äº§å“ {item['product']} æ—¶å‡ºé”™: {e}")

def chat_prompt_template_example():
    """èŠå¤©æç¤ºæ¨¡æ¿ç¤ºä¾‹"""
    print("=== èŠå¤©æç¤ºæ¨¡æ¿ç¤ºä¾‹ ===")

    llm = ChatOpenAI(
        model="glm-4.6",
        temperature=0.1,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # åˆ›å»ºèŠå¤©æ¨¡æ¿
    chat_template = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·è§£å†³æŠ€æœ¯é—®é¢˜ã€‚è¯·ç”¨å‹å¥½ã€ä¸“ä¸šçš„æ–¹å¼å›ç­”ã€‚"),
        ("human", "{question}")
    ])

    chain = chat_template | llm | StrOutputParser()

    # æŠ€æœ¯é—®é¢˜åˆ—è¡¨
    technical_questions = [
        "ä»€ä¹ˆæ˜¯Pythonè£…é¥°å™¨ï¼Ÿ",
        "å¦‚ä½•ä¼˜åŒ–ç½‘ç«™æ€§èƒ½ï¼Ÿ",
        "Gitå’ŒSVNæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    ]

    for question in technical_questions:
        try:
            result = chain.invoke({"question": question})
            print(f"é—®é¢˜: {question}")
            print(f"å›ç­”: {result}\n")
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")

def advanced_prompt_template_example():
    """é«˜çº§æç¤ºæ¨¡æ¿ç¤ºä¾‹"""
    print("=== é«˜çº§æç¤ºæ¨¡æ¿ç¤ºä¾‹ ===")

    llm = ChatOpenAI(
        model="glm-4.6",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # å¤æ‚çš„å¤šè§’è‰²æç¤ºæ¨¡æ¿
    role_playing_template = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„{role}ã€‚ä½ çš„ä¸“ä¸šé¢†åŸŸæ˜¯{specialty}ã€‚
è¯·ä»¥{tone}çš„è¯­æ°”å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶åŒ…å«{detail_level}çº§åˆ«çš„ç»†èŠ‚ã€‚"""),
        ("human", "{question}")
    ])

    chain = role_playing_template | llm | StrOutputParser()

    # ä¸åŒè§’è‰²é…ç½®
    role_configs = [
        {
            "role": "è½¯ä»¶å·¥ç¨‹å¸ˆ",
            "specialty": "Pythonå¼€å‘",
            "tone": "å‹å¥½ä¸“ä¸š",
            "detail_level": "è¯¦ç»†",
            "question": "å¦‚ä½•å†™å¥½çš„ä»£ç æ³¨é‡Šï¼Ÿ"
        },
        {
            "role": "äº§å“ç»ç†",
            "specialty": "ç”¨æˆ·ä½“éªŒè®¾è®¡",
            "tone": "åŠ¡å®å»ºè®®",
            "detail_level": "å…¨é¢",
            "question": "å¦‚ä½•æé«˜ç”¨æˆ·ç•™å­˜ç‡ï¼Ÿ"
        },
        {
            "role": "æ•°æ®ç§‘å­¦å®¶",
            "specialty": "æœºå™¨å­¦ä¹ ",
            "tone": "å­¦æœ¯ä¸¥è°¨",
            "detail_level": "æ·±åº¦",
            "question": "å¦‚ä½•è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Ÿ"
        }
    ]

    for config in role_configs:
        try:
            result = chain.invoke(config)
            print(f"è§’è‰²: {config['role']} ({config['specialty']})")
            print(f"é—®é¢˜: {config['question']}")
            print(f"å›ç­”: {result}\n")
        except Exception as e:
            print(f"å¤„ç†è§’è‰² {config['role']} æ—¶å‡ºé”™: {e}")

def conversation_buffer_memory_example():
    """å¯¹è¯ç¼“å†²è®°å¿†ç¤ºä¾‹ - ä½¿ç”¨æ–°API"""
    print("=== å¯¹è¯ç¼“å†²è®°å¿†ç¤ºä¾‹ ===")

    llm = ChatOpenAI(
        model="glm-4.6",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # åˆ›å»ºæ¶ˆæ¯å†å²
    message_history = InMemoryChatMessageHistory()

    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚è¯·æ ¹æ®å¯¹è¯å†å²å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}")
    ])

    # åˆ›å»ºé“¾
    chain = prompt | llm | StrOutputParser()

    # åˆ›å»ºå¸¦æ¶ˆæ¯å†å²çš„å¯è¿è¡Œå¯¹è±¡
    runnable_with_history = RunnableWithMessageHistory(
        chain,
        lambda session_id: message_history,
        input_messages_key="input",
        history_messages_key="history",
    )

    # æ¨¡æ‹Ÿå¯¹è¯
    dialogues = [
        "ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ ç¼–ç¨‹",
        "Pythonå’ŒJavaå“ªä¸ªæ›´é€‚åˆåˆå­¦è€…ï¼Ÿ",
        "å­¦ä¹ Pythonéœ€è¦ä»€ä¹ˆåŸºç¡€çŸ¥è¯†ï¼Ÿ",
        "ä½ åˆšæ‰æ¨èäº†ä»€ä¹ˆï¼Ÿ",
        "èƒ½è¯¦ç»†è¯´è¯´é‚£ä¸ªå»ºè®®å—ï¼Ÿ"
    ]

    print("å¼€å§‹å¯¹è¯...")
    for i, user_input in enumerate(dialogues, 1):
        try:
            response = runnable_with_history.invoke(
                {"input": user_input},
                config={"configurable": {"session_id": "test_session"}}
            )
            print(f"è½®æ¬¡ {i}:")
            print(f"ç”¨æˆ·: {user_input}")
            print(f"AIåŠ©æ‰‹: {response}\n")
        except Exception as e:
            print(f"å¤„ç†å¯¹è¯è½®æ¬¡ {i} æ—¶å‡ºé”™: {e}")

def window_memory_example():
    """çª—å£è®°å¿†ç¤ºä¾‹ - ç®€åŒ–ç‰ˆæœ¬"""
    print("=== çª—å£è®°å¿†ç¤ºä¾‹ ===")

    llm = ChatOpenAI(
        model="glm-4.6",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # åˆ›å»ºæ¶ˆæ¯å†å²ï¼ˆæ‰‹åŠ¨ç®¡ç†çª—å£ï¼‰
    message_history = InMemoryChatMessageHistory()
    window_size = 3  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯

    def get_recent_messages():
        """è·å–æœ€è¿‘çš„æ¶ˆæ¯"""
        messages = message_history.messages
        # ä¿ç•™æœ€è¿‘6æ¡æ¶ˆæ¯ï¼ˆç”¨æˆ·+AIå„3æ¡ï¼‰
        return messages[-window_size*2:] if len(messages) > window_size*2 else messages

    # åˆ›å»ºæç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚ä½ åªèƒ½è®°ä½æœ€è¿‘{window_size}è½®å¯¹è¯ã€‚è¯·æ ¹æ®æœ€è¿‘çš„å¯¹è¯å†å²å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}")
    ])

    # åˆ›å»ºé“¾
    chain = prompt | llm | StrOutputParser()

    # é•¿å¯¹è¯æµ‹è¯•
    long_dialogues = [
        "æˆ‘çš„åå­—æ˜¯å¼ ä¸‰",
        "æˆ‘å–œæ¬¢å­¦ä¹ ç¼–ç¨‹",
        "æˆ‘æ­£åœ¨å­¦ä¹ Python",
        "Pythonæ˜¯ä¸€é—¨å¾ˆå¥½çš„è¯­è¨€",
        "æˆ‘ä¹Ÿå–œæ¬¢æœºå™¨å­¦ä¹ ",
        "æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£",
        "æˆ‘åˆšæ‰è¯´æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ",  # è¿™ä¸ªé—®é¢˜AIå¯èƒ½è®°ä¸ä½ï¼Œå› ä¸ºè¶…è¿‡äº†3è½®
        "æˆ‘åˆšæ‰è¯´æˆ‘å–œæ¬¢ä»€ä¹ˆï¼Ÿ",    # è¿™ä¸ªå¯èƒ½ä¹Ÿè®°ä¸ä½
        "æœºå™¨å­¦ä¹ æ€ä¹ˆæ ·ï¼Ÿ"         # è¿™ä¸ªåº”è¯¥èƒ½å›ç­”ï¼Œå› ä¸ºæœ€è¿‘æåˆ°äº†
    ]

    print("å¼€å§‹é•¿å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(long_dialogues, 1):
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            message_history.add_user_message(user_input)

            # è·å–æœ€è¿‘æ¶ˆæ¯
            recent_messages = get_recent_messages()

            # è°ƒç”¨é“¾
            response = chain.invoke({
                "input": user_input,
                "history": recent_messages
            })

            # æ·»åŠ AIå›å¤
            message_history.add_ai_message(response)

            print(f"è½®æ¬¡ {i}:")
            print(f"ç”¨æˆ·: {user_input}")
            print(f"AIåŠ©æ‰‹: {response}\n")

        except Exception as e:
            print(f"å¤„ç†å¯¹è¯è½®æ¬¡ {i} æ—¶å‡ºé”™: {e}")

def summary_memory_example():
    """æ‘˜è¦è®°å¿†ç¤ºä¾‹ - ç®€åŒ–ç‰ˆæœ¬"""
    print("=== æ‘˜è¦è®°å¿†ç¤ºä¾‹ ===")

    llm = ChatOpenAI(
        model="glm-4.6",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # æ‰‹åŠ¨ç®¡ç†æ‘˜è¦
    conversation_summary = ""
    message_count = 0
    summary_frequency = 3  # æ¯3è½®å¯¹è¯æ›´æ–°ä¸€æ¬¡æ‘˜è¦

    def update_summary(user_input, ai_response):
        nonlocal conversation_summary, message_count
        message_count += 1

        # æ·»åŠ åˆ°æ‘˜è¦
        if conversation_summary:
            conversation_summary += f"\nç”¨æˆ·: {user_input}\nAI: {ai_response}"
        else:
            conversation_summary = f"ç”¨æˆ·: {user_input}\nAI: {ai_response}"

        # æ¯3è½®å¯¹è¯ç”Ÿæˆæ–°æ‘˜è¦
        if message_count % summary_frequency == 0:
            try:
                summary_prompt = f"""è¯·å°†ä»¥ä¸‹å¯¹è¯å†…å®¹æ€»ç»“ä¸ºç®€æ´çš„æ‘˜è¦ï¼š

{conversation_summary}

æ‘˜è¦ï¼š"""

                new_summary = llm.invoke(summary_prompt)
                conversation_summary = f"å¯¹è¯æ‘˜è¦: {new_summary.content}"

                print(f"æ›´æ–°æ‘˜è¦: {conversation_summary}")
                print("-" * 50)

            except Exception as e:
                print(f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {e}")

    # å¤æ‚çš„å¤šè¯é¢˜å¯¹è¯
    multi_topic_dialogues = [
        "æˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„å‘å±•å†å²",
        "äººå·¥æ™ºèƒ½çš„é‡Œç¨‹ç¢‘äº‹ä»¶æœ‰å“ªäº›ï¼Ÿ",
        "ç°åœ¨è®©æˆ‘èŠèŠæœºå™¨å­¦ä¹ ",
        "æœºå™¨å­¦ä¹ çš„ä¸»è¦ç±»å‹æœ‰å“ªäº›ï¼Ÿ",
        "æˆ‘è¿˜æƒ³äº†è§£æ·±åº¦å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ ¹æ®æˆ‘ä»¬ä¹‹å‰çš„æ‰€æœ‰å¯¹è¯ï¼Œè¯·æ€»ç»“ä¸€ä¸‹AIã€MLã€DLçš„å…³ç³»"
    ]

    print("å¼€å§‹å¤šè¯é¢˜å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(multi_topic_dialogues, 1):
        try:
            # åˆ›å»ºåŒ…å«æ‘˜è¦çš„æç¤º
            if conversation_summary:
                system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯ä¹‹å‰çš„å¯¹è¯æ‘˜è¦ï¼š

{conversation_summary}

è¯·åŸºäºè¿™ä¸ªæ‘˜è¦å’Œå½“å‰é—®é¢˜å›ç­”ç”¨æˆ·ã€‚"""
            else:
                system_prompt = "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ã€‚è¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"

            prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                ("human", "{input}")
            ])

            chain = prompt | llm | StrOutputParser()

            response = chain.invoke({"input": user_input})

            print(f"è½®æ¬¡ {i}:")
            print(f"ç”¨æˆ·: {user_input}")
            print(f"AIåŠ©æ‰‹: {response}\n")

            # æ›´æ–°æ‘˜è¦
            update_summary(user_input, response)

        except Exception as e:
            print(f"å¤„ç†å¯¹è¯è½®æ¬¡ {i} æ—¶å‡ºé”™: {e}")

def custom_memory_example():
    """è‡ªå®šä¹‰è®°å¿†ç¤ºä¾‹"""
    print("=== è‡ªå®šä¹‰è®°å¿†ç¤ºä¾‹ ===")

    llm = ChatOpenAI(
        model="glm-4.6",
        temperature=0.7,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # åˆ›å»ºè‡ªå®šä¹‰æ¶ˆæ¯å†å²
    message_history = InMemoryChatMessageHistory()

    # å­˜å‚¨é‡è¦ä¿¡æ¯
    important_info = {}

    # åˆ›å»ºèŠå¤©æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªæœ‰è®°å¿†çš„AIåŠ©æ‰‹ã€‚è¯·è®°ä½å¯¹è¯çš„é‡è¦ä¿¡æ¯ï¼Œå¹¶åœ¨é€‚å½“æ—¶å€™å¼•ç”¨ä¹‹å‰çš„å†…å®¹ã€‚

é‡è¦ä¿¡æ¯è®°å½•ï¼š
{important_info}

å¯¹è¯å†å²ï¼š
{history}"""),
        ("human", "{input}")
    ])

    # åˆ›å»ºé“¾
    chain = prompt | llm | StrOutputParser()

    # è‡ªå®šä¹‰å¯¹è¯ç®¡ç†å‡½æ•°
    def chat_with_memory(user_input: str):
        """å¸¦è®°å¿†çš„èŠå¤©å‡½æ•°"""
        # æå–é‡è¦ä¿¡æ¯
        if "æˆ‘å«" in user_input or "æˆ‘çš„åå­—æ˜¯" in user_input:
            # æå–åå­—
            for word in user_input.split():
                if len(word) > 1 and word not in ["æˆ‘å«", "åå­—", "æ˜¯", "æˆ‘çš„"]:
                    important_info["ç”¨æˆ·å§“å"] = word
                    break

        if "ä½¿ç”¨" in user_input or "æ‡‚" in user_input or "ä¼š" in user_input:
            # æå–æŠ€èƒ½
            important_info["ç”¨æˆ·æŠ€èƒ½"] = user_input

        # è°ƒç”¨é“¾è·å–å“åº”
        response = chain.invoke({
            "input": user_input,
            "history": message_history.messages,
            "important_info": str(important_info) if important_info else "æš‚æ— é‡è¦ä¿¡æ¯"
        })

        return response

    # æµ‹è¯•è®°å¿†åŠŸèƒ½
    memory_test_dialogues = [
        "æˆ‘å«æå››ï¼Œæ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆ",
        "æˆ‘ä¸»è¦ä½¿ç”¨Reactè¿›è¡Œå‰ç«¯å¼€å‘",
        "æˆ‘è¿˜æ‡‚ä¸€äº›Node.js",
        "æ ¹æ®æˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯ï¼Œä½ èƒ½ä»‹ç»ä¸€ä¸‹æˆ‘å—ï¼Ÿ",
        "æˆ‘åˆšæ‰æåˆ°æˆ‘æ‡‚ä»€ä¹ˆåç«¯æŠ€æœ¯ï¼Ÿ",
        "æˆ‘æƒ³æå‡æˆ‘çš„å‰ç«¯æŠ€èƒ½ï¼Œæœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ"
    ]

    print("å¼€å§‹è‡ªå®šä¹‰è®°å¿†å¯¹è¯æµ‹è¯•...")
    for i, user_input in enumerate(memory_test_dialogues, 1):
        try:
            response = chat_with_memory(user_input)

            # æ·»åŠ æ¶ˆæ¯åˆ°å†å²
            message_history.add_user_message(user_input)
            message_history.add_ai_message(response)

            print(f"è½®æ¬¡ {i}:")
            print(f"ç”¨æˆ·: {user_input}")
            print(f"AIåŠ©æ‰‹: {response}\n")

            # æ˜¾ç¤ºé‡è¦ä¿¡æ¯
            if i == 3:  # ç¬¬ä¸‰è½®åæ˜¾ç¤ºè®°å½•çš„ä¿¡æ¯
                print("ğŸ“ è®°å½•çš„é‡è¦ä¿¡æ¯:")
                for key, value in important_info.items():
                    print(f"  {key}: {value}")
                print("-" * 50 + "\n")

        except Exception as e:
            print(f"å¤„ç†å¯¹è¯è½®æ¬¡ {i} æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    print("æ¬¢è¿æ¥åˆ°LangChainæ¨¡æ¿å’Œè®°å¿†ç®¡ç†å­¦ä¹ ä¸–ç•Œï¼")

    # è¿è¡ŒåŸºç¡€æç¤ºæ¨¡æ¿ç¤ºä¾‹
    basic_prompt_template_example()

    print("\n" + "="*50 + "\n")

    # è¿è¡ŒèŠå¤©æç¤ºæ¨¡æ¿ç¤ºä¾‹
    chat_prompt_template_example()

    print("\n" + "="*50 + "\n")

    # è¿è¡Œé«˜çº§æç¤ºæ¨¡æ¿ç¤ºä¾‹
    advanced_prompt_template_example()

    print("\n" + "="*50 + "\n")

    # è¿è¡Œå¯¹è¯ç¼“å†²è®°å¿†ç¤ºä¾‹
    conversation_buffer_memory_example()

    print("\n" + "="*50 + "\n")

    # è¿è¡Œçª—å£è®°å¿†ç¤ºä¾‹
    window_memory_example()

    print("\n" + "="*50 + "\n")

    # è¿è¡Œæ‘˜è¦è®°å¿†ç¤ºä¾‹
    summary_memory_example()

    print("\n" + "="*50 + "\n")

    # è¿è¡Œè‡ªå®šä¹‰è®°å¿†ç¤ºä¾‹
    custom_memory_example()

    print("\næ¨¡æ¿å’Œè®°å¿†ç®¡ç†ç¤ºä¾‹å®Œæˆï¼æ‚¨å·²ç»å­¦ä¼šäº†å¦‚ä½•åœ¨LangChainä¸­åˆ›å»ºå’Œç®¡ç†å„ç§æ¨¡æ¿åŠè®°å¿†ç³»ç»Ÿã€‚")