#!/usr/bin/env python3
"""
LangChain åŸºç¡€ç¤ºä¾‹ - ç®€å•LLMè°ƒç”¨
è¿™æ˜¯LangChainçš„å…¥é—¨ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨LangChainè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹
"""

import os
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
    
def basic_llm_example():
    """åŸºç¡€LLMè°ƒç”¨ç¤ºä¾‹"""
    print("=== LangChain åŸºç¡€LLMè°ƒç”¨ç¤ºä¾‹ ===\n")

    # åˆ›å»ºChatAnthropicå®ä¾‹
    # æ³¨æ„ï¼šæ‚¨éœ€è¦åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®GLM_API_KEY
    llm = ChatOpenAI(
        model="glm-4.5",
        temperature=0.1,
        max_tokens=100,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # åˆ›å»ºæ¶ˆæ¯
    message = HumanMessage(content="ä½ å¥½ï¼è¯·ç”¨ä¸­æ–‡ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")


    # è°ƒç”¨æ¨¡å‹
    try:
        response = llm.invoke([message])
        print(f"ç”¨æˆ·: {message.content}")
        print(f"AIåŠ©æ‰‹: {response.content}\n")
    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}")
        print("è¯·ç¡®ä¿æ‚¨å·²ç»è®¾ç½®äº†æœ‰æ•ˆçš„GLM APIå¯†é’¥ã€‚")

def multiple_questions_example():
    """å¤šä¸ªé—®é¢˜ç¤ºä¾‹"""
    print("=== å¤šä¸ªé—®é¢˜ç¤ºä¾‹ ===\n")

    # åˆ›å»ºChatAnthropicå®ä¾‹
    llm = ChatOpenAI(
        name ="glm-4",  # æ™ºè°±AIæ”¯æŒçš„æ¨¡å‹
        verbose= True,
        temperature=0.7,  # æ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼Œ0-1ä¹‹é—´
        max_completion_tokens= 150,    # é™åˆ¶è¾“å‡ºé•¿åº¦
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # å‡†å¤‡å¤šä¸ªé—®é¢˜
    questions = [
        "ä»€ä¹ˆæ˜¯LangChainï¼Ÿ",
        "LangChainæœ‰å“ªäº›ä¸»è¦åŠŸèƒ½ï¼Ÿ",
        "å¦‚ä½•å¼€å§‹ä½¿ç”¨LangChainï¼Ÿ"
    ]

    for i, question in enumerate(questions, 1):
        message = HumanMessage(content=question)
        try:
            response = llm.invoke([message])
            print(f"é—®é¢˜ {i}: {question}")
            print(f"å›ç­” {i}: {response.content}\n")
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜ {i} æ—¶å‡ºé”™: {e}")

def custom_parameters_example():
    """è‡ªå®šä¹‰å‚æ•°ç¤ºä¾‹"""
    print("=== è‡ªå®šä¹‰å‚æ•°ç¤ºä¾‹ ===\n")

    # åŒä¸€ä¸ªé—®é¢˜ï¼Œä¸åŒçš„temperatureè®¾ç½®
    question = "è¯·ç”¨åˆ›æ„çš„æ–¹å¼æè¿°ä¸€ä¸‹ç¼–ç¨‹çš„ä¹è¶£ã€‚"

    # ä½temperature - æ›´ç¡®å®šæ€§çš„å›ç­”
    llm_deterministic = ChatOpenAI(
        model="glm-4",
        temperature=0.1,
        max_tokens=100,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    # é«˜temperature - æ›´æœ‰åˆ›æ„çš„å›ç­”
    llm_creative = ChatOpenAI(
        model="glm-4",
        temperature=1.0,
        max_tokens=100,
        openai_api_key=os.getenv("GLM_API_KEY"),
        openai_api_base=os.getenv("GLM_BASE_URL")
    )

    message = HumanMessage(content=question)

    try:
        print("é—®é¢˜:", question)
        print("\nä½temperature (0.1) - ç¡®å®šæ€§å›ç­”:")
        response1 = llm_deterministic.invoke([message])
        print(response1.content)

        print("\né«˜temperature (1.0) - åˆ›æ„å›ç­”:")
        response2 = llm_creative.invoke([message])
        print(response2.content)

    except Exception as e:
        print(f"è°ƒç”¨æ¨¡å‹æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import sys
    import io
    # è®¾ç½®UTF-8ç¼–ç è¾“å‡º
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("ğŸ¤– æ¬¢è¿æ¥åˆ°LangChainå­¦ä¹ ä¸–ç•Œï¼\n")

    # è¿è¡ŒåŸºç¡€ç¤ºä¾‹
    basic_llm_example()

    print("\n" + "="*50 + "\n")

    # è¿è¡Œå¤šä¸ªé—®é¢˜ç¤ºä¾‹
    #multiple_questions_example()

    print("\n" + "="*50 + "\n")

    # è¿è¡Œè‡ªå®šä¹‰å‚æ•°ç¤ºä¾‹
    #custom_parameters_example()

    print("\nâœ¨ ç¤ºä¾‹å®Œæˆï¼æ‚¨å·²ç»å­¦ä¼šäº†å¦‚ä½•åœ¨LangChainä¸­è¿›è¡ŒåŸºç¡€çš„LLMè°ƒç”¨ã€‚")