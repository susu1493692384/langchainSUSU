#!/usr/bin/env python3
"""
LangChain å‘é‡å­˜å‚¨å’Œæ£€ç´¢ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œæ–‡æ¡£æ£€ç´¢å’Œé—®ç­”
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.vectorstores import FAISS, Chroma
from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.chains import RetrievalQA, ConversationalRetrievalChain
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.schema import Document
import json
from typing import List, Dict

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def create_sample_documents():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£é›†åˆ"""
    print("=== åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ ===\n")

    documents = [
        Document(
            page_content="""
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
AIåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å­é¢†åŸŸã€‚

æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
            """,
            metadata={"source": "AIåŸºç¡€çŸ¥è¯†", "category": "AIæ¦‚è¿°"}
        ),
        Document(
            page_content="""
Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½è€Œé—»åã€‚
Pythonåœ¨æ•°æ®ç§‘å­¦ã€æœºå™¨å­¦ä¹ ã€Webå¼€å‘ã€è‡ªåŠ¨åŒ–è„šæœ¬ç­‰é¢†åŸŸå¹¿æ³›ä½¿ç”¨ã€‚

Pythonçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
- ç®€æ´æ˜“è¯»çš„è¯­æ³•
- ä¸°å¯Œçš„æ ‡å‡†åº“
- è·¨å¹³å°å…¼å®¹æ€§
- å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ
- å¤§é‡ç¬¬ä¸‰æ–¹åº“å’Œæ¡†æ¶

åœ¨AIå¼€å‘ä¸­ï¼ŒTensorFlowã€PyTorchã€Scikit-learnç­‰æµè¡Œçš„æœºå™¨å­¦ä¹ åº“éƒ½æä¾›Pythonæ¥å£ã€‚
            """,
            metadata={"source": "Pythonç¼–ç¨‹", "category": "ç¼–ç¨‹è¯­è¨€"}
        ),
        Document(
            page_content="""
æ•°æ®ç§‘å­¦æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘é¢†åŸŸï¼Œç»“åˆäº†ç»Ÿè®¡å­¦ã€æ•°å­¦ã€ç¼–ç¨‹å’Œé¢†åŸŸçŸ¥è¯†æ¥ä»æ•°æ®ä¸­æå–è§è§£ã€‚

æ•°æ®ç§‘å­¦å·¥ä½œæµç¨‹åŒ…æ‹¬ï¼š
1. æ•°æ®æ”¶é›†å’Œæ¸…æ´—
2. æ¢ç´¢æ€§æ•°æ®åˆ†æ
3. ç‰¹å¾å·¥ç¨‹
4. æ¨¡å‹æ„å»ºå’Œè®­ç»ƒ
5. æ¨¡å‹è¯„ä¼°å’Œä¼˜åŒ–
6. éƒ¨ç½²å’Œç›‘æ§

å¸¸ç”¨çš„æ•°æ®ç§‘å­¦å·¥å…·åŒ…æ‹¬ï¼š
- ç¼–ç¨‹è¯­è¨€ï¼šPythonã€R
- æ•°æ®å¤„ç†ï¼šPandasã€NumPy
- å¯è§†åŒ–ï¼šMatplotlibã€Seaborn
- æœºå™¨å­¦ä¹ ï¼šScikit-learnã€TensorFlow
            """,
            metadata={"source": "æ•°æ®ç§‘å­¦", "category": "æ•°æ®ç§‘å­¦"}
        ),
        Document(
            page_content="""
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚

æ·±åº¦å­¦ä¹ çš„ä¸»è¦æ¶æ„åŒ…æ‹¬ï¼š
- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼šä¸»è¦ç”¨äºå›¾åƒå¤„ç†
- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼šä¸»è¦ç”¨äºåºåˆ—æ•°æ®å¤„ç†
- Transformerï¼šä¸»è¦ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†
- ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ï¼šä¸»è¦ç”¨äºç”Ÿæˆæ¨¡å‹

æ·±åº¦å­¦ä¹ åœ¨ä»¥ä¸‹é¢†åŸŸå–å¾—äº†é‡å¤§çªç ´ï¼š
- å›¾åƒè¯†åˆ«å’Œåˆ†ç±»
- è‡ªç„¶è¯­è¨€å¤„ç†
- è¯­éŸ³è¯†åˆ«
- è‡ªåŠ¨é©¾é©¶
- åŒ»ç–—è¯Šæ–­
            """,
            metadata={"source": "æ·±åº¦å­¦ä¹ ", "category": "æœºå™¨å­¦ä¹ "}
        ),
        Document(
            page_content="""
æç¤ºè¯å·¥ç¨‹æ˜¯è®¾è®¡å’Œä¼˜åŒ–AIæ¨¡å‹è¾“å…¥æç¤ºè¯çš„è‰ºæœ¯å’Œç§‘å­¦ã€‚
å¥½çš„æç¤ºè¯å¯ä»¥æ˜¾è‘—æé«˜AIæ¨¡å‹çš„è¾“å‡ºè´¨é‡å’Œç›¸å…³æ€§ã€‚

æœ‰æ•ˆçš„æç¤ºè¯è®¾è®¡åŸåˆ™ï¼š
1. æ˜ç¡®å…·ä½“ï¼šæ¸…æ¥šåœ°è¯´æ˜ä½ æƒ³è¦ä»€ä¹ˆ
2. æä¾›ä¸Šä¸‹æ–‡ï¼šç»™æ¨¡å‹è¶³å¤Ÿçš„ä¿¡æ¯
3. è®¾å®šè§’è‰²ï¼šå‘Šè¯‰æ¨¡å‹æ‰®æ¼”ä»€ä¹ˆè§’è‰²
4. æŒ‡å®šæ ¼å¼ï¼šè¦æ±‚ç‰¹å®šçš„è¾“å‡ºæ ¼å¼
5. ä½¿ç”¨ç¤ºä¾‹ï¼šæä¾›å¥½çš„ä¾‹å­ï¼ˆFew-shotå­¦ä¹ ï¼‰

æç¤ºè¯å·¥ç¨‹å¯¹äºæ„å»ºå¯é ã€ä¸€è‡´çš„AIåº”ç”¨è‡³å…³é‡è¦ã€‚
            """,
            metadata={"source": "æç¤ºè¯å·¥ç¨‹", "category": "AIåº”ç”¨"}
        )
    ]

    print(f"åˆ›å»ºäº† {len(documents)} ä¸ªç¤ºä¾‹æ–‡æ¡£")
    for doc in documents:
        print(f"- {doc.metadata['source']} ({doc.metadata['category']})")
    print()

    return documents

def text_splitting_example(documents):
    """æ–‡æœ¬åˆ†å‰²ç¤ºä¾‹"""
    print("=== æ–‡æœ¬åˆ†å‰²ç¤ºä¾‹ ===\n")

    # å­—ç¬¦åˆ†å‰²å™¨
    char_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=200,
        chunk_overlap=50,
        length_function=len
    )

    char_chunks = char_splitter.split_documents(documents)
    print(f"å­—ç¬¦åˆ†å‰²å™¨ç”Ÿæˆäº† {len(char_chunks)} ä¸ªæ–‡æ¡£å—")

    # é€’å½’å­—ç¬¦åˆ†å‰²å™¨
    recursive_splitter = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
        separators=["\n\n", "\n", " ", ""]
    )

    recursive_chunks = recursive_splitter.split_documents(documents)
    print(f"é€’å½’åˆ†å‰²å™¨ç”Ÿæˆäº† {len(recursive_chunks)} ä¸ªæ–‡æ¡£å—")

    # æ˜¾ç¤ºåˆ†å‰²ç»“æœ
    print("\nåˆ†å‰²ç»“æœç¤ºä¾‹:")
    for i, chunk in enumerate(recursive_chunks[:3]):
        print(f"å— {i+1} (æ¥æº: {chunk.metadata['source']}):")
        print(f"{chunk.page_content[:150]}...")
        print()

    return recursive_chunks

def vector_store_example(chunks):
    """å‘é‡å­˜å‚¨ç¤ºä¾‹"""
    print("=== å‘é‡å­˜å‚¨ç¤ºä¾‹ ===\n")

    # åˆ›å»ºåµŒå…¥æ¨¡å‹
    embeddings = OpenAIEmbeddings()

    # ä½¿ç”¨FAISSåˆ›å»ºå‘é‡å­˜å‚¨
    print("åˆ›å»ºFAISSå‘é‡å­˜å‚¨...")
    faiss_vectorstore = FAISS.from_documents(chunks, embeddings)
    print("FAISSå‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ")

    # ä½¿ç”¨Chromaåˆ›å»ºå‘é‡å­˜å‚¨
    print("åˆ›å»ºChromaå‘é‡å­˜å‚¨...")
    chroma_vectorstore = Chroma.from_documents(chunks, embeddings)
    print("Chromaå‘é‡å­˜å‚¨åˆ›å»ºå®Œæˆ")

    return faiss_vectorstore, chroma_vectorstore, embeddings

def similarity_search_example(vectorstore):
    """ç›¸ä¼¼æ€§æœç´¢ç¤ºä¾‹"""
    print("=== ç›¸ä¼¼æ€§æœç´¢ç¤ºä¾‹ ===\n")

    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "Pythonæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        "å¦‚ä½•å­¦ä¹ æ•°æ®ç§‘å­¦ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ åº”ç”¨åœ¨å“ªé‡Œï¼Ÿ",
        "æ€æ ·å†™å¥½çš„æç¤ºè¯ï¼Ÿ"
    ]

    for query in queries:
        print(f"æŸ¥è¯¢: {query}")

        # ç›¸ä¼¼æ€§æœç´¢
        docs = vectorstore.similarity_search(query, k=2)

        for i, doc in enumerate(docs, 1):
            print(f"  ç»“æœ {i} (æ¥æº: {doc.metadata['source']}):")
            print(f"    {doc.page_content[:100]}...")

        print()

def max_marginal_relevance_search_example(vectorstore):
    """æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ç¤ºä¾‹"""
    print("=== æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ç¤ºä¾‹ ===\n")

    query = "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»"

    print(f"æŸ¥è¯¢: {query}")

    # æ™®é€šç›¸ä¼¼æ€§æœç´¢
    print("\næ™®é€šç›¸ä¼¼æ€§æœç´¢ç»“æœ:")
    sim_docs = vectorstore.similarity_search(query, k=3)
    for i, doc in enumerate(sim_docs, 1):
        print(f"  {i}. {doc.metadata['source']}: {doc.page_content[:50]}...")

    # æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
    print("\næœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ç»“æœ (å¢åŠ å¤šæ ·æ€§):")
    mmr_docs = vectorstore.max_marginal_relevance_search(query, k=3, fetch_k=10)
    for i, doc in enumerate(mmr_docs, 1):
        print(f"  {i}. {doc.metadata['source']}: {doc.page_content[:50]}...")

    print()

def retrieval_qa_example(vectorstore):
    """æ£€ç´¢é—®ç­”é“¾ç¤ºä¾‹"""
    print("=== æ£€ç´¢é—®ç­”é“¾ç¤ºä¾‹ ===\n")

    # åˆ›å»ºLLM
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # åˆ›å»ºæ£€ç´¢é—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # å°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£"å¡"è¿›ä¸Šä¸‹æ–‡
        retriever=retriever,
        return_source_documents=True
    )

    # æµ‹è¯•é—®é¢˜
    questions = [
        "è¯·è§£é‡Šäººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ çš„å…³ç³»",
        "Pythonåœ¨æ•°æ®ç§‘å­¦ä¸­æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ çš„ä¸»è¦åº”ç”¨é¢†åŸŸæœ‰å“ªäº›ï¼Ÿ",
        "å¦‚ä½•æé«˜AIæ¨¡å‹çš„è¾“å‡ºè´¨é‡ï¼Ÿ"
    ]

    for question in questions:
        print(f"é—®é¢˜: {question}")
        try:
            result = qa_chain.invoke({"query": question})
            print(f"å›ç­”: {result['result']}")
            print("ç›¸å…³æ–‡æ¡£:")
            for doc in result['source_documents']:
                print(f"  - {doc.metadata['source']}")
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        print()

def custom_retrieval_chain_example(vectorstore, embeddings):
    """è‡ªå®šä¹‰æ£€ç´¢é“¾ç¤ºä¾‹"""
    print("=== è‡ªå®šä¹‰æ£€ç´¢é“¾ç¤ºä¾‹ ===\n")

    # åˆ›å»ºLLM
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # åˆ›å»ºä¸Šä¸‹æ–‡å‹ç¼©å™¨
    compressor = LLMChainExtractor.from_llm(llm)
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=retriever
    )

    # åˆ›å»ºè‡ªå®šä¹‰æç¤ºæ¨¡æ¿
    prompt = PromptTemplate(
        template="""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯´æ˜æ— æ³•å›ç­”ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}

è¯¦ç»†å›ç­”:""",
        input_variables=["context", "question"]
    )

    # åˆ›å»ºé“¾
    def retrieve_and_answer(question: str):
        """æ£€ç´¢å¹¶å›ç­”é—®é¢˜çš„è‡ªå®šä¹‰å‡½æ•°"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = retriever.get_relevant_documents(question)

        # å¦‚æœæ²¡æœ‰ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨å‹ç¼©æ£€ç´¢å™¨
        if not docs or len(docs[0].page_content) < 50:
            docs = compression_retriever.get_relevant_documents(question)

        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œè¿”å›æ— æ³•å›ç­”
        if not docs:
            return "æŠ±æ­‰ï¼Œæˆ‘æ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.page_content for doc in docs])

        # ç”Ÿæˆå›ç­”
        chain = prompt | llm | StrOutputParser()
        result = chain.invoke({"context": context, "question": question})

        return result, docs

    # æµ‹è¯•é—®é¢˜
    questions = [
        "æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "å¦‚ä½•åœ¨é¡¹ç›®ä¸­åº”ç”¨Pythonè¿›è¡Œæ•°æ®åˆ†æï¼Ÿ",
        "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ",
        "AIæ¨¡å‹çš„æ€§èƒ½å¦‚ä½•è¯„ä¼°ï¼Ÿ"
    ]

    for question in questions:
        print(f"é—®é¢˜: {question}")
        try:
            answer, docs = retrieve_and_answer(question)
            print(f"å›ç­”: {answer}")
            print("ç›¸å…³æ–‡æ¡£:")
            for doc in docs:
                print(f"  - {doc.metadata['source']}")
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        print()

def conversational_retrieval_example(vectorstore):
    """å¯¹è¯å¼æ£€ç´¢ç¤ºä¾‹"""
    print("=== å¯¹è¯å¼æ£€ç´¢ç¤ºä¾‹ ===\n")

    # åˆ›å»ºLLM
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # åˆ›å»ºå¯¹è¯å¼æ£€ç´¢é“¾
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True,
        verbose=False
    )

    # æ¨¡æ‹Ÿå¯¹è¯
    dialogues = [
        {"question": "æˆ‘æƒ³å­¦ä¹ AIï¼Œåº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ"},
        {"question": "Pythonåœ¨AIå¼€å‘ä¸­é‡è¦å—ï¼Ÿ"},
        {"question": "èƒ½æ¨èä¸€äº›å­¦ä¹ èµ„æºå—ï¼Ÿ"},
        {"question": "åˆšæ‰æåˆ°çš„ä¸»è¦å†…å®¹æœ‰å“ªäº›ï¼Ÿ"}  # è¿™ä¸ªé—®é¢˜éœ€è¦è®°å¿†
    ]

    chat_history = []

    for dialogue in dialogues:
        question = dialogue["question"]
        print(f"ç”¨æˆ·: {question}")

        try:
            result = conversation_chain.invoke({
                "question": question,
                "chat_history": chat_history
            })

            answer = result["answer"]
            print(f"AIåŠ©æ‰‹: {answer}")

            # æ›´æ–°å¯¹è¯å†å²
            chat_history.append((question, answer))

            print("ç›¸å…³æ–‡æ¡£:")
            for doc in result["source_documents"]:
                print(f"  - {doc.metadata['source']}")

        except Exception as e:
            print(f"å¤„ç†å¯¹è¯æ—¶å‡ºé”™: {e}")

        print()

def save_and_load_vectorstore_example(chunks, embeddings):
    """ä¿å­˜å’ŒåŠ è½½å‘é‡å­˜å‚¨ç¤ºä¾‹"""
    print("=== ä¿å­˜å’ŒåŠ è½½å‘é‡å­˜å‚¨ç¤ºä¾‹ ===\n")

    # åˆ›å»ºå‘é‡å­˜å‚¨
    print("åˆ›å»ºå‘é‡å­˜å‚¨...")
    vectorstore = FAISS.from_documents(chunks, embeddings)

    # ä¿å­˜å‘é‡å­˜å‚¨
    save_path = "./faiss_index"
    print(f"ä¿å­˜å‘é‡å­˜å‚¨åˆ° {save_path}...")
    vectorstore.save_local(save_path)
    print("å‘é‡å­˜å‚¨å·²ä¿å­˜")

    # åŠ è½½å‘é‡å­˜å‚¨
    print(f"ä» {save_path} åŠ è½½å‘é‡å­˜å‚¨...")
    loaded_vectorstore = FAISS.load_local(save_path, embeddings, allow_dangerous_deserialization=True)
    print("å‘é‡å­˜å‚¨å·²åŠ è½½")

    # æµ‹è¯•åŠ è½½çš„å‘é‡å­˜å‚¨
    query = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
    print(f"\næµ‹è¯•æŸ¥è¯¢: {query}")
    docs = loaded_vectorstore.similarity_search(query, k=2)

    for i, doc in enumerate(docs, 1):
        print(f"  ç»“æœ {i}: {doc.page_content[:100]}...")

    print()

if __name__ == "__main__":
    print("ğŸ“š æ¬¢è¿æ¥åˆ°LangChainå‘é‡å­˜å‚¨å’Œæ£€ç´¢å­¦ä¹ ä¸–ç•Œï¼\n")

    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    documents = create_sample_documents()

    # æ–‡æœ¬åˆ†å‰²
    chunks = text_splitting_example(documents)

    # å‘é‡å­˜å‚¨
    faiss_vectorstore, chroma_vectorstore, embeddings = vector_store_example(chunks)

    print("\n" + "="*50 + "\n")

    # ç›¸ä¼¼æ€§æœç´¢
    similarity_search_example(faiss_vectorstore)

    print("\n" + "="*50 + "\n")

    # æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢
    max_marginal_relevance_search_example(faiss_vectorstore)

    print("\n" + "="*50 + "\n")

    # æ£€ç´¢é—®ç­”é“¾
    retrieval_qa_example(faiss_vectorstore)

    print("\n" + "="*50 + "\n")

    # è‡ªå®šä¹‰æ£€ç´¢é“¾
    custom_retrieval_chain_example(faiss_vectorstore, embeddings)

    print("\n" + "="*50 + "\n")

    # å¯¹è¯å¼æ£€ç´¢
    conversational_retrieval_example(faiss_vectorstore)

    print("\n" + "="*50 + "\n")

    # ä¿å­˜å’ŒåŠ è½½å‘é‡å­˜å‚¨
    save_and_load_vectorstore_example(chunks, embeddings)

    print("\nâœ¨ å‘é‡å­˜å‚¨å’Œæ£€ç´¢ç¤ºä¾‹å®Œæˆï¼æ‚¨å·²ç»å­¦ä¼šäº†å¦‚ä½•åœ¨LangChainä¸­ä½¿ç”¨å‘é‡å­˜å‚¨è¿›è¡Œæ–‡æ¡£æ£€ç´¢å’Œé—®ç­”ã€‚")