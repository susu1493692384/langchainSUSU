# RAGFlow + LangChain å®Œæ•´ä»£ç ç¤ºä¾‹æŒ‡å—

## ğŸ“‹ ç›®å½•

1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [RAGFlow è¿æ¥é…ç½®](#ragflow-è¿æ¥é…ç½®)
3. [åŸºæœ¬æ£€ç´¢åŠŸèƒ½](#åŸºæœ¬æ£€ç´¢åŠŸèƒ½)
4. [æç¤ºè¯ä¸ä¸Šä¸‹æ–‡æ„å»º](#æç¤ºè¯ä¸ä¸Šä¸‹æ–‡æ„å»º)
5. [å®Œæ•´åº”ç”¨ç¤ºä¾‹](#å®Œæ•´åº”ç”¨ç¤ºä¾‹)
6. [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)

---

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### 1. å®‰è£…ä¾èµ–åŒ…

```bash
pip install langchain langchain-core langchain-community
pip install langchain-openai
pip install requests python-dotenv pydantic
pip install faiss-cpu chromadb  # å¯é€‰å‘é‡æ•°æ®åº“
```

### 2. ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# RAGFlow é…ç½®
RAGFLOW_API_URL=http://localhost:9380
RAGFLOW_API_KEY=your_ragflow_api_key_here

# LLM é…ç½® (GLM æˆ– OpenAI)
GLM_API_KEY=your_glm_api_key_here
GLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4/
LLM_MODEL=glm-4.5

# æˆ–è€…ä½¿ç”¨ OpenAI
OPENAI_API_KEY=your_openai_api_key_here

# åµŒå…¥æ¨¡å‹é…ç½®
EMBEDDING_MODEL=embedding-2
```

---

## ğŸ”— RAGFlow è¿æ¥é…ç½®

### 1. åŸºæœ¬è¿æ¥å™¨

```python
import os
import requests
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class RAGFlowAPIConnector:
    """RAGFlow APIè¿æ¥å™¨"""

    def __init__(self,
                 base_url: str = None,
                 api_key: str = None,
                 timeout: int = 60):
        """
        åˆå§‹åŒ–RAGFlowè¿æ¥å™¨

        Args:
            base_url: RAGFlowæœåŠ¡åœ°å€ (é»˜è®¤: http://localhost:9380)
            api_key: RAGFlow APIå¯†é’¥
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.base_url = (os.getenv("RAGFLOW_API_URL") if base_url is None else base_url).rstrip('/')
        self.api_key = api_key or os.getenv("RAGFLOW_API_KEY")
        self.timeout = timeout
        self.session = requests.Session()

        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'Content-Type': 'application/json',
            'Authorization': f'Bearer {self.api_key}' if self.api_key else None
        })

    def test_connection(self) -> bool:
        """æµ‹è¯•RAGFlowè¿æ¥"""
        try:
            # å°è¯•å¤šä¸ªå¥åº·æ£€æŸ¥ç«¯ç‚¹
            endpoints = ["/api/health", "/health", "/", "/api/v1/datasets"]

            for endpoint in endpoints:
                try:
                    response = self.session.get(f"{self.base_url}{endpoint}", timeout=10)
                    if response.status_code in [200, 401, 403]:
                        return True
                except:
                    continue

            print("æ‰€æœ‰å¥åº·æ£€æŸ¥ç«¯ç‚¹éƒ½æ— æ³•è®¿é—®")
            return False
        except Exception as e:
            print(f"è¿æ¥RAGFlowå¤±è´¥: {e}")
            return False

    def get_knowledge_bases(self) -> List[Dict]:
        """è·å–æ‰€æœ‰çŸ¥è¯†åº“åˆ—è¡¨"""
        try:
            response = self.session.get(f"{self.base_url}/api/v1/datasets", timeout=self.timeout)
            if response.status_code == 200:
                result = response.json()

                if result.get("code") == 0 and isinstance(result.get("data"), list):
                    return result.get("data", [])
                else:
                    print(f"API é”™è¯¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                    return []
            else:
                print(f"è·å–çŸ¥è¯†åº“å¤±è´¥: HTTP {response.status_code}")
                return []
        except Exception as e:
            print(f"è·å–çŸ¥è¯†åº“å¼‚å¸¸: {e}")
            return []

# ä½¿ç”¨ç¤ºä¾‹
def connection_example():
    """è¿æ¥ç¤ºä¾‹"""
    # åˆ›å»ºè¿æ¥å™¨
    connector = RAGFlowAPIConnector()

    # æµ‹è¯•è¿æ¥
    if connector.test_connection():
        print("âœ… RAGFlowè¿æ¥æˆåŠŸ!")

        # è·å–çŸ¥è¯†åº“åˆ—è¡¨
        knowledge_bases = connector.get_knowledge_bases()
        print(f"ğŸ“š å‘ç° {len(knowledge_bases)} ä¸ªçŸ¥è¯†åº“:")

        for kb in knowledge_bases:
            if isinstance(kb, str):
                print(f"  - {kb}")
            elif isinstance(kb, dict):
                kb_name = kb.get('name', 'æœªçŸ¥')
                kb_desc = kb.get('description', 'æ— æè¿°')
                print(f"  - {kb_name}: {kb_desc}")
    else:
        print("âŒ RAGFlowè¿æ¥å¤±è´¥")

if __name__ == "__main__":
    connection_example()
```

### 2. æ£€ç´¢åŠŸèƒ½å®ç°

```python
def search_knowledge_base(self,
                        kb_name: str,
                        query: str,
                        top_k: int = 5,
                        similarity_threshold: float = 0.7) -> List[Dict]:
    """
    åœ¨æŒ‡å®šçŸ¥è¯†åº“ä¸­æœç´¢æ–‡æ¡£

    Args:
        kb_name: çŸ¥è¯†åº“åç§°æˆ–ID
        query: æŸ¥è¯¢å†…å®¹
        top_k: è¿”å›ç»“æœæ•°é‡
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨
    """
    try:
        # æ„å»ºæœç´¢è¯·æ±‚
        data = {
            "question": query,
            "dataset_ids": [kb_name],  # RAGFlowä½¿ç”¨dataset_idsæ•°ç»„
            "top_k": top_k,
            "similarity_threshold": similarity_threshold
        }

        # å‘é€æœç´¢è¯·æ±‚
        response = self.session.post(
            f"{self.base_url}/api/v1/retrieval",
            json=data,
            timeout=self.timeout
        )

        if response.status_code == 200:
            result = response.json()

            if result.get("code") == 0:
                # æå–æœç´¢ç»“æœ
                data = result.get("data", {})
                chunks = data.get("chunks", [])

                if not chunks and isinstance(data, list):
                    chunks = data

                # æ ¼å¼åŒ–ç»“æœ
                formatted_results = []
                for chunk in chunks:
                    if isinstance(chunk, dict):
                        formatted_results.append({
                            "content": chunk.get("content", chunk.get("text", str(chunk))),
                            "source": chunk.get("document_source", "ragflow"),
                            "score": chunk.get("similarity", chunk.get("score", 0.0)),
                            "doc_id": chunk.get("document_id", chunk.get("id", "")),
                            "title": chunk.get("document_name", chunk.get("title", "")),
                            "raw_data": chunk
                        })

                return formatted_results
            else:
                print(f"æœç´¢ API é”™è¯¯: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                return []
        else:
            print(f"æœç´¢å¤±è´¥: HTTP {response.status_code}")
            return []

    except Exception as e:
        print(f"æœç´¢å¼‚å¸¸: {e}")
        return []

# ä½¿ç”¨ç¤ºä¾‹
def search_example():
    """æœç´¢ç¤ºä¾‹"""
    connector = RAGFlowAPIConnector()

    if connector.test_connection():
        # è·å–çŸ¥è¯†åº“
        knowledge_bases = connector.get_knowledge_bases()

        if knowledge_bases:
            kb_name = knowledge_bases[0].get('id') if isinstance(knowledge_bases[0], dict) else knowledge_bases[0]

            # æ‰§è¡Œæœç´¢
            query = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
            results = connector.search_knowledge_base(
                kb_name=kb_name,
                query=query,
                top_k=5,
                similarity_threshold=0.7
            )

            print(f"\nğŸ” æœç´¢æŸ¥è¯¢: {query}")
            print(f"ğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£:")

            for i, result in enumerate(results, 1):
                print(f"\n{i}. [ç›¸ä¼¼åº¦: {result['score']:.3f}] {result['title']}")
                print(f"   æ¥æº: {result['source']}")
                content_preview = result['content'][:100] + "..." if len(result['content']) > 100 else result['content']
                print(f"   å†…å®¹: {content_preview}")
```

---

## ğŸ” åŸºæœ¬æ£€ç´¢åŠŸèƒ½

### 1. LangChain é›†æˆæ£€ç´¢å™¨

```python
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from pydantic import Field, ConfigDict

class RAGFlowRetriever(BaseRetriever):
    """RAGFlowæ£€ç´¢å™¨ - å°†RAGFlowé›†æˆåˆ°LangChainä¸­"""

    connector: RAGFlowAPIConnector = Field(description="RAGFlowè¿æ¥å™¨")
    kb_name: str = Field(description="çŸ¥è¯†åº“åç§°")
    top_k: int = Field(default=5, description="è¿”å›ç»“æœæ•°é‡")
    similarity_threshold: float = Field(default=0.7, description="ç›¸ä¼¼åº¦é˜ˆå€¼")

    model_config = ConfigDict(arbitrary_types_allowed=True)

    def _get_relevant_documents(self, query: str) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # è°ƒç”¨RAGFlow APIæœç´¢
        search_results = self.connector.search_knowledge_base(
            kb_name=self.kb_name,
            query=query,
            top_k=self.top_k,
            similarity_threshold=self.similarity_threshold
        )

        # è½¬æ¢ä¸ºLangChain Documentæ ¼å¼
        documents = []
        for result in search_results:
            doc = Document(
                page_content=result.get("content", ""),
                metadata={
                    "source": result.get("source", "ragflow"),
                    "score": result.get("score", 0.0),
                    "doc_id": result.get("doc_id", ""),
                    "kb_name": self.kb_name,
                    "title": result.get("title", ""),
                    "url": result.get("url", "")
                }
            )
            documents.append(doc)

        return documents

    def get_relevant_documents(self, query: str) -> List[Document]:
        """å…¬å…±æ–¹æ³•ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        return self._get_relevant_documents(query)

# ä½¿ç”¨ç¤ºä¾‹
def retriever_example():
    """æ£€ç´¢å™¨ä½¿ç”¨ç¤ºä¾‹"""
    # åˆ›å»ºè¿æ¥å™¨å’Œæ£€ç´¢å™¨
    connector = RAGFlowAPIConnector()

    if connector.test_connection():
        # è·å–çŸ¥è¯†åº“
        knowledge_bases = connector.get_knowledge_bases()
        if knowledge_bases:
            kb_name = knowledge_bases[0].get('id') if isinstance(knowledge_bases[0], dict) else knowledge_bases[0]

            # åˆ›å»ºæ£€ç´¢å™¨
            retriever = RAGFlowRetriever(
                connector=connector,
                kb_name=kb_name,
                top_k=5,
                similarity_threshold=0.7
            )

            # æ‰§è¡Œæ£€ç´¢
            query = "æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"
            documents = retriever.get_relevant_documents(query)

            print(f"ğŸ” æŸ¥è¯¢: {query}")
            print(f"ğŸ“‹ æ£€ç´¢åˆ° {len(documents)} ä¸ªæ–‡æ¡£:")

            for i, doc in enumerate(documents, 1):
                metadata = doc.metadata
                print(f"\n{i}. [åˆ†æ•°: {metadata['score']:.3f}] {metadata['title']}")
                print(f"   æ¥æº: {metadata['source']}")
                print(f"   å†…å®¹: {doc.page_content[:100]}...")
```

### 2. å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨

```python
class MultiKBRetriever(BaseRetriever):
    """å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨ - åŒæ—¶æœç´¢å¤šä¸ªçŸ¥è¯†åº“"""

    retrievers: Dict[str, RAGFlowRetriever] = Field(default_factory=dict, description="çŸ¥è¯†åº“æ£€ç´¢å™¨å­—å…¸")

    def __init__(self, connector: RAGFlowAPIConnector, kb_names: List[str]):
        super().__init__(retrievers={})
        for kb_name in kb_names:
            self.retrievers[kb_name] = RAGFlowRetriever(
                connector=connector,
                kb_name=kb_name,
                top_k=3,  # æ¯ä¸ªçŸ¥è¯†åº“è¿”å›3ä¸ªç»“æœ
                similarity_threshold=0.7
            )

    def _get_relevant_documents(self, query: str) -> List[Document]:
        """ä»æ‰€æœ‰çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        all_docs = []
        for kb_name, retriever in self.retrievers.items():
            docs = retriever.get_relevant_documents(query)
            # æ·»åŠ çŸ¥è¯†åº“æ ‡è¯†
            for doc in docs:
                doc.metadata["knowledge_base"] = kb_name
            all_docs.extend(docs)

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        all_docs.sort(key=lambda x: x.metadata.get("score", 0), reverse=True)
        return all_docs[:10]  # è¿”å›å‰10ä¸ªç»“æœ

    def get_relevant_documents(self, query: str) -> List[Document]:
        """å…¬å…±æ–¹æ³•ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        return self._get_relevant_documents(query)

    model_config = ConfigDict(arbitrary_types_allowed=True)

# ä½¿ç”¨ç¤ºä¾‹
def multi_kb_example():
    """å¤šçŸ¥è¯†åº“æ£€ç´¢ç¤ºä¾‹"""
    connector = RAGFlowAPIConnector()

    if connector.test_connection():
        knowledge_bases = connector.get_knowledge_bases()
        if len(knowledge_bases) >= 2:
            # é€‰æ‹©å‰ä¸¤ä¸ªçŸ¥è¯†åº“
            kb_names = []
            for kb in knowledge_bases[:2]:
                kb_name = kb.get('id') if isinstance(kb, dict) else kb
                kb_names.append(kb_name)

            # åˆ›å»ºå¤šçŸ¥è¯†åº“æ£€ç´¢å™¨
            multi_retriever = MultiKBRetriever(connector, kb_names)

            # æ‰§è¡Œæ£€ç´¢
            query = "æ·±åº¦å­¦ä¹ çš„åº”ç”¨é¢†åŸŸ"
            documents = multi_retriever.get_relevant_documents(query)

            print(f"ğŸ” å¤šçŸ¥è¯†åº“æŸ¥è¯¢: {query}")
            print(f"ğŸ“‹ æ£€ç´¢åˆ° {len(documents)} ä¸ªæ–‡æ¡£:")

            for i, doc in enumerate(documents, 1):
                metadata = doc.metadata
                kb_name = metadata.get('knowledge_base', 'æœªçŸ¥')
                print(f"\n{i}. [çŸ¥è¯†åº“: {kb_name}] [åˆ†æ•°: {metadata['score']:.3f}] {metadata['title']}")
                print(f"   æ¥æº: {metadata['source']}")
                print(f"   å†…å®¹: {doc.page_content[:100]}...")
```

---

## ğŸ’¬ æç¤ºè¯ä¸ä¸Šä¸‹æ–‡æ„å»º

### 1. åŸºç¡€æç¤ºè¯æ¨¡æ¿

```python
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI

class RAGFlowQAChain:
    """RAGFlowé—®ç­”é“¾"""

    def __init__(self, retriever: BaseRetriever, llm: any):
        self.retriever = retriever
        self.llm = llm

    def create_basic_chain(self):
        """åˆ›å»ºåŸºç¡€é—®ç­”é“¾"""
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´æ˜ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"""

        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        def format_docs(docs):
            """æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£"""
            return "\n\n".join(doc.page_content for doc in docs)

        # æ„å»ºé“¾
        chain = (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return chain

    def create_contextual_chain(self):
        """åˆ›å»ºä¸Šä¸‹æ–‡å¢å¼ºé—®ç­”é“¾"""
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·åŸºäºçŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚

çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœä¸Šä¸‹æ–‡ä¸­ä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚
å›ç­”æ—¶è¯·ä¿æŒä¸“ä¸šæ€§å’Œå‹å¥½æ€§ï¼š"""

        prompt = ChatPromptTemplate.from_template(template)

        def format_docs_with_scores(docs):
            """æ ¼å¼åŒ–æ–‡æ¡£ï¼ŒåŒ…å«ç›¸ä¼¼åº¦åˆ†æ•°"""
            formatted_docs = []
            for doc in docs:
                score = doc.metadata.get("score", 0.0)
                source = doc.metadata.get("source", "æœªçŸ¥")
                title = doc.metadata.get("title", "")

                doc_content = f"[ç›¸ä¼¼åº¦: {score:.3f}] æ¥æº: {source}"
                if title:
                    doc_content += f" | æ ‡é¢˜: {title}"
                doc_content += f"\n{doc.page_content}"

                formatted_docs.append(doc_content)

            return "\n\n---\n\n".join(formatted_docs)

        chain = (
            {
                "context": self.retriever | format_docs_with_scores,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return chain

    def create_chain_with_sources(self):
        """åˆ›å»ºå¸¦æ¥æºå¼•ç”¨çš„é—®ç­”é“¾"""
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶åœ¨å›ç­”ä¸­å¼•ç”¨ä¿¡æ¯æ¥æºã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶æ ‡æ³¨ä¿¡æ¯æ¥æºï¼š"""

        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        def format_docs_with_sources(docs):
            """æ ¼å¼åŒ–æ–‡æ¡£ï¼ŒåŒ…å«è¯¦ç»†æ¥æºä¿¡æ¯"""
            formatted_docs = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get("source", "æœªçŸ¥")
                title = doc.metadata.get("title", "")
                score = doc.metadata.get("score", 0.0)
                kb_name = doc.metadata.get("kb_name", "")
                doc_id = doc.metadata.get("doc_id", "")

                doc_content = f"æ–‡æ¡£ {i}:"
                doc_content += f"\n- æ ‡é¢˜: {title}"
                doc_content += f"\n- æ¥æº: {source}"
                doc_content += f"\n- çŸ¥è¯†åº“: {kb_name}"
                doc_content += f"\n- ç›¸ä¼¼åº¦: {score:.3f}"
                doc_content += f"\n- æ–‡æ¡£ID: {doc_id}"
                doc_content += f"\n- å†…å®¹: {doc.page_content}"

                formatted_docs.append(doc_content)

            return "\n\n---\n\n".join(formatted_docs)

        chain = (
            {
                "context": self.retriever | format_docs_with_sources,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return chain

# ä½¿ç”¨ç¤ºä¾‹
def qa_chain_example():
    """é—®ç­”é“¾ç¤ºä¾‹"""
    # åˆå§‹åŒ–ç»„ä»¶
    connector = RAGFlowAPIConnector()

    if connector.test_connection():
        # è·å–çŸ¥è¯†åº“
        knowledge_bases = connector.get_knowledge_bases()
        if knowledge_bases:
            kb_name = knowledge_bases[0].get('id') if isinstance(knowledge_bases[0], dict) else knowledge_bases[0]

            # åˆ›å»ºæ£€ç´¢å™¨å’ŒLLM
            retriever = RAGFlowRetriever(connector, kb_name)

            # ä½¿ç”¨GLMæˆ–OpenAI
            if os.getenv("GLM_API_KEY"):
                llm = ChatOpenAI(
                    model=os.getenv("LLM_MODEL", "glm-4"),
                    temperature=0.1,
                    openai_api_key=os.getenv("GLM_API_KEY"),
                    openai_api_base=os.getenv("GLM_BASE_URL", "https://open.bigmodel.cn/api/paas/v4")
                )
            else:
                llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    temperature=0.1,
                    openai_api_key=os.getenv("OPENAI_API_KEY")
                )

            # åˆ›å»ºé—®ç­”é“¾
            qa_chain = RAGFlowQAChain(retriever, llm)

            # æµ‹è¯•ä¸åŒç±»å‹çš„é“¾
            test_question = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"

            print(f"â“ é—®é¢˜: {test_question}")
            print("\n" + "="*50)

            # åŸºç¡€é“¾
            basic_chain = qa_chain.create_basic_chain()
            print("ğŸ”— åŸºç¡€é—®ç­”é“¾:")
            answer = basic_chain.invoke(test_question)
            print(answer)

            print("\n" + "="*50)

            # ä¸Šä¸‹æ–‡å¢å¼ºé“¾
            contextual_chain = qa_chain.create_contextual_chain()
            print("ğŸ”— ä¸Šä¸‹æ–‡å¢å¼ºé—®ç­”é“¾:")
            answer = contextual_chain.invoke(test_question)
            print(answer)

            print("\n" + "="*50)

            # å¸¦æ¥æºé“¾
            sources_chain = qa_chain.create_chain_with_sources()
            print("ğŸ”— å¸¦æ¥æºå¼•ç”¨é—®ç­”é“¾:")
            answer = sources_chain.invoke(test_question)
            print(answer)
```

### 2. é«˜çº§æç¤ºè¯æŠ€å·§

```python
class AdvancedPromptTemplate:
    """é«˜çº§æç¤ºè¯æ¨¡æ¿"""

    @staticmethod
    def create_conditional_template():
        """åˆ›å»ºæ¡ä»¶å“åº”æ¨¡æ¿"""
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

{context}

å›ç­”æŒ‡å—ï¼š
1. å¦‚æœä¸Šä¸‹æ–‡ä¸­åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·åŸºäºä¸Šä¸‹æ–‡è¯¦ç»†å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­ä¿¡æ¯ä¸è¶³ï¼Œè¯·è¯šå®åœ°è¯´æ˜
3. å¦‚æœå®Œå…¨æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç¤¼è²Œåœ°è§£é‡Šæ— æ³•å›ç­”

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›ä¸“ä¸šçš„å›ç­”ï¼š"""

        return PromptTemplate(template=template, input_variables=["context", "question"])

    @staticmethod
    def create_step_by_step_template():
        """åˆ›å»ºåˆ†æ­¥æ¨ç†æ¨¡æ¿"""
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ†æå¸ˆã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å›ç­”é—®é¢˜ï¼š

æ­¥éª¤1: åˆ†æç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒéœ€æ±‚
æ­¥éª¤2: ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æå–ç›¸å…³ä¿¡æ¯
æ­¥éª¤3: ç»¼åˆä¿¡æ¯å¾—å‡ºç»“è®º
æ­¥éª¤4: æä¾›æ¸…æ™°ã€å‡†ç¡®çš„å›ç­”

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æŒ‰æ­¥éª¤è¿›è¡Œåˆ†æå’Œå›ç­”ï¼š"""

        return PromptTemplate(template=template, input_variables=["context", "question"])

    @staticmethod
    def create_role_based_template(role: str):
        """åˆ›å»ºè§’è‰²åŒ–æ¨¡æ¿"""
        role_templates = {
            "ä¸“å®¶": """ä½ æ˜¯ä¸€ä½èµ„æ·±ä¸“å®¶ï¼Œæ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯æä¾›æƒå¨ã€ä¸“ä¸šçš„å›ç­”ã€‚

{context}

ä½œä¸ºé¢†åŸŸä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œä¸“ä¸šåˆ†æï¼š
{question}

è¯·æä¾›ä¸“ä¸šçš„è§è§£å’Œå»ºè®®ï¼š""",

            "æ•™å¸ˆ": """ä½ æ˜¯ä¸€ä½è€å¿ƒçš„æ•™å¸ˆï¼Œæ“…é•¿ç”¨ç®€å•æ˜“æ‡‚çš„æ–¹å¼è§£é‡Šå¤æ‚æ¦‚å¿µã€‚

{context}

è¯·ç”¨æ•™å­¦çš„æ–¹å¼å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
{question}

è¯·æä¾›æ¸…æ™°ã€æ˜“æ‡‚çš„è§£é‡Šï¼š""",

            "åˆ†æå¸ˆ": """ä½ æ˜¯ä¸€ä½æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿ä»ä¿¡æ¯ä¸­æå–å…³é”®æ´å¯Ÿã€‚

{context}

è¯·ä»æ•°æ®åˆ†æçš„è§’åº¦å›ç­”ï¼š
{question}

è¯·æä¾›åŸºäºæ•°æ®çš„åˆ†æï¼š"""
        }

        template = role_templates.get(role, role_templates["ä¸“å®¶"])
        return PromptTemplate(template=template, input_variables=["context", "question"])

# ä½¿ç”¨ç¤ºä¾‹
def advanced_prompt_example():
    """é«˜çº§æç¤ºè¯ç¤ºä¾‹"""
    connector = RAGFlowAPIConnector()

    if connector.test_connection():
        knowledge_bases = connector.get_knowledge_bases()
        if knowledge_bases:
            kb_name = knowledge_bases[0].get('id') if isinstance(knowledge_bases[0], dict) else knowledge_bases[0]

            retriever = RAGFlowRetriever(connector, kb_name)
            llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.1)

            # åˆ›å»ºé«˜çº§æç¤ºè¯é“¾
            def format_docs(docs):
                return "\n\n".join(doc.page_content for doc in docs)

            test_question = "æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"

            # æ¡ä»¶å“åº”æ¨¡æ¿
            conditional_prompt = AdvancedPromptTemplate.create_conditional_template()
            conditional_chain = (
                {
                    "context": retriever | format_docs,
                    "question": RunnablePassthrough()
                }
                | conditional_prompt
                | llm
                | StrOutputParser()
            )

            print(f"â“ é—®é¢˜: {test_question}")
            print("\nğŸ¯ æ¡ä»¶å“åº”æ¨¡æ¿:")
            print(conditional_chain.invoke(test_question))

            # åˆ†æ­¥æ¨ç†æ¨¡æ¿
            step_prompt = AdvancedPromptTemplate.create_step_by_step_template()
            step_chain = (
                {
                    "context": retriever | format_docs,
                    "question": RunnablePassthrough()
                }
                | step_prompt
                | llm
                | StrOutputParser()
            )

            print("\nğŸ” åˆ†æ­¥æ¨ç†æ¨¡æ¿:")
            print(step_chain.invoke(test_question))

            # è§’è‰²åŒ–æ¨¡æ¿
            expert_prompt = AdvancedPromptTemplate.create_role_based_template("ä¸“å®¶")
            expert_chain = (
                {
                    "context": retriever | format_docs,
                    "question": RunnablePassthrough()
                }
                | expert_prompt
                | llm
                | StrOutputParser()
            )

            print("\nğŸ‘¨â€ğŸ« ä¸“å®¶è§’è‰²æ¨¡æ¿:")
            print(expert_chain.invoke(test_question))
```

---

## ğŸš€ å®Œæ•´åº”ç”¨ç¤ºä¾‹

### 1. å®Œæ•´çš„RAGFlowåº”ç”¨ç±»

```python
import os
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class RAGFlowApp:
    """å®Œæ•´çš„RAGFlowåº”ç”¨"""

    def __init__(self, ragflow_url: str = None, ragflow_api_key: str = None):
        """åˆå§‹åŒ–åº”ç”¨"""
        # RAGFlowè¿æ¥å™¨
        self.connector = RAGFlowAPIConnector(ragflow_url, ragflow_api_key)

        # LLMé…ç½®
        if os.getenv("GLM_API_KEY"):
            self.llm = ChatOpenAI(
                model=os.getenv("LLM_MODEL", "glm-4.5"),
                temperature=0.1,
                openai_api_key=os.getenv("GLM_API_KEY"),
                openai_api_base=os.getenv("GLM_BASE_URL", "https://open.bigmodel.cn/api/paas/v4")
            )
        else:
            self.llm = ChatOpenAI(
                model="gpt-3.5-turbo",
                temperature=0.1,
                openai_api_key=os.getenv("OPENAI_API_KEY")
            )

        self.available_kbs = []
        self.retrievers = {}

    def initialize(self) -> bool:
        """åˆå§‹åŒ–åº”ç”¨"""
        print("ğŸš€ åˆå§‹åŒ–RAGFlowåº”ç”¨...")

        # æµ‹è¯•è¿æ¥
        if not self.connector.test_connection():
            print("âŒ RAGFlowè¿æ¥å¤±è´¥")
            return False

        # è·å–çŸ¥è¯†åº“
        self.available_kbs = self.connector.get_knowledge_bases()
        print(f"âœ… è¿æ¥æˆåŠŸï¼Œå‘ç° {len(self.available_kbs)} ä¸ªçŸ¥è¯†åº“")

        return True

    def create_retriever(self, kb_name: str, top_k: int = 5) -> Optional[RAGFlowRetriever]:
        """åˆ›å»ºæ£€ç´¢å™¨"""
        retriever = RAGFlowRetriever(
            connector=self.connector,
            kb_name=kb_name,
            top_k=top_k,
            similarity_threshold=0.7
        )

        self.retrievers[kb_name] = retriever
        return retriever

    def create_qa_chain(self, kb_name: str, chain_type: str = "basic"):
        """åˆ›å»ºé—®ç­”é“¾"""
        if kb_name not in self.retrievers:
            self.create_retriever(kb_name)

        retriever = self.retrievers[kb_name]

        if chain_type == "basic":
            return self._create_basic_chain(retriever)
        elif chain_type == "with_sources":
            return self._create_chain_with_sources(retriever)
        elif chain_type == "contextual":
            return self._create_contextual_chain(retriever)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„é“¾ç±»å‹: {chain_type}")

    def _create_basic_chain(self, retriever):
        """åˆ›å»ºåŸºç¡€é“¾"""
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"""

        prompt = PromptTemplate(template=template, input_variables=["context", "question"])

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return chain

    def _create_chain_with_sources(self, retriever):
        """åˆ›å»ºå¸¦æ¥æºçš„é“¾"""
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå¹¶åœ¨å›ç­”ä¸­å¼•ç”¨ä¿¡æ¯æ¥æºã€‚

{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶æ ‡æ³¨ä¿¡æ¯æ¥æºï¼š"""

        prompt = PromptTemplate(template=template, input_variables=["context", "question"])

        def format_docs_with_sources(docs):
            formatted_docs = []
            for i, doc in enumerate(docs, 1):
                source = doc.metadata.get("source", "æœªçŸ¥")
                title = doc.metadata.get("title", "")
                score = doc.metadata.get("score", 0.0)

                doc_content = f"æ–‡æ¡£ {i} (æ¥æº: {source}, ç›¸ä¼¼åº¦: {score:.3f})\n"
                if title:
                    doc_content += f"æ ‡é¢˜: {title}\n"
                doc_content += f"å†…å®¹: {doc.page_content}"

                formatted_docs.append(doc_content)

            return "\n\n---\n\n".join(formatted_docs)

        chain = (
            {
                "context": retriever | format_docs_with_sources,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return chain

    def chat(self, kb_name: str, question: str, chain_type: str = "basic") -> str:
        """èŠå¤©åŠŸèƒ½"""
        try:
            chain = self.create_qa_chain(kb_name, chain_type)
            return chain.invoke(question)
        except Exception as e:
            return f"èŠå¤©å‡ºé”™: {e}"

# å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
def complete_app_example():
    """å®Œæ•´åº”ç”¨ç¤ºä¾‹"""
    print("ğŸ¯ RAGFlowå®Œæ•´åº”ç”¨ç¤ºä¾‹")
    print("=" * 60)

    # åˆ›å»ºåº”ç”¨å®ä¾‹
    app = RAGFlowApp()

    # åˆå§‹åŒ–
    if not app.initialize():
        return

    # æ˜¾ç¤ºå¯ç”¨çŸ¥è¯†åº“
    print("\nğŸ“š å¯ç”¨çŸ¥è¯†åº“:")
    for i, kb in enumerate(app.available_kbs, 1):
        if isinstance(kb, str):
            print(f"{i}. {kb}")
        elif isinstance(kb, dict):
            kb_name = kb.get('name', 'æœªçŸ¥')
            kb_desc = kb.get('description', 'æ— æè¿°')
            print(f"{i}. {kb_name}: {kb_desc}")

    # é€‰æ‹©ç¬¬ä¸€ä¸ªçŸ¥è¯†åº“è¿›è¡Œæ¼”ç¤º
    if app.available_kbs:
        first_kb = app.available_kbs[0]
        kb_name = first_kb.get('id') if isinstance(first_kb, dict) else first_kb

        print(f"\nğŸ¯ é€‰æ‹©çŸ¥è¯†åº“: {kb_name}")

        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
            "è‡ªç„¶è¯­è¨€å¤„ç†çš„æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]

        for question in test_questions:
            print(f"\nâ“ é—®é¢˜: {question}")
            print("-" * 40)

            try:
                # åŸºç¡€é—®ç­”
                print("ğŸ”— åŸºç¡€é—®ç­”:")
                basic_answer = app.chat(kb_name, question, "basic")
                print(basic_answer)

                print("\nğŸ”— å¸¦æ¥æºé—®ç­”:")
                sources_answer = app.chat(kb_name, question, "with_sources")
                print(sources_answer)

            except Exception as e:
                print(f"âŒ é—®ç­”å‡ºé”™: {e}")

            print("=" * 60)

if __name__ == "__main__":
    complete_app_example()
```

### 2. äº¤äº’å¼å‘½ä»¤è¡Œåº”ç”¨

```python
def interactive_qa_app():
    """äº¤äº’å¼é—®ç­”åº”ç”¨"""
    print("ğŸ¤– RAGFlow äº¤äº’å¼é—®ç­”åº”ç”¨")
    print("=" * 50)

    # åˆå§‹åŒ–åº”ç”¨
    app = RAGFlowApp()
    if not app.initialize():
        print("âŒ åº”ç”¨åˆå§‹åŒ–å¤±è´¥")
        return

    # é€‰æ‹©çŸ¥è¯†åº“
    if not app.available_kbs:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„çŸ¥è¯†åº“")
        return

    print("\nğŸ“š è¯·é€‰æ‹©çŸ¥è¯†åº“:")
    for i, kb in enumerate(app.available_kbs, 1):
        if isinstance(kb, str):
            print(f"{i}. {kb}")
        elif isinstance(kb, dict):
            kb_name = kb.get('name', 'æœªçŸ¥')
            kb_desc = kb.get('description', 'æ— æè¿°')[:50]
            print(f"{i}. {kb_name} - {kb_desc}")

    try:
        choice = int(input("\nè¯·é€‰æ‹©çŸ¥è¯†åº“ (è¾“å…¥æ•°å­—): ")) - 1
        if 0 <= choice < len(app.available_kbs):
            selected_kb = app.available_kbs[choice]
            kb_name = selected_kb.get('id') if isinstance(selected_kb, dict) else selected_kb

            print(f"\nâœ… å·²é€‰æ‹©çŸ¥è¯†åº“: {kb_name}")
            print("ğŸ’¬ å¼€å§‹é—®ç­” (è¾“å…¥ 'quit' é€€å‡º)")
            print("-" * 50)

            while True:
                question = input("\nâ“ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()

                if question.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼")
                    break

                if not question:
                    continue

                print("ğŸ¤” æ­£åœ¨æ€è€ƒ...")

                try:
                    # è·å–å›ç­”
                    answer = app.chat(kb_name, question, "with_sources")
                    print(f"\nğŸ¤– å›ç­”:")
                    print(answer)
                    print("-" * 50)
                except Exception as e:
                    print(f"âŒ å›ç­”å‡ºé”™: {e}")

        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

    except ValueError:
        print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")

if __name__ == "__main__":
    interactive_qa_app()
```

---

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. æ‰¹é‡å¤„ç†åŠŸèƒ½

```python
def batch_search_example():
    """æ‰¹é‡æœç´¢ç¤ºä¾‹"""
    connector = RAGFlowAPIConnector()

    if connector.test_connection():
        knowledge_bases = connector.get_knowledge_bases()
        if knowledge_bases:
            kb_name = knowledge_bases[0].get('id') if isinstance(knowledge_bases[0], dict) else knowledge_bases[0]

            # æ‰¹é‡æŸ¥è¯¢
            questions = [
                "æœºå™¨å­¦ä¹ çš„å®šä¹‰",
                "æ·±åº¦å­¦ä¹ çš„å‘å±•å†å²",
                "ç¥ç»ç½‘ç»œçš„åŸºæœ¬åŸç†",
                "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨"
            ]

            print("ğŸ”„ æ‰¹é‡æœç´¢ä¸­...")

            for i, question in enumerate(questions, 1):
                print(f"\n{i}. é—®é¢˜: {question}")

                results = connector.search_knowledge_base(
                    kb_name=kb_name,
                    query=question,
                    top_k=3,
                    similarity_threshold=0.6
                )

                if results:
                    print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ")
                    for j, result in enumerate(results, 1):
                        print(f"     {j}. [ç›¸ä¼¼åº¦: {result['score']:.3f}] {result['title']}")
                else:
                    print("   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")

if __name__ == "__main__":
    batch_search_example()
```

### 2. æ€§èƒ½ç›‘æ§

```python
import time
from typing import Dict, List

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç±»"""

    def __init__(self):
        self.search_times: List[float] = []
        self.llm_times: List[float] = []
        self.total_times: List[float] = []

    def monitor_search(self, connector, kb_name: str, query: str):
        """ç›‘æ§æœç´¢æ€§èƒ½"""
        start_time = time.time()

        results = connector.search_knowledge_base(
            kb_name=kb_name,
            query=query,
            top_k=5,
            similarity_threshold=0.7
        )

        search_time = time.time() - start_time
        self.search_times.append(search_time)

        return results, search_time

    def monitor_qa_chain(self, chain, question: str):
        """ç›‘æ§é—®ç­”é“¾æ€§èƒ½"""
        start_time = time.time()

        answer = chain.invoke(question)

        total_time = time.time() - start_time
        self.total_times.append(total_time)

        return answer, total_time

    def get_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = {}

        if self.search_times:
            stats['search'] = {
                'count': len(self.search_times),
                'avg_time': sum(self.search_times) / len(self.search_times),
                'min_time': min(self.search_times),
                'max_time': max(self.search_times)
            }

        if self.total_times:
            stats['total'] = {
                'count': len(self.total_times),
                'avg_time': sum(self.total_times) / len(self.total_times),
                'min_time': min(self.total_times),
                'max_time': max(self.total_times)
            }

        return stats

    def print_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡"""
        stats = self.get_stats()

        print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š:")
        print("=" * 40)

        if 'search' in stats:
            search_stats = stats['search']
            print(f"ğŸ” æœç´¢æ€§èƒ½:")
            print(f"   æ¬¡æ•°: {search_stats['count']}")
            print(f"   å¹³å‡æ—¶é—´: {search_stats['avg_time']:.3f}s")
            print(f"   æœ€å¿«æ—¶é—´: {search_stats['min_time']:.3f}s")
            print(f"   æœ€æ…¢æ—¶é—´: {search_stats['max_time']:.3f}s")

        if 'total' in stats:
            total_stats = stats['total']
            print(f"\nğŸ’¬ é—®ç­”æ€§èƒ½:")
            print(f"   æ¬¡æ•°: {total_stats['count']}")
            print(f"   å¹³å‡æ—¶é—´: {total_stats['avg_time']:.3f}s")
            print(f"   æœ€å¿«æ—¶é—´: {total_stats['min_time']:.3f}s")
            print(f"   æœ€æ…¢æ—¶é—´: {total_stats['max_time']:.3f}s")

def performance_test_example():
    """æ€§èƒ½æµ‹è¯•ç¤ºä¾‹"""
    connector = RAGFlowAPIConnector()
    monitor = PerformanceMonitor()

    if connector.test_connection():
        knowledge_bases = connector.get_knowledge_bases()
        if knowledge_bases:
            kb_name = knowledge_bases[0].get('id') if isinstance(knowledge_bases[0], dict) else knowledge_bases[0]

            # åˆ›å»ºåº”ç”¨å’Œé—®ç­”é“¾
            app = RAGFlowApp()
            app.initialize()
            chain = app.create_qa_chain(kb_name, "with_sources")

            # æµ‹è¯•é—®é¢˜
            test_questions = [
                "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
                "æœºå™¨å­¦ä¹ çš„åº”ç”¨",
                "æ·±åº¦å­¦ä¹ çš„å‘å±•",
                "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯",
                "è®¡ç®—æœºè§†è§‰çš„åº”ç”¨é¢†åŸŸ"
            ]

            print("ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•...")

            for question in test_questions:
                print(f"\nğŸ” æµ‹è¯•é—®é¢˜: {question}")

                # ç›‘æ§æœç´¢
                results, search_time = monitor.monitor_search(connector, kb_name, question)
                print(f"   æœç´¢æ—¶é—´: {search_time:.3f}s, ç»“æœæ•°: {len(results)}")

                # ç›‘æ§é—®ç­”
                answer, total_time = monitor.monitor_qa_chain(chain, question)
                print(f"   æ€»æ—¶é—´: {total_time:.3f}s")

            # æ‰“å°æ€§èƒ½ç»Ÿè®¡
            print("\n")
            monitor.print_stats()

if __name__ == "__main__":
    performance_test_example()
```

### 3. ç¼“å­˜æœºåˆ¶

```python
import pickle
import hashlib
from pathlib import Path
from typing import Dict, Any, Optional

class RAGFlowCache:
    """RAGFlowç¼“å­˜ç³»ç»Ÿ"""

    def __init__(self, cache_dir: str = "ragflow_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.search_cache = {}
        self.retriever_cache = {}

    def _get_cache_key(self, kb_name: str, query: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºåŒ…å«æ‰€æœ‰å‚æ•°çš„å­—ç¬¦ä¸²
        params_str = f"{kb_name}:{query}:{sorted(kwargs.items())}"
        # ç”ŸæˆMD5å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
        return hashlib.md5(params_str.encode()).hexdigest()

    def cache_search_result(self, kb_name: str, query: str, results: List[Dict], **kwargs):
        """ç¼“å­˜æœç´¢ç»“æœ"""
        cache_key = self._get_cache_key(kb_name, query, **kwargs)
        cache_data = {
            'results': results,
            'timestamp': time.time(),
            'kb_name': kb_name,
            'query': query,
            'params': kwargs
        }

        # ä¿å­˜åˆ°å†…å­˜
        self.search_cache[cache_key] = cache_data

        # ä¿å­˜åˆ°æ–‡ä»¶
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
        except Exception as e:
            print(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def get_cached_search_result(self, kb_name: str, query: str, **kwargs) -> Optional[List[Dict]]:
        """è·å–ç¼“å­˜çš„æœç´¢ç»“æœ"""
        cache_key = self._get_cache_key(kb_name, query, **kwargs)

        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self.search_cache:
            cached_data = self.search_cache[cache_key]
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ1å°æ—¶ï¼‰
            if time.time() - cached_data['timestamp'] < 3600:
                return cached_data['results']
            else:
                # åˆ é™¤è¿‡æœŸç¼“å­˜
                del self.search_cache[cache_key]

        # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    cached_data = pickle.load(f)

                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if time.time() - cached_data['timestamp'] < 3600:
                    # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                    self.search_cache[cache_key] = cached_data
                    return cached_data['results']
                else:
                    # åˆ é™¤è¿‡æœŸç¼“å­˜æ–‡ä»¶
                    cache_file.unlink()
            except Exception as e:
                print(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")

        return None

    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.search_cache.clear()
        self.retriever_cache.clear()

        # åˆ é™¤ç¼“å­˜æ–‡ä»¶
        for cache_file in self.cache_dir.glob("*.pkl"):
            try:
                cache_file.unlink()
            except Exception as e:
                print(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

# ç¼“å­˜ç‰ˆæœ¬çš„è¿æ¥å™¨
class CachedRAGFlowConnector(RAGFlowAPIConnector):
    """å¸¦ç¼“å­˜çš„RAGFlowè¿æ¥å™¨"""

    def __init__(self, base_url: str = None, api_key: str = None, timeout: int = 60, cache: RAGFlowCache = None):
        super().__init__(base_url, api_key, timeout)
        self.cache = cache or RAGFlowCache()

    def search_knowledge_base(self, kb_name: str, query: str, top_k: int = 5, similarity_threshold: float = 0.7):
        """å¸¦ç¼“å­˜çš„æœç´¢"""
        # å…ˆæ£€æŸ¥ç¼“å­˜
        cached_results = self.cache.get_cached_search_result(
            kb_name=kb_name,
            query=query,
            top_k=top_k,
            similarity_threshold=similarity_threshold
        )

        if cached_results is not None:
            print("ğŸ’¾ ä½¿ç”¨ç¼“å­˜ç»“æœ")
            return cached_results

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œæœç´¢
        print("ğŸ” æ‰§è¡Œæ–°æœç´¢")
        results = super().search_knowledge_base(kb_name, query, top_k, similarity_threshold)

        # ç¼“å­˜ç»“æœ
        self.cache.cache_search_result(
            kb_name=kb_name,
            query=query,
            results=results,
            top_k=top_k,
            similarity_threshold=similarity_threshold
        )

        return results

def cache_example():
    """ç¼“å­˜ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸ“¦ RAGFlowç¼“å­˜ç¤ºä¾‹")
    print("=" * 40)

    # åˆ›å»ºç¼“å­˜å’Œè¿æ¥å™¨
    cache = RAGFlowCache()
    connector = CachedRAGFlowConnector(cache=cache)

    if connector.test_connection():
        knowledge_bases = connector.get_knowledge_bases()
        if knowledge_bases:
            kb_name = knowledge_bases[0].get('id') if isinstance(knowledge_bases[0], dict) else knowledge_bases[0]

            test_queries = [
                "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
                "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",  # ç›¸åŒæŸ¥è¯¢ï¼Œåº”è¯¥ä½¿ç”¨ç¼“å­˜
                "æœºå™¨å­¦ä¹ çš„å‘å±•å†å²"
            ]

            print(f"ğŸ¯ çŸ¥è¯†åº“: {kb_name}")
            print("\nğŸ” æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢:")

            for i, query in enumerate(test_queries, 1):
                print(f"\n{i}. æŸ¥è¯¢: {query}")

                start_time = time.time()
                results = connector.search_knowledge_base(
                    kb_name=kb_name,
                    query=query,
                    top_k=3,
                    similarity_threshold=0.7
                )
                search_time = time.time() - start_time

                print(f"   æœç´¢æ—¶é—´: {search_time:.3f}s")
                print(f"   ç»“æœæ•°: {len(results)}")

                if results:
                    for j, result in enumerate(results[:2], 1):
                        print(f"     {j}. {result['title'][:50]}... [åˆ†æ•°: {result['score']:.3f}]")

if __name__ == "__main__":
    cache_example()
```

---

## ğŸ“š æ€»ç»“

è¿™ä¸ªå®Œæ•´çš„ä»£ç ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ï¼š

1. **è¿æ¥RAGFlowæœåŠ¡**ï¼šé€šè¿‡APIè¿æ¥å™¨å®ç°ç¨³å®šè¿æ¥
2. **å®ç°æ£€ç´¢åŠŸèƒ½**ï¼šå•çŸ¥è¯†åº“å’Œå¤šçŸ¥è¯†åº“æ£€ç´¢
3. **æ„å»ºæç¤ºè¯**ï¼šå¤šç§é«˜çº§æç¤ºè¯æ¨¡æ¿å’ŒæŠ€å·§
4. **åˆ›å»ºå®Œæ•´åº”ç”¨**ï¼šåŒ…æ‹¬äº¤äº’å¼åº”ç”¨å’Œæ€§èƒ½ç›‘æ§
5. **é«˜çº§åŠŸèƒ½**ï¼šç¼“å­˜æœºåˆ¶ã€æ‰¹é‡å¤„ç†ç­‰

### ğŸ”‘ å…³é”®è¦ç‚¹

- **é…ç½®ç®¡ç†**ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†æ•æ„Ÿä¿¡æ¯
- **é”™è¯¯å¤„ç†**ï¼šå®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé™çº§æœºåˆ¶
- **æ€§èƒ½ä¼˜åŒ–**ï¼šç¼“å­˜å’Œæ€§èƒ½ç›‘æ§
- **å¯æ‰©å±•æ€§**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•
- **ç”¨æˆ·ä½“éªŒ**ï¼šä¸°å¯Œçš„åé¦ˆå’Œè¿›åº¦æç¤º

### ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# 1. å®‰è£…ä¾èµ–
pip install langchain langchain-core langchain-community langchain-openai
pip install requests python-dotenv pydantic

# 2. é…ç½®ç¯å¢ƒå˜é‡
# åˆ›å»º .env æ–‡ä»¶å¹¶é…ç½®å¿…è¦çš„APIå¯†é’¥

# 3. è¿è¡Œç¤ºä¾‹
python your_script.py
```

è¿™ä¸ªç¤ºä¾‹æä¾›äº†å®Œæ•´çš„RAGFlow + LangChainé›†æˆè§£å†³æ–¹æ¡ˆï¼Œæ‚¨å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è¿›è¡Œè°ƒæ•´å’Œæ‰©å±•ã€‚