#!/usr/bin/env python3
"""
LangChain ä¸“é¢˜æ•™ç¨‹ - RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å®Œå…¨æŒ‡å—
Retrieval-Augmented Generation å®Œæ•´å¼€å‘æ•™ç¨‹
"""

import os
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import hashlib

import numpy as np
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter, TokenTextSplitter
from langchain_community.vectorstores import FAISS, Chroma
from langchain_core.retrievers import BaseRetriever
# Note: Some retriever components may not be available in current LangChain version
# We'll implement basic functionality without these advanced retrievers
# Note: Some chain modules may not be available in current LangChain version
# from langchain.chains import RetrievalQA
# from langchain.chains.conversational_retrieval import ConversationalRetrievalChain
# Note: Memory module may not be available in current LangChain version
# from langchain_core.memory import ConversationBufferMemory
# Note: Callbacks module may not be available in current LangChain version
# from langchain_community.callbacks import get_openai_callback
# Note: Using OpenAI-compatible embeddings instead of ZhipuAiClient
# from zai import ZhipuAiClient
# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ========================
# RAGæ ¸å¿ƒç»„ä»¶ (RAG Core Components)
# ========================

class RAGSystem:
    """RAGç³»ç»Ÿä¸»ç±»"""

    def __init__(self,
                 llm_model: str = "glm-4.6",
                 embedding_model: str = "embedding-3",
                 temperature: float = 0.1):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ

        Args:
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹
            embedding_model: ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹
            temperature: LLMæ¸©åº¦å‚æ•°
        """
        self.llm = ChatOpenAI(
            model=llm_model,
            temperature=temperature,
            openai_api_key=os.getenv("GLM_API_KEY"),
            openai_api_base=os.getenv("GLM_BASE_URL")
        )
        # ä½¿ç”¨ OpenAI å…¼å®¹çš„ embeddings
        self.embeddings = OpenAIEmbeddings(
            model=embedding_model,
            openai_api_key=os.getenv("GLM_API_KEY"),
            openai_api_base=os.getenv("GLM_BASE_URL")
        )

        self.vector_store = None
        self.retriever = None
        self.qa_chain = None

# ========================
# æ–‡æ¡£å¤„ç†ç»„ä»¶ (Document Processing)
# ========================

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self):
        self.chunk_strategies = {
            'recursive': RecursiveCharacterTextSplitter,
            'token': TokenTextSplitter
        }

    def create_sample_documents(self) -> List[Document]:
        """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£é›†åˆ"""
        docs = [
            Document(
                page_content="""
                äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
                AIåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ç­‰å¤šä¸ªå­é¢†åŸŸã€‚
                ç°ä»£AIæŠ€æœ¯åœ¨åŒ»ç–—è¯Šæ–­ã€è‡ªåŠ¨é©¾é©¶ã€è¯­éŸ³è¯†åˆ«ã€å›¾åƒè¯†åˆ«ç­‰é¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚
                """,
                metadata={"source": "AIåŸºç¡€çŸ¥è¯†", "category": "æŠ€æœ¯", "difficulty": "å…¥é—¨"}
            ),
            Document(
                page_content="""
                æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ è€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚
                ä¸»è¦ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚
                ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œæ— ç›‘ç£å­¦ä¹ å‘ç°æ•°æ®æ¨¡å¼ï¼Œå¼ºåŒ–å­¦ä¹ é€šè¿‡å¥–åŠ±æœºåˆ¶å­¦ä¹ ã€‚
                """,
                metadata={"source": "æœºå™¨å­¦ä¹ ä»‹ç»", "category": "æŠ€æœ¯", "difficulty": "ä¸­çº§"}
            ),
            Document(
                page_content="""
                æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
                å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å›¾åƒè¯†åˆ«ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’ŒTransformeråœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ•ˆæœæ˜¾è‘—ã€‚
                GPTã€BERTç­‰é¢„è®­ç»ƒæ¨¡å‹æ˜¯æ·±åº¦å­¦ä¹ åœ¨NLPé¢†åŸŸçš„é‡å¤§çªç ´ã€‚
                """,
                metadata={"source": "æ·±åº¦å­¦ä¹ åŸç†", "category": "æŠ€æœ¯", "difficulty": "é«˜çº§"}
            ),
            Document(
                page_content="""
                LangChainæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºLLMåº”ç”¨çš„æ¡†æ¶ï¼Œæä¾›äº†æ¨¡å—åŒ–çš„ç»„ä»¶æ¥ç®€åŒ–å¼€å‘è¿‡ç¨‹ã€‚
                æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬Modelsï¼ˆæ¨¡å‹ï¼‰ã€Promptsï¼ˆæç¤ºï¼‰ã€Chainsï¼ˆé“¾ï¼‰ã€Memoryï¼ˆè®°å¿†ï¼‰ã€Retrieversï¼ˆæ£€ç´¢å™¨ï¼‰å’ŒAgentsï¼ˆæ™ºèƒ½ä½“ï¼‰ã€‚
                LangChainæ”¯æŒå¤šç§LLMæä¾›å•†å’Œå‘é‡æ•°æ®åº“ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿå¿«é€Ÿæ„å»ºå¤æ‚çš„AIåº”ç”¨ã€‚
                """,
                metadata={"source": "LangChainæ¡†æ¶", "category": "æ¡†æ¶", "difficulty": "ä¸­çº§"}
            ),
            Document(
                page_content="""
                RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„AIæŠ€æœ¯ã€‚
                å®ƒé¦–å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶åå°†æ£€ç´¢åˆ°çš„å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™LLMç”Ÿæˆå›ç­”ã€‚
                RAGèƒ½å¤Ÿå‡å°‘å¹»è§‰ï¼Œæé«˜å›ç­”çš„å‡†ç¡®æ€§å’Œæ—¶æ•ˆæ€§ï¼Œåœ¨çŸ¥è¯†é—®ç­”ã€æ–‡æ¡£åˆ†æç­‰åœºæ™¯ä¸­è¡¨ç°ä¼˜å¼‚ã€‚
                """,
                metadata={"source": "RAGæŠ€æœ¯", "category": "æŠ€æœ¯", "difficulty": "ä¸­çº§"}
            ),
            Document(
                page_content="""
                å‘é‡æ•°æ®åº“æ˜¯RAGç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œç”¨äºå­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºã€‚
                å¸¸è§çš„å‘é‡æ•°æ®åº“åŒ…æ‹¬FAISSã€Chromaã€Pineconeã€Weaviateç­‰ã€‚
                å‘é‡æ•°æ®åº“ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¦‚ä½™å¼¦ç›¸ä¼¼åº¦ã€æ¬§å‡ é‡Œå¾—è·ç¦»ï¼‰æ¥æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚
                """,
                metadata={"source": "å‘é‡æ•°æ®åº“", "category": "æ•°æ®åº“", "difficulty": "ä¸­çº§"}
            )
        ]
        return docs

    def load_documents_from_files(self, file_paths: List[str]) -> List[Document]:
        """ä»æ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        documents = []

        for file_path in file_paths:
            path = Path(file_path)
            if not path.exists():
                print(f"è­¦å‘Š: æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
                continue

            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()

                doc = Document(
                    page_content=content,
                    metadata={
                        "source": path.name,
                        "file_type": path.suffix,
                        "file_size": len(content),
                        "modified_time": path.stat().st_mtime
                    }
                )
                documents.append(doc)
                print(f"æˆåŠŸåŠ è½½æ–‡æ¡£: {path.name}")

            except Exception as e:
                print(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

        return documents

    def chunk_documents(self,
                        documents: List[Document],
                        strategy: str = 'recursive',
                        chunk_size: int = 1000,
                        chunk_overlap: int = 200) -> List[Document]:
        """åˆ†å‰²æ–‡æ¡£ä¸ºå—"""

        if strategy not in self.chunk_strategies:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†å‰²ç­–ç•¥: {strategy}")

        splitter_class = self.chunk_strategies[strategy]

        if strategy == 'recursive':
            splitter = splitter_class(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
            )
        elif strategy == 'token':
            splitter = splitter_class(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )

        chunks = splitter.split_documents(documents)

        # ä¸ºæ¯ä¸ªå—æ·»åŠ å”¯ä¸€ID
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = f"{chunk.metadata['source']}_chunk_{i}"
            chunk.metadata['chunk_index'] = i
            chunk.metadata['total_chunks'] = len(chunks)

        print(f"ä½¿ç”¨ {strategy} ç­–ç•¥åˆ†å‰²æ–‡æ¡£: {len(documents)} ä¸ªæ–‡æ¡£ â†’ {len(chunks)} ä¸ªå—")
        return chunks

# ========================
# å‘é‡å­˜å‚¨ç®¡ç† (Vector Storage Management)
# ========================

class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self, embeddings):
        self.embeddings = embeddings
        self.stores = {}

    def create_faiss_store(self, documents: List[Document]) -> FAISS:
        """åˆ›å»ºFAISSå‘é‡å­˜å‚¨"""
        print("åˆ›å»ºFAISSå‘é‡å­˜å‚¨...")

        # åˆ›å»ºå‘é‡å­˜å‚¨
        vector_store = FAISS.from_documents(documents, self.embeddings)

        # æ·»åŠ æ–‡æ¡£ç´¢å¼•
        docstore = {f"doc_{i}": doc for i, doc in enumerate(documents)}
        index_to_docstore_id = {i: f"doc_{i}" for i in range(len(documents))}

        vector_store.docstore = docstore
        vector_store.index_to_docstore_id = index_to_docstore_id

        print(f"FAISSå­˜å‚¨åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")
        return vector_store

    def create_chroma_store(self,
                          documents: List[Document],
                          collection_name: str = "rag_collection") -> Chroma:
        """åˆ›å»ºChromaå‘é‡å­˜å‚¨"""
        print(f"åˆ›å»ºChromaå‘é‡å­˜å‚¨ (é›†åˆ: {collection_name})...")

        # åˆ›å»ºæŒä¹…åŒ–ç›®å½•
        persist_directory = f"./chroma_db_{collection_name}"

        vector_store = Chroma.from_documents(
            documents=documents,
            embedding=self.embeddings,
            collection_name=collection_name,
            persist_directory=persist_directory
        )

        print(f"Chromaå­˜å‚¨åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(documents)} ä¸ªæ–‡æ¡£")
        return vector_store

    def load_chroma_store(self,
                         collection_name: str = "rag_collection") -> Optional[Chroma]:
        """åŠ è½½å·²å­˜åœ¨çš„Chromaå­˜å‚¨"""
        persist_directory = f"./chroma_db_{collection_name}"

        if not os.path.exists(persist_directory):
            print(f"Chromaå­˜å‚¨ä¸å­˜åœ¨: {persist_directory}")
            return None

        try:
            vector_store = Chroma(
                persist_directory=persist_directory,
                embedding_function=self.embeddings
            )
            print(f"æˆåŠŸåŠ è½½Chromaå­˜å‚¨: {collection_name}")
            return vector_store
        except Exception as e:
            print(f"åŠ è½½Chromaå­˜å‚¨å¤±è´¥: {e}")
            return None

    def save_faiss_store(self, vector_store: FAISS, file_path: str):
        """ä¿å­˜FAISSå­˜å‚¨åˆ°æ–‡ä»¶"""
        try:
            vector_store.save_local(file_path)
            print(f"FAISSå­˜å‚¨å·²ä¿å­˜åˆ°: {file_path}")
        except Exception as e:
            print(f"ä¿å­˜FAISSå­˜å‚¨å¤±è´¥: {e}")

    def load_faiss_store(self, file_path: str, embeddings) -> Optional[FAISS]:
        """ä»æ–‡ä»¶åŠ è½½FAISSå­˜å‚¨"""
        try:
            vector_store = FAISS.load_local(file_path, embeddings, allow_dangerous_deserialization=True)
            print(f"æˆåŠŸåŠ è½½FAISSå­˜å‚¨: {file_path}")
            return vector_store
        except Exception as e:
            print(f"åŠ è½½FAISSå­˜å‚¨å¤±è´¥: {e}")
            return None

# ========================
# é«˜çº§æ£€ç´¢å™¨ (Advanced Retrievers)
# ========================

class AdvancedRetrievers:
    """é«˜çº§æ£€ç´¢å™¨é›†åˆ - ç®€åŒ–ç‰ˆæœ¬"""

    @staticmethod
    def create_basic_retriever(vector_store, search_kwargs={"k": 3}):
        """åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨"""
        print("åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨...")
        return vector_store.as_retriever(
            search_type="similarity",
            search_kwargs=search_kwargs
        )

    # æ³¨æ„ï¼šä»¥ä¸‹æ£€ç´¢å™¨åœ¨å½“å‰LangChainç‰ˆæœ¬ä¸­å¯èƒ½ä¸å¯ç”¨
    # æˆ‘ä»¬ä½¿ç”¨åŸºç¡€æ£€ç´¢å™¨æ¥æ›¿ä»£

    @staticmethod
    def create_multi_query_retriever_alternative(llm, base_retriever):
        """å¤šæŸ¥è¯¢æ£€ç´¢å™¨çš„æ›¿ä»£å®ç°"""
        print("åˆ›å»ºå¤šæŸ¥è¯¢æ£€ç´¢å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰...")
        # æš‚æ—¶è¿”å›åŸºç¡€æ£€ç´¢å™¨
        return base_retriever

    @staticmethod
    def create_contextual_compression_retriever_alternative(llm, base_retriever):
        """ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨çš„æ›¿ä»£å®ç°"""
        print("åˆ›å»ºä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰...")
        # æš‚æ—¶è¿”å›åŸºç¡€æ£€ç´¢å™¨
        return base_retriever

    @staticmethod
    def create_parent_document_retriever_alternative(child_splitter,
                                                  parent_splitter,
                                                  vector_store):
        """çˆ¶å­æ–‡æ¡£æ£€ç´¢å™¨çš„æ›¿ä»£å®ç°"""
        print("åˆ›å»ºçˆ¶å­æ–‡æ¡£æ£€ç´¢å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰...")
        # æš‚æ—¶è¿”å›åŸºç¡€æ£€ç´¢å™¨
        return vector_store.as_retriever(search_kwargs={"k": 3})

# ========================
# RAGé“¾æ„å»º (RAG Chain Construction)
# ========================

class RAGChainBuilder:
    """RAGé“¾æ„å»ºå™¨"""

    def __init__(self, llm, retriever):
        self.llm = llm
        self.retriever = retriever

    def create_basic_rag_chain(self):
        """åˆ›å»ºåŸºç¡€RAGé“¾"""
        print("åˆ›å»ºåŸºç¡€RAGé“¾...")

        # RAGæç¤ºæ¨¡æ¿
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®åœ°è¯´æ˜ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯ã€‚

        ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜ï¼š{question}

        è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼š"""

        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        # åˆ›å»ºé“¾
        chain = (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return chain

    def create_conversational_rag_chain(self):
        """åˆ›å»ºå¯¹è¯å¼RAGé“¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        print("åˆ›å»ºå¯¹è¯å¼RAGé“¾ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰...")

        # å¯¹è¯æç¤ºæ¨¡æ¿
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

        ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜ï¼š{question}

        è¯·æä¾›è‡ªç„¶ã€è¿è´¯çš„å›ç­”ï¼š"""

        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        # æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        # åˆ›å»ºç®€åŒ–çš„å¯¹è¯é“¾ï¼ˆä½¿ç”¨åŸºç¡€RAGé“¾ç»“æ„ï¼‰
        chain = (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return chain

    def create_rag_with_source_chain(self):
        """åˆ›å»ºå¸¦æ¥æºå¼•ç”¨çš„RAGé“¾"""
        print("åˆ›å»ºå¸¦æ¥æºå¼•ç”¨çš„RAGé“¾...")

        # æ ¼å¼åŒ–æ–‡æ¡£çš„å‡½æ•°
        def format_docs(docs):
            return "\n\n".join([
                f"æ–‡æ¡£æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}\n"
                f"å†…å®¹: {doc.page_content}"
                for doc in docs
            ])

        # RAGæç¤ºæ¨¡æ¿
        template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        åœ¨å›ç­”ä¸­è¯·å¼•ç”¨ä¿¡æ¯çš„æ¥æºã€‚

        ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        {context}

        ç”¨æˆ·é—®é¢˜ï¼š{question}

        è¯·æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ï¼Œå¹¶æ ‡æ³¨ä¿¡æ¯æ¥æºï¼š"""

        prompt = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        # åˆ›å»ºé“¾
        chain = (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

        return chain

# ========================
# RAGç³»ç»Ÿè¯„ä¼° (RAG System Evaluation)
# ========================

class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°å™¨"""

    def __init__(self, llm):
        self.llm = llm

    def evaluate_retrieval_quality(self,
                                 query: str,
                                 retrieved_docs: List[Document]) -> Dict[str, Any]:
        """è¯„ä¼°æ£€ç´¢è´¨é‡"""
        print(f"è¯„ä¼°æŸ¥è¯¢ '{query}' çš„æ£€ç´¢è´¨é‡...")

        # è¯„ä¼°æŒ‡æ ‡
        metrics = {
            "query": query,
            "retrieved_count": len(retrieved_docs),
            "avg_doc_length": np.mean([len(doc.page_content) for doc in retrieved_docs]) if retrieved_docs else 0,
            "sources": list(set([doc.metadata.get('source', 'æœªçŸ¥') for doc in retrieved_docs])),
            "categories": list(set([doc.metadata.get('category', 'æœªçŸ¥') for doc in retrieved_docs]))
        }

        # ç›¸å…³æ€§è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        query_words = set(query.lower().split())
        relevance_scores = []

        for doc in retrieved_docs:
            doc_words = set(doc.page_content.lower().split())
            overlap = len(query_words & doc_words)
            similarity = overlap / len(query_words) if query_words else 0
            relevance_scores.append(similarity)

        metrics["avg_relevance_score"] = np.mean(relevance_scores) if relevance_scores else 0

        return metrics

    def evaluate_response_quality(self,
                                 question: str,
                                 context: str,
                                 response: str) -> Dict[str, Any]:
        """è¯„ä¼°å›ç­”è´¨é‡"""
        print("è¯„ä¼°å›ç­”è´¨é‡...")

        # è¯„ä¼°æç¤º
        evaluation_prompt = PromptTemplate(
            input_variables=["question", "context", "response"],
            template="""è¯·è¯„ä¼°ä»¥ä¸‹AIå›ç­”çš„è´¨é‡ï¼Œä»å¤šä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š

        é—®é¢˜: {question}

        ä¸Šä¸‹æ–‡ä¿¡æ¯: {context}

        AIå›ç­”: {response}

        è¯·ä»ä»¥ä¸‹ç»´åº¦è¯„åˆ†ï¼š
        1. å‡†ç¡®æ€§ - å›ç­”æ˜¯å¦åŸºäºä¸Šä¸‹æ–‡ä¿¡æ¯ä¸”å‡†ç¡®æ— è¯¯
        2. å®Œæ•´æ€§ - æ˜¯å¦å……åˆ†å›ç­”äº†é—®é¢˜
        3. æ¸…æ™°æ€§ - è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
        4. ç›¸å…³æ€§ - æ˜¯å¦ç›´æ¥å›åº”äº†ç”¨æˆ·é—®é¢˜
        5. æœ‰ç”¨æ€§ - å¯¹ç”¨æˆ·æ˜¯å¦æœ‰å®é™…å¸®åŠ©

        è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„åˆ†ç»“æœï¼š"""
        )

        try:
            result = self.llm.invoke(evaluation_prompt.format(
                question=question,
                context=context[:1000] + "..." if len(context) > 1000 else context,
                response=response
            ))

            # è¿™é‡Œåº”è¯¥è§£æJSONï¼Œç®€åŒ–å¤„ç†ç›´æ¥è¿”å›
            return {
                "evaluation": result.content,
                "response_length": len(response),
                "uses_context": any(word in response.lower() for word in context.lower().split()[:10])
            }

        except Exception as e:
            return {
                "error": str(e),
                "evaluation": "è¯„ä¼°å¤±è´¥"
            }

    def benchmark_retrieval(self,
                          queries: List[str],
                          retriever) -> Dict[str, Any]:
        """æ£€ç´¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"å¼€å§‹æ£€ç´¢åŸºå‡†æµ‹è¯•ï¼Œå…± {len(queries)} ä¸ªæŸ¥è¯¢...")

        results = []
        total_start_time = time.time()

        for i, query in enumerate(queries):
            print(f"æµ‹è¯•æŸ¥è¯¢ {i+1}/{len(queries)}: {query}")

            start_time = time.time()
            try:
                docs = retriever.get_relevant_documents(query)
                end_time = time.time()

                metrics = self.evaluate_retrieval_quality(query, docs)
                metrics["retrieval_time"] = end_time - start_time
                metrics["success"] = True

                results.append(metrics)

            except Exception as e:
                results.append({
                    "query": query,
                    "error": str(e),
                    "success": False,
                    "retrieval_time": time.time() - start_time
                })

        total_time = time.time() - total_start_time

        # æ±‡æ€»ç»Ÿè®¡
        successful_results = [r for r in results if r.get("success", False)]

        summary = {
            "total_queries": len(queries),
            "successful_queries": len(successful_results),
            "success_rate": len(successful_results) / len(queries) * 100,
            "total_time": total_time,
            "avg_retrieval_time": np.mean([r.get("retrieval_time", 0) for r in successful_results]) if successful_results else 0,
            "avg_retrieved_docs": np.mean([r.get("retrieved_count", 0) for r in successful_results]) if successful_results else 0,
            "avg_relevance_score": np.mean([r.get("avg_relevance_score", 0) for r in successful_results]) if successful_results else 0,
            "detailed_results": results
        }

        return summary

# ========================
# æ¼”ç¤ºå‡½æ•° (Demo Functions)
# ========================

def basic_rag_demo():
    """åŸºç¡€RAGæ¼”ç¤º"""
    print("=" * 60)
    print("ğŸš€ åŸºç¡€RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)

    # åˆå§‹åŒ–ç»„ä»¶
    rag_system = RAGSystem()
    processor = DocumentProcessor()
    vector_manager = VectorStoreManager(rag_system.embeddings)

    # 1. åŠ è½½å’Œå¤„ç†æ–‡æ¡£
    print("\nğŸ“š æ­¥éª¤1: åŠ è½½å’Œå¤„ç†æ–‡æ¡£")
    documents = processor.create_sample_documents()
    chunks = processor.chunk_documents(documents, strategy='recursive')

    # 2. åˆ›å»ºå‘é‡å­˜å‚¨
    print("\nğŸ’¾ æ­¥éª¤2: åˆ›å»ºå‘é‡å­˜å‚¨")
    vector_store = vector_manager.create_faiss_store(chunks)

    # 3. åˆ›å»ºæ£€ç´¢å™¨
    print("\nğŸ” æ­¥éª¤3: åˆ›å»ºæ£€ç´¢å™¨")
    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )

    # 4. æ„å»ºRAGé“¾
    print("\nâ›“ï¸ æ­¥éª¤4: æ„å»ºRAGé“¾")
    chain_builder = RAGChainBuilder(rag_system.llm, retriever)
    rag_chain = chain_builder.create_basic_rag_chain()

    # 5. æµ‹è¯•é—®ç­”
    print("\nğŸ’¬ æ­¥éª¤5: æµ‹è¯•é—®ç­”")
    test_questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
        "LangChainæ˜¯ä»€ä¹ˆï¼Ÿ",
        "RAGæŠ€æœ¯çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]

    for question in test_questions:
        print(f"\nâ“ é—®é¢˜: {question}")
        try:
            start_time = time.time()
            answer = rag_chain.invoke(question)
            end_time = time.time()

            print(f"ğŸ¤– å›ç­”: {answer}")
            print(f"â±ï¸ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")

        except Exception as e:
            print(f"âŒ å¤„ç†é—®é¢˜å¤±è´¥: {e}")

        print("-" * 40)

def advanced_rag_demo():
    """é«˜çº§RAGæ¼”ç¤º"""
    print("=" * 60)
    print("ğŸš€ é«˜çº§RAGç³»ç»Ÿæ¼”ç¤ºï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰")
    print("=" * 60)

    # åˆå§‹åŒ–ç»„ä»¶
    rag_system = RAGSystem()
    processor = DocumentProcessor()
    vector_manager = VectorStoreManager(rag_system.embeddings)
    advanced_retrievers = AdvancedRetrievers()

    # 1. å‡†å¤‡æ–‡æ¡£å’Œå‘é‡å­˜å‚¨
    print("\nğŸ“š å‡†å¤‡æ–‡æ¡£å’Œå‘é‡å­˜å‚¨")
    documents = processor.create_sample_documents()
    chunks = processor.chunk_documents(documents, strategy='recursive')
    vector_store = vector_manager.create_faiss_store(chunks)

    # 2. åˆ›å»ºåŸºç¡€æ£€ç´¢å™¨
    base_retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    # 3. åˆ›å»ºé«˜çº§æ£€ç´¢å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    print("\nğŸ” åˆ›å»ºé«˜çº§æ£€ç´¢å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰")

    # å¤šæŸ¥è¯¢æ£€ç´¢å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    multi_query_retriever = advanced_retrievers.create_multi_query_retriever_alternative(
        rag_system.llm, base_retriever
    )

    # ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    compression_retriever = advanced_retrievers.create_contextual_compression_retriever_alternative(
        rag_system.llm, base_retriever
    )

    # 4. åˆ›å»ºä¸åŒçš„RAGé“¾
    print("\nâ›“ï¸ åˆ›å»ºä¸åŒçš„RAGé“¾")

    chains = {
        "åŸºç¡€RAG": RAGChainBuilder(rag_system.llm, base_retriever).create_basic_rag_chain(),
        "å¤šæŸ¥è¯¢RAGï¼ˆç®€åŒ–ç‰ˆï¼‰": RAGChainBuilder(rag_system.llm, multi_query_retriever).create_basic_rag_chain(),
        "å‹ç¼©RAGï¼ˆç®€åŒ–ç‰ˆï¼‰": RAGChainBuilder(rag_system.llm, compression_retriever).create_basic_rag_chain(),
        "å¸¦æ¥æºRAG": RAGChainBuilder(rag_system.llm, base_retriever).create_rag_with_source_chain()
    }

    # 5. æ¯”è¾ƒä¸åŒæ£€ç´¢å™¨çš„æ•ˆæœ
    print("\nğŸ“Š æ¯”è¾ƒä¸åŒRAGæ–¹æ³•çš„æ•ˆæœ")
    test_question = "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ"

    for method_name, chain in chains.items():
        print(f"\n--- {method_name} ---")
        try:
            start_time = time.time()
            answer = chain.invoke(test_question)
            end_time = time.time()

            print(f"å›ç­”: {answer[:200]}...")
            print(f"å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")

        except Exception as e:
            print(f"å¤„ç†å¤±è´¥: {e}")

def conversational_rag_demo():
    """å¯¹è¯å¼RAGæ¼”ç¤º"""
    print("=" * 60)
    print("ğŸš€ å¯¹è¯å¼RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)

    # åˆå§‹åŒ–ç»„ä»¶
    rag_system = RAGSystem()
    processor = DocumentProcessor()
    vector_manager = VectorStoreManager(rag_system.embeddings)

    # 1. å‡†å¤‡æ–‡æ¡£å’Œå‘é‡å­˜å‚¨
    documents = processor.create_sample_documents()
    chunks = processor.chunk_documents(documents)
    vector_store = vector_manager.create_faiss_store(chunks)

    # 2. åˆ›å»ºæ£€ç´¢å™¨å’Œå¯¹è¯é“¾
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})
    chain_builder = RAGChainBuilder(rag_system.llm, retriever)
    conversational_chain = chain_builder.create_conversational_rag_chain()

    # 3. æ¨¡æ‹Ÿå¯¹è¯
    print("\nğŸ’¬ å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")

    while True:
        question = input("\nâ“ æ‚¨çš„é—®é¢˜: ").strip()
        if question.lower() in ['quit', 'exit', 'é€€å‡º']:
            break

        if not question:
            continue

        try:
            answer = conversational_chain.invoke(question)
            print(f"\nğŸ¤– åŠ©æ‰‹: {answer}")

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")

def rag_evaluation_demo():
    """RAGç³»ç»Ÿè¯„ä¼°æ¼”ç¤º"""
    print("=" * 60)
    print("ğŸš€ RAGç³»ç»Ÿè¯„ä¼°æ¼”ç¤º")
    print("=" * 60)

    # åˆå§‹åŒ–ç»„ä»¶
    rag_system = RAGSystem()
    processor = DocumentProcessor()
    vector_manager = VectorStoreManager(rag_system.embeddings)
    evaluator = RAGEvaluator(rag_system.llm)

    # 1. å‡†å¤‡RAGç³»ç»Ÿ
    documents = processor.create_sample_documents()
    chunks = processor.chunk_documents(documents)
    vector_store = vector_manager.create_faiss_store(chunks)
    retriever = vector_store.as_retriever(search_kwargs={"k": 3})

    # 2. åˆ›å»ºRAGé“¾
    chain_builder = RAGChainBuilder(rag_system.llm, retriever)
    rag_chain = chain_builder.create_basic_rag_chain()

    # 3. æ£€ç´¢è´¨é‡è¯„ä¼°
    print("\nğŸ“Š æ£€ç´¢è´¨é‡è¯„ä¼°")
    test_queries = [
        "ä»€ä¹ˆæ˜¯AIï¼Ÿ",
        "æ·±åº¦å­¦ä¹ çš„åº”ç”¨",
        "LangChainçš„ç‰¹ç‚¹"
    ]

    for query in test_queries:
        print(f"\nğŸ” è¯„ä¼°æŸ¥è¯¢: {query}")
        docs = retriever.get_relevant_documents(query)
        metrics = evaluator.evaluate_retrieval_quality(query, docs)

        print(f"æ£€ç´¢åˆ° {metrics['retrieved_count']} ä¸ªæ–‡æ¡£")
        print(f"å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {metrics['avg_relevance_score']:.3f}")
        print(f"æ–‡æ¡£æ¥æº: {', '.join(metrics['sources'])}")

    # 4. åŸºå‡†æµ‹è¯•
    print("\nğŸƒ åŸºå‡†æ€§èƒ½æµ‹è¯•")
    benchmark_results = evaluator.benchmark_retrieval(test_queries, retriever)

    print(f"æˆåŠŸç‡: {benchmark_results['success_rate']:.1f}%")
    print(f"å¹³å‡æ£€ç´¢æ—¶é—´: {benchmark_results['avg_retrieval_time']:.3f}ç§’")
    print(f"å¹³å‡æ£€ç´¢æ–‡æ¡£æ•°: {benchmark_results['avg_retrieved_docs']:.1f}")
    print(f"å¹³å‡ç›¸å…³æ€§åˆ†æ•°: {benchmark_results['avg_relevance_score']:.3f}")

    # 5. å›ç­”è´¨é‡è¯„ä¼°
    print("\nğŸ“ å›ç­”è´¨é‡è¯„ä¼°")
    test_question = "ä»€ä¹ˆæ˜¯RAGæŠ€æœ¯ï¼Ÿ"

    # è·å–ä¸Šä¸‹æ–‡å’Œå›ç­”
    docs = retriever.get_relevant_documents(test_question)
    context = "\n".join([doc.page_content for doc in docs])
    response = rag_chain.invoke(test_question)

    # è¯„ä¼°å›ç­”è´¨é‡
    quality_metrics = evaluator.evaluate_response_quality(
        test_question, context, response
    )

    print(f"å›ç­”é•¿åº¦: {quality_metrics.get('response_length', 0)} å­—ç¬¦")
    print(f"ä½¿ç”¨ä¸Šä¸‹æ–‡: {'æ˜¯' if quality_metrics.get('uses_context', False) else 'å¦'}")
    print(f"è¯„ä¼°ç»“æœ: {quality_metrics.get('evaluation', 'è¯„ä¼°å¤±è´¥')}")

def file_based_rag_demo():
    """åŸºäºæ–‡ä»¶çš„RAGæ¼”ç¤º"""
    print("=" * 60)
    print("ğŸš€ åŸºäºæ–‡ä»¶çš„RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)

    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    sample_files = {
        "AI_tutorial.txt": """
äººå·¥æ™ºèƒ½æ•™ç¨‹
============

1. äººå·¥æ™ºèƒ½æ¦‚è¿°
äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚
AIç³»ç»Ÿå¯ä»¥å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥å’Œç†è§£è‡ªç„¶è¯­è¨€ã€‚

2. æœºå™¨å­¦ä¹ åŸºç¡€
æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚
ä¸»è¦ç±»å‹ï¼š
- ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒ
- æ— ç›‘ç£å­¦ä¹ ï¼šå‘ç°æ•°æ®æ¨¡å¼
- å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡å¥–åŠ±å­¦ä¹ 

3. æ·±åº¦å­¦ä¹ è¿›é˜¶
æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚é—®é¢˜ã€‚
åº”ç”¨é¢†åŸŸåŒ…æ‹¬ï¼š
- å›¾åƒè¯†åˆ«
- è‡ªç„¶è¯­è¨€å¤„ç†
- è¯­éŸ³è¯†åˆ«

4. å®è·µåº”ç”¨
AIæŠ€æœ¯åœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼š
- åŒ»ç–—è¯Šæ–­
- è‡ªåŠ¨é©¾é©¶
- æ™ºèƒ½å®¢æœ
- æ¨èç³»ç»Ÿ
        """,

        "LangChain_guide.txt": """
LangChainå¼€å‘æŒ‡å—
=================

1. æ¡†æ¶ä»‹ç»
LangChainæ˜¯æ„å»ºLLMåº”ç”¨çš„å¼€æºæ¡†æ¶ï¼Œæä¾›æ¨¡å—åŒ–ç»„ä»¶ã€‚

2. æ ¸å¿ƒç»„ä»¶
- Models: è¯­è¨€æ¨¡å‹æ¥å£
- Prompts: æç¤ºå·¥ç¨‹
- Chains: é“¾å¼è°ƒç”¨
- Memory: å¯¹è¯è®°å¿†
- Retrievers: æ–‡æ¡£æ£€ç´¢
- Agents: æ™ºèƒ½ä»£ç†

3. å¿«é€Ÿå¼€å§‹
å®‰è£…: pip install langchain
åŸºç¡€ä½¿ç”¨: from langchain import OpenAI, LLMChain

4. é«˜çº§åŠŸèƒ½
- è‡ªå®šä¹‰ç»„ä»¶
- å·¥å…·é›†æˆ
- å¤šæ¨¡å‹æ”¯æŒ
        """
    }

    # ä¿å­˜ç¤ºä¾‹æ–‡ä»¶
    for filename, content in sample_files.items():
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {filename}")

    # åˆå§‹åŒ–ç»„ä»¶
    rag_system = RAGSystem()
    processor = DocumentProcessor()
    vector_manager = VectorStoreManager(rag_system.embeddings)

    # 1. ä»æ–‡ä»¶åŠ è½½æ–‡æ¡£
    print("\nğŸ“š ä»æ–‡ä»¶åŠ è½½æ–‡æ¡£")
    file_paths = list(sample_files.keys())
    documents = processor.load_documents_from_files(file_paths)

    # 2. åˆ†å‰²æ–‡æ¡£
    chunks = processor.chunk_documents(documents, strategy='recursive')

    # 3. åˆ›å»ºå‘é‡å­˜å‚¨å¹¶ä¿å­˜
    vector_store = vector_manager.create_faiss_store(chunks)
    vector_manager.save_faiss_store(vector_store, "./file_rag_index")

    # 4. åˆ›å»ºRAGé“¾
    retriever = vector_store.as_retriever(search_kwargs={"k": 2})
    chain_builder = RAGChainBuilder(rag_system.llm, retriever)
    rag_chain = chain_builder.create_rag_with_source_chain()

    # 5. æµ‹è¯•é—®ç­”
    print("\nğŸ’¬ æµ‹è¯•åŸºäºæ–‡ä»¶çš„é—®ç­”")
    test_questions = [
        "å¦‚ä½•å¼€å§‹å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ",
        "LangChainæœ‰å“ªäº›æ ¸å¿ƒç»„ä»¶ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ"
    ]

    for question in test_questions:
        print(f"\nâ“ {question}")
        try:
            answer = rag_chain.invoke(question)
            print(f"ğŸ¤– {answer}")
        except Exception as e:
            print(f"âŒ å›ç­”å¤±è´¥: {e}")
        print("-" * 40)

    # æ¸…ç†æ–‡ä»¶
    for filename in file_paths:
        try:
            os.remove(filename)
            print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {filename}")
        except:
            pass

# ========================
# ä¸»å‡½æ•° (Main Function)
# ========================

def main():
    """ä¸»å‡½æ•°"""
    print("LangChain RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å®Œå…¨æŒ‡å—")
    print("=" * 60)
    print("æœ¬æ•™ç¨‹å°†å¸¦æ‚¨æ·±å…¥æŒæ¡RAGæŠ€æœ¯çš„å„ä¸ªæ–¹é¢")

    demo_options = {
        "1": ("åŸºç¡€RAGç³»ç»Ÿ", basic_rag_demo),
        "2": ("é«˜çº§RAGæŠ€æœ¯", advanced_rag_demo),
        "3": ("å¯¹è¯å¼RAG", conversational_rag_demo),
        "4": ("RAGç³»ç»Ÿè¯„ä¼°", rag_evaluation_demo),
        "5": ("åŸºäºæ–‡ä»¶çš„RAG", file_based_rag_demo),
        "6": ("è¿è¡Œæ‰€æœ‰æ¼”ç¤º", run_all_demos)
    }

    while True:
        print("\n" + "=" * 60)
        print("é€‰æ‹©è¦æ¼”ç¤ºçš„RAGæŠ€æœ¯ï¼š")
        print("=" * 60)

        for key, (name, _) in demo_options.items():
            print(f"{key}. {name}")
        print("0. é€€å‡º")

        choice = input("\nè¯·é€‰æ‹© (0-6): ").strip()

        if choice == "0":
            print("\næ„Ÿè°¢ä½¿ç”¨RAGå®Œå…¨æŒ‡å—ï¼")
            break
        elif choice in demo_options:
            name, demo_func = demo_options[choice]
            print(f"\nå¼€å§‹æ¼”ç¤º: {name}")
            try:
                demo_func()
            except Exception as e:
                print(f"æ¼”ç¤ºå‡ºé”™: {e}")
                print("è¯·æ£€æŸ¥ç¯å¢ƒé…ç½®å’Œä¾èµ–å®‰è£…")
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥0-6ä¹‹é—´çš„æ•°å­—")

def run_all_demos():
    """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
    print("\nğŸš€ è¿è¡Œæ‰€æœ‰RAGæ¼”ç¤º...")

    demos = [
        ("åŸºç¡€RAGç³»ç»Ÿ", basic_rag_demo),
        ("é«˜çº§RAGæŠ€æœ¯", advanced_rag_demo),
        ("RAGç³»ç»Ÿè¯„ä¼°", rag_evaluation_demo),
        ("åŸºäºæ–‡ä»¶çš„RAG", file_based_rag_demo)
    ]

    for name, demo_func in demos:
        print(f"\n{'='*80}")
        print(f"ğŸš€ æ¼”ç¤º: {name}")
        print(f"{'='*80}")
        try:
            demo_func()
        except Exception as e:
            print(f"âŒ æ¼”ç¤º '{name}' å‡ºé”™: {e}")

        input("\næŒ‰å›è½¦é”®ç»§ç»­ä¸‹ä¸€ä¸ªæ¼”ç¤º...")

if __name__ == "__main__":
    main()