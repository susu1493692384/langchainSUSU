#!/usr/bin/env python3
"""
LangChain Embeddings å®Œå…¨ä½¿ç”¨æŒ‡å—
"""

import os
import numpy as np
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS, Chroma
from sklearn.metrics.pairwise import cosine_similarity

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ========================
# 1. åŸºæœ¬Embeddingsä½¿ç”¨
# ========================

def basic_embeddings_demo():
    """åŸºæœ¬Embeddingsä½¿ç”¨æ¼”ç¤º"""
    print("=" * 60)
    print("ğŸ” åŸºæœ¬Embeddingsä½¿ç”¨æ¼”ç¤º")
    print("=" * 60)

    # åˆå§‹åŒ–OpenAI Embeddings
    embeddings = OpenAIEmbeddings(
        model="text-embedding-ada-002",  # æˆ– "text-embedding-3-small", "text-embedding-3-large"
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    # ç¤ºä¾‹æ–‡æœ¬
    texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "AIè‡´åŠ›äºåˆ›å»ºæ™ºèƒ½æœºå™¨",
        "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "è‹¹æœæ˜¯ä¸€ç§æ°´æœ"
    ]

    print(f"\nğŸ“ å¤„ç† {len(texts)} ä¸ªæ–‡æœ¬...")

    # ç”Ÿæˆembeddings
    print("\nğŸ”„ ç”Ÿæˆembeddings...")
    embeddings_list = embeddings.embed_documents(texts)

    print(f"âœ… ç”Ÿæˆäº† {len(embeddings_list)} ä¸ªembeddingå‘é‡")
    print(f"ğŸ“ æ¯ä¸ªå‘é‡çš„ç»´åº¦: {len(embeddings_list[0])}")

    # æŸ¥è¯¢å•ä¸ªæ–‡æœ¬çš„embedding
    query_text = "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
    query_embedding = embeddings.embed_query(query_text)
    print(f"\nâ“ æŸ¥è¯¢æ–‡æœ¬: {query_text}")
    print(f"ğŸ“ æŸ¥è¯¢å‘é‡ç»´åº¦: {len(query_embedding)}")

    return embeddings, embeddings_list, query_embedding

# ========================
# 2. ç›¸ä¼¼æ€§è®¡ç®—
# ========================

def similarity_calculations_demo(embeddings, embeddings_list, query_embedding):
    """ç›¸ä¼¼æ€§è®¡ç®—æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ–‡æœ¬ç›¸ä¼¼æ€§è®¡ç®—æ¼”ç¤º")
    print("=" * 60)

    # ç¤ºä¾‹æ–‡æœ¬
    texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "AIè‡´åŠ›äºåˆ›å»ºæ™ºèƒ½æœºå™¨",
        "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
        "è‹¹æœæ˜¯ä¸€ç§æ°´æœ"
    ]

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = []
    for i, doc_embedding in enumerate(embeddings_list):
        similarity = cosine_similarity(
            [query_embedding],
            [doc_embedding]
        )[0][0]
        similarities.append((i, texts[i], similarity))

    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[2], reverse=True)

    print(f"\nğŸ¯ ä¸æŸ¥è¯¢ '{texts[3]}' æœ€ç›¸ä¼¼çš„æ–‡æœ¬:")
    print("-" * 60)
    for i, (idx, text, similarity) in enumerate(similarities):
        print(f"{i+1}. {text}")
        print(f"   ç›¸ä¼¼åº¦: {similarity:.4f}")
        print()

    return similarities

# ========================
# 3. Hugging Face Embeddings
# ========================

def huggingface_embeddings_demo():
    """Hugging Face Embeddingsæ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ¤— Hugging Face Embeddingsæ¼”ç¤º")
    print("=" * 60)

    try:
        # ä½¿ç”¨ä¸­æ–‡æ¨¡å‹
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-small-zh-v1.5",  # ä¸­æ–‡embeddingæ¨¡å‹
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        texts = [
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯",
            "æ·±åº¦å­¦ä¹ å±äºæœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†",
            "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯AIçš„åº”ç”¨é¢†åŸŸ",
            "ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œé€‚åˆå¤–å‡º"
        ]

        print(f"\nğŸ“ ä½¿ç”¨æ¨¡å‹: BAAI/bge-small-zh-v1.5")
        print(f"ğŸ”„ æ­£åœ¨å¤„ç† {len(texts)} ä¸ªä¸­æ–‡æ–‡æœ¬...")

        # ç”Ÿæˆembeddings
        embeddings_list = embeddings.embed_documents(texts)
        print(f"âœ… ç”Ÿæˆå®Œæˆï¼Œå‘é‡ç»´åº¦: {len(embeddings_list[0])}")

        # è®¡ç®—ç›¸ä¼¼åº¦
        query = "äººå·¥æ™ºèƒ½æŠ€æœ¯"
        query_embedding = embeddings.embed_query(query)

        similarities = []
        for i, doc_embedding in enumerate(embeddings_list):
            similarity = cosine_similarity([query_embedding], [doc_embedding])[0][0]
            similarities.append((texts[i], similarity))

        similarities.sort(key=lambda x: x[1], reverse=True)

        print(f"\nğŸ¯ ä¸æŸ¥è¯¢ '{query}' çš„ç›¸ä¼¼åº¦æ’åº:")
        for text, similarity in similarities:
            print(f"   {text}: {similarity:.4f}")

        return embeddings

    except Exception as e:
        print(f"âŒ Hugging Face Embeddingså‡ºé”™: {e}")
        print("ğŸ’¡ æç¤º: éœ€è¦å®‰è£… transformers, sentence_transformers åº“")
        return None

# ========================
# 4. æ‰¹å¤„ç†ä¼˜åŒ–
# ========================

def batch_processing_demo():
    """æ‰¹å¤„ç†ä¼˜åŒ–æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("âš¡ æ‰¹å¤„ç†ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-ada-002",
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    # ç”Ÿæˆå¤§é‡æ–‡æœ¬
    texts = [f"è¿™æ˜¯ç¬¬{i}ä¸ªç¤ºä¾‹æ–‡æœ¬ï¼Œç”¨äºæ¼”ç¤ºæ‰¹å¤„ç†æ•ˆæœ" for i in range(100)]

    import time

    # é€ä¸ªå¤„ç†
    print("\nğŸŒ é€ä¸ªå¤„ç†æ¨¡å¼:")
    start_time = time.time()
    individual_embeddings = []
    for text in texts[:10]:  # åªå¤„ç†å‰10ä¸ªä½œä¸ºç¤ºä¾‹
        embedding = embeddings.embed_query(text)
        individual_embeddings.append(embedding)
    individual_time = time.time() - start_time
    print(f"   å¤„ç†10ä¸ªæ–‡æœ¬è€—æ—¶: {individual_time:.2f}ç§’")

    # æ‰¹é‡å¤„ç†
    print("\nğŸš€ æ‰¹é‡å¤„ç†æ¨¡å¼:")
    start_time = time.time()
    batch_embeddings = embeddings.embed_documents(texts[:10])
    batch_time = time.time() - start_time
    print(f"   å¤„ç†10ä¸ªæ–‡æœ¬è€—æ—¶: {batch_time:.2f}ç§’")

    print(f"\nğŸ’¡ æ‰¹é‡å¤„ç†æ¯”é€ä¸ªå¤„ç†å¿« {individual_time/batch_time:.1f} å€")

# ========================
# 5. å‘é‡æ•°æ®åº“é›†æˆ
# ========================

def vector_store_demo():
    """å‘é‡æ•°æ®åº“é›†æˆæ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ—„ï¸ å‘é‡æ•°æ®åº“é›†æˆæ¼”ç¤º")
    print("=" * 60)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-ada-002",
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    documents = [
        Document(page_content="Pythonæ˜¯ä¸€ç§æµè¡Œçš„ç¼–ç¨‹è¯­è¨€", metadata={"source": "ç¼–ç¨‹æŒ‡å—"}),
        Document(page_content="æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥ä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼", metadata={"source": "AIæ•™ç¨‹"}),
        Document(page_content="æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œ", metadata={"source": "AIæ•™ç¨‹"}),
        Document(page_content="JavaScriptä¸»è¦ç”¨äºç½‘é¡µå¼€å‘", metadata={"source": "Webå¼€å‘"}),
        Document(page_content="è‡ªç„¶è¯­è¨€å¤„ç†å¸®åŠ©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€", metadata={"source": "AIæ•™ç¨‹"})
    ]

    print(f"ğŸ“ å‡†å¤‡äº† {len(documents)} ä¸ªæ–‡æ¡£")

    # åˆ›å»ºFAISSå‘é‡å­˜å‚¨
    print("\nğŸ”§ åˆ›å»ºFAISSå‘é‡å­˜å‚¨...")
    faiss_store = FAISS.from_documents(documents, embeddings)
    print("âœ… FAISSå­˜å‚¨åˆ›å»ºå®Œæˆ")

    # ç›¸ä¼¼æ€§æœç´¢
    query = "äººå·¥æ™ºèƒ½ç›¸å…³æŠ€æœ¯"
    print(f"\nğŸ” æœç´¢æŸ¥è¯¢: {query}")

    # æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£
    similar_docs = faiss_store.similarity_search(query, k=3)

    print(f"\nğŸ“‹ æ‰¾åˆ° {len(similar_docs)} ä¸ªç›¸ä¼¼æ–‡æ¡£:")
    for i, doc in enumerate(similar_docs, 1):
        print(f"{i}. {doc.page_content}")
        print(f"   æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}")
        print()

    # å¸¦åˆ†æ•°çš„ç›¸ä¼¼æ€§æœç´¢
    print("ğŸ“Š å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æœç´¢:")
    docs_with_scores = faiss_store.similarity_search_with_score(query, k=3)

    for i, (doc, score) in enumerate(docs_with_scores, 1):
        print(f"{i}. {doc.page_content}")
        print(f"   ç›¸ä¼¼åº¦åˆ†æ•°: {score:.4f}")
        print()

    return faiss_store

# ========================
# 6. å®é™…åº”ç”¨åœºæ™¯
# ========================

def real_world_applications():
    """å®é™…åº”ç”¨åœºæ™¯æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸŒ å®é™…åº”ç”¨åœºæ™¯æ¼”ç¤º")
    print("=" * 60)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-ada-002",
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    # åœºæ™¯1: æ–‡æ¡£åˆ†ç±»
    print("\nğŸ“‚ åœºæ™¯1: æ–‡æ¡£åˆ†ç±»")
    documents = [
        "è‹¹æœå…¬å¸å‘å¸ƒäº†æ–°çš„iPhoneæ‰‹æœº",
        "ç ”ç©¶è¡¨æ˜è‹¹æœå«æœ‰ä¸°å¯Œçš„ç»´ç”Ÿç´ ",
        "è°·æ­Œæ¨å‡ºæ–°çš„AIæœç´¢åŠŸèƒ½",
        "å¥åº·é¥®é£Ÿå»ºè®®æ¯å¤©åƒæ°´æœ",
        "å¾®è½¯æ”¶è´­äº†OpenAI"
    ]

    # é¢„å®šä¹‰ç±»åˆ«
    categories = {
        "ç§‘æŠ€": ["è‹¹æœå…¬å¸", "iPhone", "è°·æ­Œ", "AI", "å¾®è½¯", "OpenAI"],
        "å¥åº·": ["è‹¹æœ", "ç»´ç”Ÿç´ ", "å¥åº·é¥®é£Ÿ", "æ°´æœ"]
    }

    # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ†ç±»
    doc_embeddings = embeddings.embed_documents(documents)

    for i, (doc, embedding) in enumerate(zip(documents, doc_embeddings)):
        # ç®€å•çš„å…³é”®è¯åŒ¹é…åˆ†ç±»ï¼ˆå®é™…åº”ç”¨ä¸­ä¼šæ›´å¤æ‚ï¼‰
        if any(tech_word in doc for tech_word in categories["ç§‘æŠ€"]):
            category = "ç§‘æŠ€"
        else:
            category = "å¥åº·"

        print(f"æ–‡æ¡£ {i+1}: {doc[:30]}...")
        print(f"åˆ†ç±»: {category}")
        print()

    # åœºæ™¯2: è¯­ä¹‰æœç´¢
    print("ğŸ” åœºæ™¯2: è¯­ä¹‰æœç´¢")
    knowledge_base = [
        "æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œ",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¸®åŠ©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€",
        "è®¡ç®—æœºè§†è§‰ä½¿æœºå™¨èƒ½å¤Ÿç†è§£å’Œåˆ†æå›¾åƒ",
        "å¼ºåŒ–å­¦ä¹ é€šè¿‡è¯•é”™æ¥è®­ç»ƒæ™ºèƒ½ä½“"
    ]

    # åˆ›å»ºå‘é‡å­˜å‚¨
    vector_store = FAISS.from_texts(knowledge_base, embeddings)

    # è¯­ä¹‰æœç´¢æŸ¥è¯¢
    queries = [
        "å¦‚ä½•è®©è®¡ç®—æœºçœ‹æ‡‚å›¾ç‰‡",
        "AIå¦‚ä½•å­¦ä¹ ",
        "æ™ºèƒ½èŠå¤©æœºå™¨äººåŸç†"
    ]

    for query in queries:
        print(f"\nâ“ æŸ¥è¯¢: {query}")
        results = vector_store.similarity_search(query, k=2)
        for j, result in enumerate(results, 1):
            print(f"   {j}. {result.page_content}")

# ========================
# 7. æ€§èƒ½ä¼˜åŒ–æŠ€å·§
# ========================

def performance_optimization():
    """æ€§èƒ½ä¼˜åŒ–æŠ€å·§æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("âš¡ æ€§èƒ½ä¼˜åŒ–æŠ€å·§æ¼”ç¤º")
    print("=" * 60)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",  # ä½¿ç”¨æ›´å°æ›´å¿«çš„æ¨¡å‹
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    # æŠ€å·§1: ç¼“å­˜embeddings
    print("\nğŸ’¾ æŠ€å·§1: ç¼“å­˜embeddings")

    embedding_cache = {}

    def get_cached_embedding(text):
        if text not in embedding_cache:
            embedding_cache[text] = embeddings.embed_query(text)
            print(f"   è®¡ç®—å¹¶ç¼“å­˜: {text[:20]}...")
        else:
            print(f"   ä½¿ç”¨ç¼“å­˜: {text[:20]}...")
        return embedding_cache[text]

    # é‡å¤æ–‡æœ¬æ¼”ç¤ºç¼“å­˜æ•ˆæœ
    texts = ["äººå·¥æ™ºèƒ½æŠ€æœ¯", "äººå·¥æ™ºèƒ½æŠ€æœ¯", "æœºå™¨å­¦ä¹ ", "äººå·¥æ™ºèƒ½æŠ€æœ¯"]

    for text in texts:
        _ = get_cached_embedding(text)

    print(f"ç¼“å­˜å¤§å°: {len(embedding_cache)}")

    # æŠ€å·§2: é€‰æ‹©åˆé€‚çš„æ¨¡å‹
    print("\nğŸ¯ æŠ€å·§2: æ¨¡å‹é€‰æ‹©å¯¹æ¯”")

    models_info = {
        "text-embedding-3-small": {"dimensions": 1536, "cost": "ä½", "speed": "å¿«"},
        "text-embedding-3-large": {"dimensions": 3072, "cost": "é«˜", "speed": "ä¸­"},
        "text-embedding-ada-002": {"dimensions": 1536, "cost": "ä¸­", "speed": "ä¸­"}
    }

    print("æ¨¡å‹å¯¹æ¯”:")
    for model, info in models_info.items():
        print(f"  {model}:")
        print(f"    ç»´åº¦: {info['dimensions']}")
        print(f"    æˆæœ¬: {info['cost']}")
        print(f"    é€Ÿåº¦: {info['speed']}")
        print()

    # æŠ€å·§3: æ‰¹å¤„ç†å¤§å°ä¼˜åŒ–
    print("ğŸ“¦ æŠ€å·§3: æ‰¹å¤„ç†å¤§å°ä¼˜åŒ–")
    print("   å»ºè®®æ‰¹å¤„ç†å¤§å°:")
    print("   - OpenAI: 100-1000ä¸ªæ–‡æœ¬")
    print("   - æœ¬åœ°æ¨¡å‹: æ ¹æ®GPUå†…å­˜è°ƒæ•´")
    print("   - è¿‡å¤§ä¼šå¯¼è‡´è¶…æ—¶ï¼Œè¿‡å°ä¼šå½±å“æ•ˆç‡")

# ========================
# 8. é”™è¯¯å¤„ç†å’Œæœ€ä½³å®è·µ
# ========================

def error_handling_best_practices():
    """é”™è¯¯å¤„ç†å’Œæœ€ä½³å®è·µæ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œæœ€ä½³å®è·µæ¼”ç¤º")
    print("=" * 60)

    # æœ€ä½³å®è·µ1: å¼‚å¸¸å¤„ç†
    print("\nğŸ”§ æœ€ä½³å®è·µ1: å¼‚å¸¸å¤„ç†")

    def safe_embed_text(text, embeddings, max_retries=3):
        """å®‰å…¨çš„æ–‡æœ¬embeddingå‡½æ•°"""
        for attempt in range(max_retries):
            try:
                return embeddings.embed_query(text)
            except Exception as e:
                if attempt == max_retries - 1:
                    print(f"   âŒ åµŒå…¥å¤±è´¥: {e}")
                    return None
                print(f"   âš ï¸ ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œé‡è¯•ä¸­...")
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿

    # æµ‹è¯•å¼‚å¸¸å¤„ç†
    embeddings = OpenAIEmbeddings(
        openai_api_key=os.getenv("ANTHROPIC_API_KEY"),
        openai_api_base=os.getenv("ANTHROPIC_BASE_URL")
    )

    result = safe_embed_text("æµ‹è¯•æ–‡æœ¬", embeddings)
    print(f"   âœ… åµŒå…¥æˆåŠŸ: {result is not None}")

    # æœ€ä½³å®è·µ2: æ–‡æœ¬é¢„å¤„ç†
    print("\nğŸ§¹ æœ€ä½³å®è·µ2: æ–‡æœ¬é¢„å¤„ç†")

    def preprocess_text(text):
        """æ–‡æœ¬é¢„å¤„ç†"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = ' '.join(text.split())
        # æˆªæ–­è¿‡é•¿æ–‡æœ¬ï¼ˆæ ¹æ®æ¨¡å‹é™åˆ¶ï¼‰
        max_length = 8191  # OpenAIçš„tokené™åˆ¶
        if len(text) > max_length * 4:  # ç²—ç•¥ä¼°ç®—tokenæ•°
            text = text[:max_length * 4] + "..."
        return text

    long_text = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„æ–‡æœ¬ï¼Œ" * 1000
    processed_text = preprocess_text(long_text)
    print(f"   åŸå§‹é•¿åº¦: {len(long_text)} å­—ç¬¦")
    print(f"   å¤„ç†åé•¿åº¦: {len(processed_text)} å­—ç¬¦")

    # æœ€ä½³å®è·µ3: ç›‘æ§å’Œæ—¥å¿—
    print("\nğŸ“Š æœ€ä½³å®è·µ3: ç›‘æ§å’Œæ—¥å¿—")

    class EmbeddingMonitor:
        def __init__(self):
            self.request_count = 0
            self.total_tokens = 0

        def log_request(self, text_length):
            self.request_count += 1
            # ç²—ç•¥ä¼°ç®—tokenæ•°ï¼ˆ1 token â‰ˆ 4å­—ç¬¦ï¼‰
            estimated_tokens = text_length // 4
            self.total_tokens += estimated_tokens
            print(f"   è¯·æ±‚ #{self.request_count}: ~{estimated_tokens} tokens")

    monitor = EmbeddingMonitor()
    test_texts = ["çŸ­æ–‡æœ¬", "è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰é•¿åº¦çš„æµ‹è¯•æ–‡æœ¬", "è¿™æ˜¯ä¸€ä¸ª" * 100]

    for text in test_texts:
        monitor.log_request(len(text))

    print(f"   æ€»è¯·æ±‚æ•°: {monitor.request_count}")
    print(f"   æ€»tokenæ•°: ~{monitor.total_tokens}")

# ========================
# ä¸»å‡½æ•°
# ========================

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LangChain Embeddings å®Œå…¨ä½¿ç”¨æŒ‡å—")
    print("=" * 80)

    try:
        # 1. åŸºæœ¬ç”¨æ³•
        embeddings, embeddings_list, query_embedding = basic_embeddings_demo()

        # 2. ç›¸ä¼¼æ€§è®¡ç®—
        similarities = similarity_calculations_demo(embeddings, embeddings_list, query_embedding)

        # 3. Hugging Face Embeddings
        hf_embeddings = huggingface_embeddings_demo()

        # 4. æ‰¹å¤„ç†ä¼˜åŒ–
        batch_processing_demo()

        # 5. å‘é‡æ•°æ®åº“é›†æˆ
        vector_store = vector_store_demo()

        # 6. å®é™…åº”ç”¨åœºæ™¯
        real_world_applications()

        # 7. æ€§èƒ½ä¼˜åŒ–
        performance_optimization()

        # 8. é”™è¯¯å¤„ç†å’Œæœ€ä½³å®è·µ
        error_handling_best_practices()

        print("\n" + "=" * 80)
        print("ğŸ‰ Embeddingsæ•™ç¨‹å®Œæˆï¼")
        print("=" * 80)

        print("\nğŸ’¡ ä¸»è¦è¦ç‚¹æ€»ç»“:")
        print("1. âœ… Embeddingså°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­ä¹‰å‘é‡")
        print("2. ğŸ” å¯ç”¨äºç›¸ä¼¼æ€§æœç´¢å’Œæ–‡æœ¬åˆ†æ")
        print("3. âš¡ æ‰¹å¤„ç†å¤§å¹…æå‡å¤„ç†æ•ˆç‡")
        print("4. ğŸ—„ï¸ ä¸å‘é‡æ•°æ®åº“é›†æˆæ„å»ºæœç´¢ç³»ç»Ÿ")
        print("5. ğŸ›¡ï¸ éœ€è¦é€‚å½“çš„é”™è¯¯å¤„ç†å’Œä¼˜åŒ–")

    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("\nğŸ’¡ è¯·æ£€æŸ¥:")
        print("1. ç¯å¢ƒå˜é‡æ˜¯å¦æ­£ç¡®è®¾ç½®")
        print("2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("3. APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ")

if __name__ == "__main__":
    main()